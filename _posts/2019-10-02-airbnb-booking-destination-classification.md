---
title: "Comprehensive Modeling Analysis: Predicting Airbnb New User Bookings"
date: 2019-10-02
tags: [machine learning, data science, classification, neural network]
header:
  image: "/images/projects/airbnb-booking-destination-classification/airbnb.jpg"
excerpt: "This classifier uses an unsupervised-supervised modeling pipeline to  predict booking destination countries of first time AirBnB users."
---

The full code can be found [here](https://colab.research.google.com/drive/1J3U0fmLw8Mv9Ppu_CJBNNE0tyTfsexcc).

Home rental services such as Airbnb allow people to find affordable temporary housing on short notice. The efficiency of these services can be increased by making decisions informed by knowledge about how future customers will use them. By being able to determine how new customers will use home rental services for the first time, proactive changes can be made to the service and the surrounding operations. This results in a better product for the customers and improved operations for stakeholders. Data science techniques allow for the prediction of future user activities at the cost of relatively few resources. The abundance of data generated by short-term home rental services such as Airbnb, allows for plenty of opportunities for data science to be used to improve the operations. In order to utilize this readily usable data, I am interested in predicting booking destinations of first time Airbnb users.

Airbnb’s platform allows customers to efficiently find homes that fit their needs through the use of features such as filtered searches and wishlists. After finding desirable lodging, customers input payment information and book the locations. Throughout this process data is generated through information provided by the users and details saved about the user’s web sessions.

Several types of classification models were used to predict the first booking destination countries of Airbnb users. Models focused on using demographic and web session data to assign booking destination countries to individual users. The different models were compared by their ability to accurately and efficiently predict booking country. A final model was chosen from those that were evaluated and deemed suitable to be scaled for production. The process used to build this product is as follows:


Initiation and Data Preprocessing
* Import Packages and Files
* Data Cleaning
* Feature Engineering

Data Analysis and Exploration
* Viewing the Distribution of the Different Classes
* Checking the Correlatedness of Different Variables
* Interpreting Descriptive Statistics

Preparing The Data For Modeling
* Class Balancing and Mathematical Transformations
* Feature Selection and Reduction
* Establishing Variables for Training and Testing

Unsupervised Clustering Analysis
* Selecting Appropriate Clustering Method
* Analyzing Clusters Made Using K Means

Supervised Learning Models
1. Using Unaltered Features
2. Using Features Selected using F Values
3. Using Features Selected using Chi Squared

Deep Learning Classification Models
1. Using Convolutional Neural Network
2. Using Recurrent Neural Network
3. Using Convolutional Neural Networks Paired With Recurrent Neural Networks

Analysis and Conclusion
* Final Model Selection and Analysis
* Conclusion and Discussion


## Initiation and Data Preprocessing

The data used for this model was released by Airbnb in the following datasets: train_users.csv and sessions.csv. The train user dataset contains information about specific users and how they first accessed the service. This dataset has over 200000 records with each one containing information about a unique user. The train user dataset contains the outcome variable, country destination. The sessions dataset contains information about actions performed during the user’s time on the Airbnb platform. This dataset contains over 10 million records with each one reflecting a specific action performed on the platform. Multiple records on the sessions dataset can refer a single user’s actions. A dataframe was created for modeling containing features generated from both datasets.


## Data Exploration and Analysis

The Airbnb data contains information about activity on the platform that occurred between January 2010 and June 2014.  The train dataset originally contained columns related to the time specific activities first occured, information about the how the user accessed Airbnb, and demographics of the users. Additional features were engineered to include the amount of time users spent doing specific activities on the platform.



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_11_0.png)


Above are plots representing the gender frequencies and age frequencies of the data, respectively. There are more females than males included in the data but the disparity between the two groups is not strong. The distribution of the ages is centered around the 30’s and skewed to the right. This likely reflects an age demographic with both the energy and resources to travel. Since both of these variables contain a large amount of null values imputation will be needed to make use of this data prior to modeling.


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_13_0.png)


Above are plots of languages used on the platform and country destination, respectively. Both plots are scaled logarithmically for readability because the dominant classes far outnumbered the rest. With regard to the language counts, English outnumbered the other classes greatly; and with regard to the country destinations, ‘NDF’ and the US outnumbered the other classes. NDF represents the class of users that haven’t booked a destination yet. Since this would be the class that the model being built would be predicting, this was not be used as an outcome variable to train the model.


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_18_0.png)



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_18_1.png)



The above plots refer to the frequencies the accounts were first created and frequencies bookings were made over the course of multiple years. The frequency of accounts being created showed an increasing trajectory over the course of five years, likely reflecting an increase in the userbase of Airbnb. There is a sharp drop in bookings made around the time that the data was collected. This doesn’t reflect a drop in the usage of the platform, but rather people who use the platform but haven’t made a booking yet (these customers would have the country destination label ‘NDF’).


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_20_0.png)



The above plots reflect the frequencies use of the platform created across different timespans.
There’s a notable drop of accounts created between June and July. Since summer is a season that is popular for travel, people are less likely to need accounts during this time (because they would’ve presumably made accounts earlier than when they would travel using the service). There is a slight drop of accounts made over the weekends as well. Initial activity drops during the day which is likely the result of people having less time during the average US workday to be on Airbnb.



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_22_0.png)



The scatterplot matrix gives information about the relationship between specific engineered features.


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_24_0.png)



The heatmap is meant to gauge the correlatedness of engineered features. There is much correlatedness among these features, which is expected since they reflect amounts of time spent on the platform.



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>secs_elapsed</th>
      <th>view_search_results_count</th>
      <th>p3_count</th>
      <th>wishlist_content_update_count</th>
      <th>user_profile_count</th>
      <th>change_trip_characteristics_count</th>
      <th>similar_listings_count</th>
      <th>user_social_connections_count</th>
      <th>update_listing_count</th>
      <th>listing_reviews_count</th>
      <th>...</th>
      <th>dashboard_secs</th>
      <th>user_wishlists_secs</th>
      <th>header_userpic_secs</th>
      <th>message_thread_secs</th>
      <th>edit_profile_secs</th>
      <th>message_post_secs</th>
      <th>contact_host_secs</th>
      <th>unavailable_dates_secs</th>
      <th>confirm_email_link_secs</th>
      <th>create_user_secs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>...</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1514234.959</td>
      <td>12.366</td>
      <td>8.265</td>
      <td>6.093</td>
      <td>3.634</td>
      <td>4.278</td>
      <td>4.172</td>
      <td>1.940</td>
      <td>2.105</td>
      <td>1.128</td>
      <td>...</td>
      <td>14830.135</td>
      <td>29595.309</td>
      <td>4515.531</td>
      <td>51134.615</td>
      <td>16820.605</td>
      <td>74545.834</td>
      <td>21243.895</td>
      <td>7023.293</td>
      <td>94984.878</td>
      <td>100.872</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1913191.475</td>
      <td>28.446</td>
      <td>20.404</td>
      <td>13.310</td>
      <td>14.324</td>
      <td>9.715</td>
      <td>10.018</td>
      <td>9.129</td>
      <td>8.068</td>
      <td>5.697</td>
      <td>...</td>
      <td>112658.964</td>
      <td>177757.650</td>
      <td>45779.443</td>
      <td>306877.869</td>
      <td>93129.557</td>
      <td>268323.753</td>
      <td>104188.163</td>
      <td>53648.290</td>
      <td>259453.037</td>
      <td>7967.568</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>256920.500</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>143.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>872862.000</td>
      <td>2.000</td>
      <td>2.000</td>
      <td>1.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>695.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2043487.500</td>
      <td>13.000</td>
      <td>9.000</td>
      <td>6.000</td>
      <td>2.000</td>
      <td>4.000</td>
      <td>4.000</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>1268.000</td>
      <td>0.000</td>
      <td>2356.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1583.000</td>
      <td>427.000</td>
      <td>0.000</td>
      <td>44589.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>38221363.000</td>
      <td>987.000</td>
      <td>1131.000</td>
      <td>524.000</td>
      <td>454.000</td>
      <td>308.000</td>
      <td>325.000</td>
      <td>280.000</td>
      <td>249.000</td>
      <td>198.000</td>
      <td>...</td>
      <td>4632786.000</td>
      <td>6713259.000</td>
      <td>2288760.000</td>
      <td>11242289.000</td>
      <td>3415239.000</td>
      <td>13260005.000</td>
      <td>3258808.000</td>
      <td>2310246.000</td>
      <td>2902601.000</td>
      <td>1723704.000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 39 columns</p>
</div>



Data will be normalized prior to modeling to account for the vast difference in scale of the variables.

## Preparing The Data For Modeling

To prepare the data for modeling, values were imputed, the data was resampled to address the class imbalance in the outcome, and multiple forms of feature reduction were implemented.
This resulted in four sets of variables: One reflecting all of the unaltered features of the dataset, one reflecting features with the highest F values, one reflecting features with the highest chi squared values, and encoded for deep learning.


```python
%%time

## Train Test Split the Four Sets of Feature and Outcome Variables

# Unaltered Features
x_train, x_test, y_train, y_test = train_test_split(pca_components, y, test_size=0.2, random_state=20)

# F-Value
fx_train, fx_test, fy_train, fy_test = train_test_split(f_pca_components, y, test_size=0.2, random_state=22)

# Chi-squared
xx_train, xx_test, xy_train, xy_test = train_test_split(x_pca_components, y, test_size=0.2, random_state=21)

# Deep Learning
X_train, X_test, Y_train, Y_test = train_test_split(np.asarray(f_pca_components), np.asarray(Y), test_size=0.2, random_state=20)

```



Training and testing sets of four variables were generated to be used in modeling.
The x and y variables represent the variables to be used for modeling that reflect the PCA components of all of the useful features of the data.
The fx and fy variables represent the variables to be used for modeling that reflect PCA components of the features with high F values.
The kx and ky variables represent the variables to be used for modeling that reflect PCA components of the features with high chi squared scores.
The X and Y variables represent the variables converted to arrays to be used for deep learning.

## Supervised Modeling using All Features



### Random Forest



    accuracy score:
    0.9535858585858585

    cross validation:
    [0.93790231 0.93769333 0.93775253 0.93673044 0.93811569]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    0    0    7 1827    0    0    0    0    7    0]
     [   0    0    0    0    0 1782    0    0    0    0    0]
     [   0    0    0    0    2    0 1787    0    0    0    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [   6   32   11   65  174   63   64    7    6 1021  279]
     [   3    2    4   14   25    7   16    0    1  124 1596]]

                  precision    recall  f1-score   support

              AU       1.00      1.00      1.00      1839
              CA       0.98      1.00      0.99      1855
              DE       0.99      1.00      1.00      1787
              ES       0.95      1.00      0.98      1814
              FR       0.90      0.99      0.94      1841
              GB       0.96      1.00      0.98      1782
              IT       0.96      1.00      0.98      1789
              NL       1.00      1.00      1.00      1794
              PT       1.00      1.00      1.00      1779
              US       0.89      0.59      0.71      1728
           other       0.85      0.89      0.87      1792

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.95      0.95      0.95     19800
    weighted avg       0.95      0.95      0.95     19800



The models that relied on the dataset’s unreduced features, in general, had the best accuracy in the study. An advantage of using all of the useful features is that as much meaningful variance was captured by the models as possible. A downside to this type of feature preparation that did stand out is the lack of efficiency. Since feature reduction didn’t take place with these models, their performance suffered and they had the longest runtimes. This method of feature selection also risks including features with variance that doesn’t aid in the predictive power of the models. However, this potential disadvantage didn’t hamper the model’s ability to perform well because many of the features that would noticeably have a negative effect on the models were already left out.



## Modeling the Data using Features with High F-Values


### Random Forest


    accuracy score:
    0.9503030303030303

    cross validation:
    [0.93890046 0.93681752 0.93831681 0.93730665 0.93761051]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1831    0    0    0    0    0    0    0]
     [   0    5    0    0 1827    1    1    2    0   11    3]
     [   0    0    0    0    0 1788    0    0    0    0    0]
     [   0    0    0    0    0    0 1833    0    0    0    0]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  10   38    9   63  164   55   75    9    2 1048  302]
     [   0    7    2   11   40    8   11    0    0  155 1579]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      1.00      1770
              CA       0.97      1.00      0.99      1775
              DE       0.99      1.00      1.00      1761
              ES       0.96      1.00      0.98      1831
              FR       0.90      0.99      0.94      1850
              GB       0.97      1.00      0.98      1788
              IT       0.95      1.00      0.98      1833
              NL       0.99      1.00      1.00      1806
              PT       1.00      1.00      1.00      1798
              US       0.86      0.59      0.70      1775
           other       0.84      0.87      0.85      1813

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.95      0.95      0.95     19800
    weighted avg       0.95      0.95      0.95     19800



The accuracy scores of the models that used features with high F values were similar to those the models that used unaltered features. This form of feature selection likely removed features that were not vital to the predictive accuracy of the models. This form of feature selection has the advantage advantage of reducing computational complexity and runtimes and in this case, this made up lack of difference in accuracy of the better performing model types.



## Modeling the Data using Features Chosen with Chi-Squared


### Random Forest


    accuracy score:
    0.9348989898989899

    cross validation:
    [0.92552386 0.92393637 0.92386844 0.9222124  0.92194506]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1803    0    0    0    1    0    3    0    0    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   0    3    0 1804    1    1    1    2    0    2    0]
     [   3    3    1    4 1786    9    2    4    0    8    2]
     [   0    1    8    6    1 1813    0    0    0    3    0]
     [   2    2    3    6    0    2 1798    2    0    0    0]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  14   70   35   86  198  102   90   26    6  866  307]
     [   1   11   12   15   37   21   20    3    3  146 1489]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1759
              CA       0.95      1.00      0.97      1807
              DE       0.97      1.00      0.98      1841
              ES       0.94      0.99      0.97      1814
              FR       0.88      0.98      0.93      1822
              GB       0.93      0.99      0.96      1832
              IT       0.94      0.99      0.97      1815
              NL       0.98      1.00      0.99      1781
              PT       0.99      1.00      1.00      1771
              US       0.84      0.48      0.61      1800
           other       0.83      0.85      0.84      1758

       micro avg       0.93      0.93      0.93     19800
       macro avg       0.93      0.93      0.93     19800
    weighted avg       0.93      0.93      0.93     19800



When it comes to comparing the accuracy scores of the full featured models and models that used features based on their chi-squared scores, most of the full featured models outperformed their counterparts. However, the significant drop in the runtimes of the models that used features with high chi-squared values does make this form of feature selection better than simply retrieving PCA components from all generated features. However feature selection based on F values also had this effect without compromising predictive accuracy.




## Modeling Data using Deep Learning


### Recurrent Neural Network


    Test accuracy: 0.9169191919432746

    confusion matrix:
    [[1823    0    0    0   16    0    0    0    0    0    0]
     [   0 1837    0    0   11    0    4    0    0    0    3]
     [   0    0 1775    0    5    0    0    0    0    7    0]
     [   0    7    1 1760   15    0   14    1    0   13    3]
     [   3    4    3    3 1775    5    2    3    0   21   22]
     [   0   11    3    2    9 1711    0   11    0   26    9]
     [   4    2    3    8   29    7 1666    2    0   48   20]
     [   0    0    0    0   15    0    0 1768    0   11    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  20   68   23   64  211   52   75   35    5  815  360]
     [   7   20    3   20   76   21   31   14    0  154 1446]]

                  precision    recall  f1-score   support

               0       0.98      0.99      0.99      1839
               1       0.94      0.99      0.97      1855
               2       0.98      0.99      0.99      1787
               3       0.95      0.97      0.96      1814
               4       0.82      0.96      0.89      1841
               5       0.95      0.96      0.96      1782
               6       0.93      0.93      0.93      1789
               7       0.96      0.99      0.97      1794
               8       1.00      1.00      1.00      1779
               9       0.74      0.47      0.58      1728
              10       0.78      0.81      0.79      1792

       micro avg       0.92      0.92      0.92     19800
       macro avg       0.91      0.91      0.91     19800
    weighted avg       0.91      0.92      0.91     19800



The convolutional neural network and recurrent neural networks were both able to reach test accuracy scores of over 90% after 50 epochs.Despite the potential of deep learning models to attain high accuracies, the computational complexity and slow runtimes makes this type of modeling unsuitable for this business objective.

## Analysis and Conclusion

The random forest model using features chosen based on F values was the best model when it came to predicting the first booking destinations, making it the strongest base for a classifier that can scaled for production.
While other model types had potential to produce more accurate predictions, they had much higher runtimes, making them unfit to run larger amounts of data. By introducing more data to this modeling pipeline, it can be trained to yield even more accurate and consistent results.

This classifier created by pairing the best supervised modeling technique and feature reduction method was built to be both accurate and scalable. Potential improvements in this product includes adding more features and further tuning of the model type. The accuracy of this model will most likely increase as it is trained with more data as well.

Understanding how to better utilize supervised modeling techniques to predict booking destination, will give insight as to how people are using the Airbnb platform and particular habits different types of customers share.
This can allow for more direct marketing to specific types of users or changes in the product that better match how the service is used. Through the use of cheap and accessible data, decisions can be made that can result in increased efficiency and revenue for the company.
