---
title: "AirBnB New User Bookings Classification Model"
date: 2019-10-01
tags: [machine learning, data science, classification]
header:
  image: "/images/airbnb.jpg"
excerpt: "This classifier uses an unsupervised-supervised modeling pipeline to  predict booking destination countries of first time AirBnB users."
---

# Comprehensive Modeling Analysis: Predicting Airbnb New User Bookings

<br>

Home rental services such as Airbnb allow people to find affordable temporary housing on short notice. The efficiency of these services can be increased by making decisions informed by knowledge about how future customers will use them. By being able to determine how new customers will use home rental services for the first time, proactive changes can be made to the service and the surrounding operations. This results in a better product for the customers and improved operations for stakeholders. Data science techniques allow for the prediction of future user activities at the cost of relatively few resources. The abundance of data generated by short-term home rental services such as Airbnb, allows for plenty of opportunities for data science to be used to improve the operations. In order to utilize this readily usable data, I am interested in predicting booking destinations of first time Airbnb users.

Airbnb’s platform allows customers to efficiently find homes that fit their needs through the use of features such as filtered searches and wishlists. After finding desirable lodging, customers input payment information and book the locations. Throughout this process data is generated through information provided by the users and details saved about the user’s web sessions.

Several types of classification models were used to predict the first booking destination countries of Airbnb users. Models focused on using demographic and web session data to assign booking destination countries to individual users. The different models were compared by their ability to accurately and efficiently predict booking country. A final model was chosen from those that were evaluated and deemed suitable to be scaled for production. The process used to build this product is as follows:

<br>

Initiation and Data Preprocessing
* Import Packages and Files
* Data Cleaning
* Feature Engineering

Data Analysis and Exploration
* Viewing the Distribution of the Different Classes
* Checking the Correlatedness of Different Variables
* Interpreting Descriptive Statistics

Preparing The Data For Modeling
* Class Balancing and Mathematical Transformations
* Feature Selection and Reduction
* Establishing Variables for Training and Testing

Unsupervised Clustering Analysis
* Selecting Appropriate Clustering Method
* Analyzing Clusters Made Using K Means

Supervised Learning Models
1. Using Unaltered Features
2. Using PCA Components
3. Using Selectkbest Function

Deep Learning Classification Models
1. Using Convolutional Neural Network
2. Using Recurrent Neural Network
3. Using Convolutional Neural and Recurrent Networks Together

Analysis and Conclusion
* Selecting and Analyzing Final Model
* Conclusion and Discussion


## Initiation and Data Preprocessing

The data used for this model was released by Airbnb in the following datasets: train_users.csv and sessions.csv. The train user dataset contains information about specific users and how they first accessed the service. This dataset has over 200000 records with each one containing information about a unique user. The train user dataset contains the outcome variable, country destination. The sessions dataset contains information about actions performed during the user’s time on the Airbnb platform. This dataset contains over 10 million records with each one reflecting a specific action performed on the platform. Multiple records on the sessions dataset can refer a single user’s actions. A dataframe was created for modeling containing features generated from both datasets.


```python
%%time

import math
from scipy import stats
import re
import warnings
import datawig

from IPython.display import display
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import geopandas
from shapely.geometry import Point
import seaborn as sns
import statsmodels.formula.api as smf
from matplotlib.mlab import PCA as mlabPCA
from sklearn.naive_bayes import BernoulliNB
from sklearn import linear_model
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn import neighbors
from sklearn.utils import resample
from sklearn import tree
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, accuracy_score, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn import ensemble
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import KernelPCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.cluster import KMeans
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import SpectralClustering


import keras
from keras.models import Model
from keras.layers import Input, Dense, TimeDistributed
from keras.layers import LSTM
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense,Dropout,Flatten,Conv1D, MaxPool1D, MaxPooling1D
from keras.optimizers import RMSprop,Adam
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential
from keras import backend as K
from keras.layers.embeddings import Embedding

from datetime import datetime
from dateutil.parser import parse


# Display Preferences
%matplotlib inline
pd.options.display.float_format = '{:.3f}'.format

# Suppress Warnings
warnings.filterwarnings(action="ignore")


# Set Plot Style
sns.set_style('darkgrid')
sns.color_palette("Paired")

```

    Using TensorFlow backend.


    CPU times: user 3.44 s, sys: 867 ms, total: 4.3 s
    Wall time: 4.71 s



```python
%%time

## Import Files

df_train = pd.read_csv('train_users_2.csv')
df_sessions = pd.read_csv('sessions.csv')

## Create DataFrame for Modeling and Analysis

df = df_train

```

    CPU times: user 8.19 s, sys: 822 ms, total: 9.01 s
    Wall time: 9.08 s



```python
## View Data for Cleaning

#df.head(7)
#df.dtypes
#df.info()
#df.describe()
#df.iloc[:,0].nunique()
#df.isnull().sum(axis = 0)
#len(df)

```


```python
%%time

## Data Cleaning

# Converting Unknown Gender to Null
df['gender'] = df['gender'].map({'-unknown-': np.nan, 'FEMALE':'female', 'MALE':'male'})

# Convert Extreme Ages to Null
df[df.age < 15] = df[df.age < 15].assign(age=np.nan)
df[df.age > 150] = df[df.age > 150].assign(age=np.nan)

```

    CPU times: user 124 ms, sys: 4.97 ms, total: 129 ms
    Wall time: 131 ms



```python
%%time

## Parse Columns to DateTime Format

df['date_account_created'] = df['date_account_created'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d', errors='coerce'))
df['timestamp_first_active'] = df['timestamp_first_active'].apply(lambda x: pd.to_datetime(x, format='%Y%m%d%H%M%S', errors='coerce'))
df['date_first_booking'] = df['date_first_booking'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d', errors='coerce'))

## Creating Features based on DateTime Columns

df['year_account_created'] = df['date_account_created'].apply(lambda x: x.year)
df['month_account_created'] = df['date_account_created'].apply(lambda x: x.month)
df['day_account_created'] = df['date_account_created'].apply(lambda x: x.day)
df['day_of_year_account_created'] = df['date_account_created'].apply(lambda x: x.dayofyear)
df['timestamp_account_created'] = df['date_account_created'].apply(lambda x: x.timestamp())

df['year_first_active'] = df['timestamp_first_active'].apply(lambda x: x.year)
df['month_first_active'] = df['timestamp_first_active'].apply(lambda x: x.month)
df['day_first_active'] = df['timestamp_first_active'].apply(lambda x: x.day)
df['hour_first_active'] = df['timestamp_first_active'].apply(lambda x: x.hour)
df['first_active_timestamp'] = df['timestamp_first_active'].apply(lambda x: x.timestamp())

df['year_first_booking'] = df['date_first_booking'].apply(lambda x: x.year)
df['month_first_booking'] = df['date_first_booking'].apply(lambda x: x.month)
df['day_first_booking'] = df['date_first_booking'].apply(lambda x: x.day)
df['hour_first_booking'] = df['date_first_booking'].apply(lambda x: x.hour)

```

    CPU times: user 1min 39s, sys: 1.12 s, total: 1min 41s
    Wall time: 1min 41s



```python
%%time

## Creating Features based on Sessions DataFrame

df_sessions['action_detail_count'] = df_sessions.groupby([
    'user_id','action_detail'])['action_detail'].transform('count')

# Total Amount of Time Spent on Platform
df_session_length = pd.DataFrame(df_sessions.groupby(['user_id'])['secs_elapsed'].sum())
df_session_features = df_session_length

# Counts of Specific Action Details
num_classes = 4
action_detail_count_classes = list(df_sessions['action_detail'].value_counts().index)[:num_classes]

for value in action_detail_count_classes:
    column = df_sessions.loc[df_sessions['action_detail'] == value][['user_id','action_detail_count']].drop_duplicates().rename(columns={'action_detail_count': value +'_count'})
    df_session_features = pd.merge(df_session_features, column, left_on='user_id', right_on='user_id', how='left').fillna(value=0)

# Time Spent on Specific Action Details
num_classes = 10
action_detail_time_classes = list(df_sessions['action_detail'].value_counts().index)[:num_classes]

for value in action_detail_time_classes:
    column = pd.DataFrame(df_sessions.loc[df_sessions['action_detail'] == value].groupby(['user_id'])['secs_elapsed'].sum()).rename(columns = {"secs_elapsed" : value+'_secs'})
    df_session_features = pd.merge(df_session_features, column, left_on='user_id', right_on='user_id', how='left').fillna(value=0)

```

    CPU times: user 19.6 s, sys: 1.24 s, total: 20.8 s
    Wall time: 21 s



```python
%%time

## Drop -Unknown- features

df_session_features = df_session_features.drop(['-unknown-_count','-unknown-_secs'],1)

## Save Session Feature Names

session_feature_names = list(df_session_features.columns[df_session_features.columns != 'user_id'])

## Merge Session Features with Training DataFrame

df = pd.merge(df, df_session_features, left_on='id', right_on='user_id', how='left').drop(['user_id'],1)
```

    CPU times: user 399 ms, sys: 116 ms, total: 514 ms
    Wall time: 515 ms


## Data Exploration and Analysis

The Airbnb data contains information about activity on the platform that occurred between January 2010 and June 2014.  The train dataset originally contained columns related to the time specific activities first occured, information about the how the user accessed Airbnb, and demographics of the users. Additional features were engineered to include the amount of time users spent doing specific activities on the platform.


```python
%%time

## First Several Rows Of Original Train Dataset

df.dropna().iloc[:,0:16].head(7)
```

    CPU times: user 258 ms, sys: 18.7 ms, total: 277 ms
    Wall time: 275 ms





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>date_account_created</th>
      <th>timestamp_first_active</th>
      <th>date_first_booking</th>
      <th>gender</th>
      <th>age</th>
      <th>signup_method</th>
      <th>signup_flow</th>
      <th>language</th>
      <th>affiliate_channel</th>
      <th>affiliate_provider</th>
      <th>first_affiliate_tracked</th>
      <th>signup_app</th>
      <th>first_device_type</th>
      <th>first_browser</th>
      <th>country_destination</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>137021</th>
      <td>d1mm9tcy42</td>
      <td>2014-01-01</td>
      <td>2014-01-01 00:09:36</td>
      <td>2014-01-04</td>
      <td>male</td>
      <td>62.000</td>
      <td>basic</td>
      <td>0</td>
      <td>en</td>
      <td>sem-non-brand</td>
      <td>google</td>
      <td>omg</td>
      <td>Web</td>
      <td>Windows Desktop</td>
      <td>Chrome</td>
      <td>other</td>
    </tr>
    <tr>
      <th>137027</th>
      <td>xwxei6hdk4</td>
      <td>2014-01-01</td>
      <td>2014-01-01 00:27:42</td>
      <td>2014-01-07</td>
      <td>female</td>
      <td>32.000</td>
      <td>facebook</td>
      <td>0</td>
      <td>en</td>
      <td>seo</td>
      <td>google</td>
      <td>linked</td>
      <td>Web</td>
      <td>iPad</td>
      <td>Mobile Safari</td>
      <td>US</td>
    </tr>
    <tr>
      <th>137032</th>
      <td>awiurksqr3</td>
      <td>2014-01-01</td>
      <td>2014-01-01 01:01:13</td>
      <td>2014-01-02</td>
      <td>female</td>
      <td>32.000</td>
      <td>facebook</td>
      <td>0</td>
      <td>en</td>
      <td>direct</td>
      <td>direct</td>
      <td>untracked</td>
      <td>Web</td>
      <td>iPad</td>
      <td>Mobile Safari</td>
      <td>US</td>
    </tr>
    <tr>
      <th>137036</th>
      <td>jrqykh9y8x</td>
      <td>2014-01-01</td>
      <td>2014-01-01 01:19:19</td>
      <td>2014-04-19</td>
      <td>female</td>
      <td>27.000</td>
      <td>facebook</td>
      <td>0</td>
      <td>zh</td>
      <td>seo</td>
      <td>google</td>
      <td>linked</td>
      <td>Web</td>
      <td>Mac Desktop</td>
      <td>Chrome</td>
      <td>FR</td>
    </tr>
    <tr>
      <th>137037</th>
      <td>s9xrwtyzsq</td>
      <td>2014-01-01</td>
      <td>2014-01-01 01:23:45</td>
      <td>2014-01-02</td>
      <td>male</td>
      <td>30.000</td>
      <td>basic</td>
      <td>0</td>
      <td>en</td>
      <td>sem-brand</td>
      <td>google</td>
      <td>omg</td>
      <td>Web</td>
      <td>iPad</td>
      <td>Mobile Safari</td>
      <td>US</td>
    </tr>
    <tr>
      <th>137048</th>
      <td>oa8oz6sj6s</td>
      <td>2014-01-01</td>
      <td>2014-01-01 02:38:48</td>
      <td>2014-02-26</td>
      <td>male</td>
      <td>40.000</td>
      <td>basic</td>
      <td>0</td>
      <td>en</td>
      <td>direct</td>
      <td>direct</td>
      <td>linked</td>
      <td>Web</td>
      <td>Mac Desktop</td>
      <td>Chrome</td>
      <td>US</td>
    </tr>
    <tr>
      <th>137056</th>
      <td>9v9pjiw5s1</td>
      <td>2014-01-01</td>
      <td>2014-01-01 03:53:58</td>
      <td>2014-01-01</td>
      <td>male</td>
      <td>25.000</td>
      <td>basic</td>
      <td>0</td>
      <td>en</td>
      <td>sem-brand</td>
      <td>google</td>
      <td>linked</td>
      <td>Web</td>
      <td>Mac Desktop</td>
      <td>Safari</td>
      <td>US</td>
    </tr>
  </tbody>
</table>
</div>




```python
%%time

## Graphing Gender and Age Frequencies

fig = plt.figure(figsize=(14, 4))
fig.subplots_adjust(hspace=.5)

# Bar Graph of Genders
plt.subplot(1, 2, 1)
ax = df.gender.value_counts().plot(kind='bar',  title="Gender Frequencies")
ax.set_xlabel("Gender")
ax.set_ylabel("Frequency")

# Histogram of Ages
plt.subplot(1, 2, 2)
ax = df.age.plot(kind='hist',   title="Distribution of Ages")
ax.set_xlabel("Age")
ax.set_ylabel("Frequency")


plt.show()
```


![png](output_11_0.png)


    CPU times: user 910 ms, sys: 111 ms, total: 1.02 s
    Wall time: 426 ms


Above are plots representing the gender frequencies and age frequencies of the data, respectively. There are more females than males included in the data but the disparity between the two groups is not strong. The distribution of the ages is centered around the 30’s and skewed to the right. This likely reflects an age demographic with both the energy and resources to travel. Since both of these variables contain a large amount of null values imputation will be needed to make use of this data prior to modeling.


```python
%%time

## Graphing  Language and Country Destination Frequencies (Logarithmic Scale is Used for Readability)

# Language Preferences Used (Logarithmic Scale)
fig = plt.figure(figsize=(14, 10))
fig.subplots_adjust(hspace=.25)

plt.subplot(2, 1, 1)
ax = df.language.value_counts().apply(math.log).head(20).plot(kind='bar',

                                    title="Frequency of Language Preferences Used (Logarithmic Scale)")
ax.set_xlabel("Language Labels")
ax.set_ylabel("Frequency")
plt.xticks(rotation=0)
sns.despine()

# Country Destinations (Logarithmic Scale)
plt.subplot(2, 1, 2)
ax = df.country_destination.value_counts().apply(math.log).plot(kind='bar',

                                    title="Frequency of Country Destinations (Logarithmic Scale)")
ax.set_xlabel("Country Destinations")
ax.set_ylabel("Frequency")
plt.xticks(rotation=0)
sns.despine()
plt.show()

```


![png](output_13_0.png)


    CPU times: user 1.25 s, sys: 107 ms, total: 1.36 s
    Wall time: 717 ms


Above are plots of languages used on the platform and country destination, respectively. Both plots are scaled logarithmically for readability because the dominant classes far outnumbered the rest. With regard to the language counts, English outnumbered the other classes greatly; and with regard to the country destinations, ‘NDF’ and the US outnumbered the other classes. NDF represents the class of users that haven’t booked a destination yet. Since this would be the class that the model being built would be predicting, this was not be used as an outcome variable to train the model.


```python
%%time

## Signup Method Frequencies Within DataFrame

df.signup_method.value_counts()
```

    CPU times: user 25.5 ms, sys: 908 µs, total: 26.4 ms
    Wall time: 25.7 ms





    basic       152897
    facebook     60008
    google         546
    Name: signup_method, dtype: int64





The vast majority of users signed onto Airbnb through the platform itsself.


```python
%%time

## Lineplots of Initial Activities on the Platform

# Frequency of Accounts Created on Spicific Dates
fig, ax = plt.subplots()
fig.set_size_inches(14, 5)
df.date_account_created.value_counts().plot(kind='line', title="Date Account Created", linewidth=1.1, color='#1F618D')
sns.despine()
plt.show()

# Frequency of First Bookings Made on Specific Dates
fig, ax = plt.subplots()
fig.set_size_inches(14, 5)
df.date_first_booking.value_counts().plot(kind='line',  title="Date First Booking", linewidth=1.1, color='#1F618D')
sns.despine()
plt.show()
```


![png](output_18_0.png)



![png](output_18_1.png)


    CPU times: user 1.43 s, sys: 162 ms, total: 1.59 s
    Wall time: 582 ms


The above plots refer to the frequencies the accounts were first created and frequencies bookings were made over the course of multiple years. The frequency of accounts being created showed an increasing trajectory over the course of five years, likely reflecting an increase in the userbase of Airbnb. There is a sharp drop in bookings made around the time that the data was collected. This doesn’t reflect a drop in the usage of the platform, but rather people who use the platform but haven’t made a booking yet (these customers would have the country destination label ‘NDF’).


```python
%%time

## Graphing Seasonal Activity   

fig = plt.figure(figsize=(14, 8))
fig.subplots_adjust(hspace=.3)

# Barplot of Monthly Usage
plt.subplot(2, 2, 1)
ax = df.month_account_created.value_counts().sort_index().plot(kind='bar',  

                                    title="Frequency of Accounts Created Each Month")
ax.set_xlabel("Month")
ax.set_ylabel("Frequency")
plt.xticks(rotation=80)

# Barplot of Usage During Week
plt.subplot(2, 2, 2)
ax = df.date_account_created.apply(lambda x: x.weekday()).value_counts().sort_index().plot(kind='bar',
                                    title="Frequency of Accounts Created Each Day of the Week")
ax.set_xlabel("Week Day")
ax.set_ylabel("Frequency")

# Line Graph of Usage over the Year
plt.subplot(2, 2, 3)
ax = df.date_account_created.apply(lambda x: x.dayofyear).value_counts().sort_index().plot(kind='line',
                                    title="Frequency of Accounts Created Each Day of the Year")
ax.set_xlabel("Day of the Year")
ax.set_ylabel("Frequency")

# Line Graph of First Activity Over a Day (24 hour scale)
plt.subplot(2, 2, 4)
ax = df.hour_first_active.value_counts().sort_index().plot(kind='line',
                                    title="Hour of Day First Active")
ax.set_xlabel("Hour of the Day (Using 24 Hour Clock)")
ax.set_ylabel("Frequency")

plt.show()
```


![png](output_20_0.png)


    CPU times: user 2.85 s, sys: 157 ms, total: 3.01 s
    Wall time: 2.23 s


The above plots reflect the frequencies use of the platform created across different timespans.
There’s a notable drop of accounts created between June and July. Since summer is a season that is popular for travel, people are less likely to need accounts during this time (because they would’ve presumably made accounts earlier than when they would travel using the service). There is a slight drop of accounts made over the weekends as well. Initial activity drops during the day which is likely the result of people having less time during the average US workday to be on Airbnb.



```python
%%time

## Creating Scatterplot Matrix with Sample of Original DataFrame

# Store Samples of DataFrame for Visualization
df_sample = df.iloc[:,30:].dropna().sample(100)

# Declare Pairgrid
g = sns.PairGrid(df_sample.dropna(), diag_sharey=False)

# Scatterplot
g.map_upper(plt.scatter, alpha=.3)

# Fit Line
g.map_lower(sns.regplot, scatter_kws=dict(alpha=0))

# KDE Plot
g.map_diag(sns.kdeplot, lw=3)
plt.show()
```


![png](output_22_0.png)


    CPU times: user 54.4 s, sys: 5.54 s, total: 59.9 s
    Wall time: 28 s


The scatterplot matrix gives information about the relationship between specific engineered features.


```python
%%time

## Visualizing the Correlatedness of the Session variables

fig, ax = plt.subplots(figsize=(8,5))         
sns.heatmap(df.iloc[:,30:].dropna().sample(100).corr(), cmap='RdBu_r', center=0)
plt.show()
```


![png](output_24_0.png)


    CPU times: user 1.14 s, sys: 178 ms, total: 1.31 s
    Wall time: 542 ms


The heatmap is meant to gauge the correlatedness of engineered features. There is much correlatedness among these features, which is expected since they reflect amounts of time spent on the platform.


```python
%%time

## Descriptive Statistics

df.iloc[:,30:].describe()
```

    CPU times: user 172 ms, sys: 25.9 ms, total: 198 ms
    Wall time: 125 ms





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>secs_elapsed</th>
      <th>view_search_results_count</th>
      <th>p3_count</th>
      <th>wishlist_content_update_count</th>
      <th>view_search_results_secs</th>
      <th>p3_secs</th>
      <th>wishlist_content_update_secs</th>
      <th>user_profile_secs</th>
      <th>change_trip_characteristics_secs</th>
      <th>similar_listings_secs</th>
      <th>user_social_connections_secs</th>
      <th>update_listing_secs</th>
      <th>listing_reviews_secs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1514234.959</td>
      <td>12.366</td>
      <td>8.265</td>
      <td>6.093</td>
      <td>276234.787</td>
      <td>208080.601</td>
      <td>41784.183</td>
      <td>27568.269</td>
      <td>21228.880</td>
      <td>7907.491</td>
      <td>16065.679</td>
      <td>67370.556</td>
      <td>8924.165</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1913191.475</td>
      <td>28.446</td>
      <td>20.404</td>
      <td>13.310</td>
      <td>555234.812</td>
      <td>539892.590</td>
      <td>153573.994</td>
      <td>120118.685</td>
      <td>99799.784</td>
      <td>50358.126</td>
      <td>91532.842</td>
      <td>273893.662</td>
      <td>56216.593</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>256920.500</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>872862.000</td>
      <td>2.000</td>
      <td>2.000</td>
      <td>1.000</td>
      <td>46453.000</td>
      <td>17676.000</td>
      <td>996.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2043487.500</td>
      <td>13.000</td>
      <td>9.000</td>
      <td>6.000</td>
      <td>296887.500</td>
      <td>151894.000</td>
      <td>16313.000</td>
      <td>3755.000</td>
      <td>3976.500</td>
      <td>1521.500</td>
      <td>0.000</td>
      <td>4857.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>38221363.000</td>
      <td>987.000</td>
      <td>1131.000</td>
      <td>524.000</td>
      <td>10194287.000</td>
      <td>15450650.000</td>
      <td>9160217.000</td>
      <td>4881477.000</td>
      <td>4871979.000</td>
      <td>1894503.000</td>
      <td>3128228.000</td>
      <td>11370071.000</td>
      <td>2464683.000</td>
    </tr>
  </tbody>
</table>
</div>



Data will be normalized prior to modeling to account for the vast difference in scale of the variables.

## Preparing The Data For Modeling

To prepare the data for modeling, values were imputed, the data was resampled to address the class imbalance in the outcome, and multiple forms of feature reduction were implemented.
This resulted in four sets of variables: One reflecting all of the unaltered features of the dataset, one reflecting PCA components, one reflecting features chosen by the selectKbest function, and encoded for deep learning.


```python
%%time

## Creating Modeling DataFrame

# Feature Selection
train_feature_names = ['language', 'age','gender','first_device_type',
                               'signup_app','affiliate_provider','affiliate_channel',
                               'month_first_active','signup_method','day_account_created',
                               'month_account_created','country_destination']

df = df[train_feature_names + session_feature_names]

# Keep Records with Session Features
df = df[df['secs_elapsed'].notnull()]

print(np.array(df.columns))
```

    ['language' 'age' 'gender' 'first_device_type' 'signup_app'
     'affiliate_provider' 'affiliate_channel' 'month_first_active'
     'signup_method' 'day_account_created' 'month_account_created'
     'country_destination' 'secs_elapsed' 'view_search_results_count'
     'p3_count' 'wishlist_content_update_count' 'view_search_results_secs'
     'p3_secs' 'wishlist_content_update_secs' 'user_profile_secs'
     'change_trip_characteristics_secs' 'similar_listings_secs'
     'user_social_connections_secs' 'update_listing_secs'
     'listing_reviews_secs']
    CPU times: user 37.6 ms, sys: 15.1 ms, total: 52.6 ms
    Wall time: 51.1 ms



```python
%%time

## Impute Ages with Group Means

df['age'] = df.groupby([ 'signup_method','country_destination', 'first_device_type'])['age'].transform(lambda x: x.fillna(x.mean()))
```

    CPU times: user 327 ms, sys: 17 ms, total: 344 ms
    Wall time: 346 ms



```python
%%time

## Creating DataFrame for Imputation of Gender

df_impute_gender = df.dropna()

## Class Balancing and Resampling Imputation DataFrame for Random Forest Imputation

records_per_class = 18000

# Separate Majority and Minority Classes
df_majority = df_impute_gender[df_impute_gender.gender=='female']
df_minority = df_impute_gender[df_impute_gender.gender=='male']

# Upsample Minority Class
df_minority_upsampled = resample(df_minority,
                                 replace=True,     
                                 n_samples=records_per_class,    
                                 random_state=123)

# Downsample Majority Class
df_majority_downsampled = resample(df_majority,
                                 replace=False,    
                                 n_samples=records_per_class,     
                                 random_state=123)

# Combine Downsampled Majority Class With Upsampled Minority Class
df_impute_gender = pd.concat([df_majority_downsampled, df_minority_upsampled])

print(df_impute_gender.gender.value_counts())
```

    female    18000
    male      18000
    Name: gender, dtype: int64
    CPU times: user 96.4 ms, sys: 16.7 ms, total: 113 ms
    Wall time: 112 ms



```python
%%time

## Random Forest Imputation

# Create Gender Column for Imputation Using Random Forests
x = pd.get_dummies(df_impute_gender.drop(['gender', 'age','signup_method','affiliate_channel','first_device_type','signup_app','affiliate_provider', 'language'], axis=1))
y = df_impute_gender['gender']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)

rfc = ensemble.RandomForestClassifier().fit(x_train,  y_train)

imputed_gender = rfc.predict(pd.get_dummies(df.drop(['gender', 'age','signup_method','affiliate_channel','first_device_type','signup_app','affiliate_provider', 'language'],1)))

# Imputing Gender with Predicted Values
df['gender'] = np.where(df['gender'].isnull(), imputed_gender, df['gender'])

# Shedding Remaining Null Values
df = df.dropna()
```

    CPU times: user 922 ms, sys: 49 ms, total: 971 ms
    Wall time: 974 ms



```python
%%time

## Class Balancing and Resampling of Training DataFrame

country_names = df.country_destination.unique()[df.country_destination.unique() != 'NDF']
records_per_class = 9000

df_resampled = pd.DataFrame()
for country in country_names:
    if len(df[df['country_destination'] == country]) >= records_per_class:
        df_resample_1 = df[df['country_destination'] == country]
        df_resample_1 = resample(df_resample_1, replace=False, n_samples=records_per_class, random_state=123)
        df_resampled = df_resampled.append(df_resample_1)
    else:
        df_resample_2 = df[df['country_destination'] == country]
        df_resample_2 = resample(df_resample_2, replace=True, n_samples=records_per_class, random_state=123)
        df_resampled = df_resampled.append(df_resample_2)


print(df_resampled.country_destination.value_counts())
```

    AU       9000
    PT       9000
    FR       9000
    NL       9000
    ES       9000
    IT       9000
    GB       9000
    other    9000
    CA       9000
    DE       9000
    US       9000
    Name: country_destination, dtype: int64
    CPU times: user 384 ms, sys: 84.9 ms, total: 469 ms
    Wall time: 470 ms



```python
%%time

## Establish DataFrame to be Used for Modeling

df = df_resampled

# Get Dummies so Categorical Variables Can be Used in Classification
df = pd.get_dummies(df.drop(['country_destination'], axis=1))

# Display New Number of Features
print('Number of Features: '+ str(len(df.columns)))

```

    Number of Features: 75
    CPU times: user 96.4 ms, sys: 29 ms, total: 125 ms
    Wall time: 124 ms



```python
%%time

## Normalize DataFrame

df = ((df-df.min())/(df.max()-df.min()))

## Creating a Numeric Version of the Outcome Variable

df_resampled['country_destination_categorical'] = df_resampled['country_destination'].astype('category').cat.codes

```

    CPU times: user 328 ms, sys: 169 ms, total: 497 ms
    Wall time: 496 ms



```python
%%time

## Establish Feature and Outcome Variables to be Used for Modeling Based on Original Features

x = df
y = df_resampled['country_destination']

# This is a Version of the Outcome Variable Encoded for Deep Learning
Y = keras.utils.to_categorical(df_resampled['country_destination_categorical'], len(country_names))

```

    CPU times: user 2.84 ms, sys: 1.89 ms, total: 4.73 ms
    Wall time: 2.86 ms



```python
%%time

## PCA

# Normalize the Data for PCA
X = StandardScaler().fit_transform(x)

# Perform PCA
sklearn_pca = PCA(n_components=15)
Y_sklearn = sklearn_pca.fit_transform(X)

# Turn PCA Result into a Dataframe
pca_components = pd.DataFrame(data=Y_sklearn)

print(
    'The percentage of total variance in the dataset explained by each',
    'component from Sklearn PCA.\n',
    sklearn_pca.explained_variance_ratio_
)

```

    The percentage of total variance in the dataset explained by each component from Sklearn PCA.
     [0.06670299 0.05382638 0.04211878 0.03175155 0.03026788 0.02698215
     0.02641819 0.02428458 0.02173672 0.02109345 0.02058062 0.01807279
     0.01559654 0.0151019  0.01473852]
    CPU times: user 3 s, sys: 771 ms, total: 3.77 s
    Wall time: 791 ms



```python
%%time

## Establish variables based on select K best to be used for modeling

selector = SelectKBest(f_classif, k=60)
k_predictors = selector.fit_transform(x,y)

```

    CPU times: user 196 ms, sys: 70.6 ms, total: 267 ms
    Wall time: 268 ms



```python
%%time

## Train Test Split the Four Sets of Feature and Outcome Variables

# Unaltered Features
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)

# PCA
px_train, px_test, py_train, py_test = train_test_split(pca_components, y, test_size=0.2, random_state=22)

# Select K Best
kx_train, kx_test, ky_train, ky_test = train_test_split(k_predictors, y, test_size=0.2, random_state=21)

# Deep Learning
X_train, X_test, Y_train, Y_test = train_test_split(np.asarray(x), np.asarray(Y), test_size=0.2, random_state=20)

```

    CPU times: user 201 ms, sys: 92.4 ms, total: 294 ms
    Wall time: 294 ms


Training and testing sets of four variables were generated to be used in modeling.
The x and y variables represent the variables to be used for modeling that reflect the all of the useful features of the data.
The px and py variables represent the variables to be used for modeling that reflect PCA components of the initial features.
The kx and ky variables represent the variables to be used for modeling that reflect features chosen by selectKbest.
The X and Y variables represent the variables converted to arrays to be used for deep learning.

## Unsupervised Clustering Analysis

Clustering was done on the unaltered features. Country destination labels were dropped prior to clustering so the records would be clustered based on their contents as opposed to the pre-generated label.

### Selecting Appropriate Clustering Method

Three methods of clustering were attempted: mean shift, affinity propogation, and k means. The appropriate clustering method was selected based on the number of clusters generated by a particular method and sillhouette scores.


```python
%%time

## Mean Shift

bandwidth = 2*estimate_bandwidth(x_train, quantile=0.2, n_samples=500)

# Declare and Fit the Model
ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(x_train)

# Extract Cluster Assignments for Each Data Point
labels = ms.labels_

# Coordinates of the Cluster Centers
cluster_centers = ms.cluster_centers_

# Count our clusters
n_clusters_ = len(np.unique(labels))

print("Number of estimated Mean Shift clusters: {}\n".format(n_clusters_))
```

    Number of estimated Mean Shift clusters: 1

    CPU times: user 5.4 s, sys: 64.8 ms, total: 5.47 s
    Wall time: 5.47 s


Mean shift returned a single cluster and was deemed unfil for analysis.


```python
%%time

## Affinity Propogation

af = AffinityPropagation().fit(x_train.sample(10000))

# Extract Cluster Assignments for Each Data Point
labels = af.labels_

# Count our Clusters
n_clusters_ = len(np.unique(labels))

print("Number of estimated Affinity Propagation clusters: {}\n".format(n_clusters_))
```

    Number of estimated Affinity Propagation clusters: 430

    CPU times: user 6min 40s, sys: 6 s, total: 6min 46s
    Wall time: 6min 50s


Affinity propogation proved to be computationaly intensive and returned 430 groups. This was also unfit for analysis.


```python
%%time

## K Means

# Testing Multiple Numbers of Clusters
print('Silhouette scores for K Means:\n')
range_n_clusters = [9,10,11,12]
for n_clusters in range_n_clusters:
    print(str(n_clusters) + ' clusters: ' + str(silhouette_score(x_train, KMeans(n_clusters=n_clusters, random_state=42).fit_predict(x_train))))
print('\n')
```

    Silhouette scores for K Means:

    9 clusters: 0.25633171822577977
    10 clusters: 0.2552001710517822
    11 clusters: 0.25648440346380796
    12 clusters: 0.2710002047276309


    CPU times: user 9min 40s, sys: 1min 20s, total: 11min
    Wall time: 6min 54s


K means yielded reasonable silhouette scores for the selected numbers of groups. K means with 11 clusters (the number of classes in the outcome variable) was chosen for further analysis.

### Analyzing Clusters

K-means generated labels were compared to the original country labels to analyze the ability of K-means to group the records based on their content.


```python
%%time

## Calculate Predicted Values.

number_of_clusters = 11
y_pred = KMeans(n_clusters=number_of_clusters, random_state=42).fit_predict(x_train)
```

    CPU times: user 9.26 s, sys: 986 ms, total: 10.2 s
    Wall time: 4.02 s



```python
%%time

## Create A Dataframe Using The Original Dataset and New Labels

df_kmeans = pd.concat([x_train,y_train], axis=1)
df_kmeans['labels'] = y_pred

## Split Results by New Labels

df_kmeans_list = []
for cluster in range(number_of_clusters):
    df_kmeans_list.append(df_kmeans.loc[df_kmeans['labels'] == cluster])

```

    CPU times: user 62.4 ms, sys: 18.1 ms, total: 80.5 ms
    Wall time: 86.2 ms



```python
%%time

## Create DataFrame to Graph Clusters

df_kmeans_country_names = []
for names in country_names:
  df_kmeans_countries_per_cluster = []
  for group in df_kmeans_list:
    df_kmeans_countries_per_cluster.append(len(group['country_destination'].loc[group['country_destination'] == names]))
  df_kmeans_country_names.append(df_kmeans_countries_per_cluster)

cluster_names =[]
for number in range(number_of_clusters):
    cluster_names.append('Cluster ' + str(number))

countries = {}
for index_number in list(range(0,10)):
    countries[country_names[index_number]] = df_kmeans_country_names[index_number]

df_countries_by_cluster = pd.DataFrame(countries, index = cluster_names)
```

    CPU times: user 117 ms, sys: 3.24 ms, total: 121 ms
    Wall time: 123 ms



```python
%%time

## Bar Graph for Countries

ax = df_countries_by_cluster.plot(kind='bar', title ="Countries by Cluster", figsize=(16, 8))
ax.legend(loc='lower left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel("Frequency")
plt.show()
```


![png](output_53_0.png)


    CPU times: user 1.05 s, sys: 91.4 ms, total: 1.14 s
    Wall time: 673 ms


While clustering didn't prove to be a reliable means of separating country destination classes, it is worth mentioning that the sizes of the cluster and sillhouette scores imply that there is a tangible metric that k means is using to separate clusters.

## Supervised Modeling using Unaltered Features

### Naive Bayes


```python
%%time

## Train and Fit Model

bnb = BernoulliNB().fit(x_train, y_train)

```

    CPU times: user 407 ms, sys: 18.6 ms, total: 426 ms
    Wall time: 382 ms



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(bnb.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(bnb, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, bnb.predict(x_test)))+'\n')

print(classification_report(y_test, bnb.predict(x_test)))

```

    accuracy score:
    0.14752525252525253

    cross validation:
    [0.15792129 0.13979309 0.1469697  0.14246022 0.14744562]

    confusion matrix:
    [[469  85 347  52  33 126 277  26 173 217  34]
     [223 157 214  39  53  90 471  39 265 271  33]
     [278 100 504  15  45  37 340  71 180 191  26]
     [195  82 222  68  75  76 481  43 256 275  41]
     [204 115 225  68  85  99 505  38 241 229  32]
     [239  87 214  40  74 125 477  31 221 249  25]
     [177  91 229  32  66  67 497  38 265 296  31]
     [330  63 178  44  90  29 451  72 241 263  33]
     [209  87 234   0  73  92 347  48 478 191  20]
     [237 111 214  44  45  47 341  20 237 386  46]
     [271  98 308  41  55  33 331  38 229 308  80]]

                  precision    recall  f1-score   support

              AU       0.17      0.26      0.20      1839
              CA       0.15      0.08      0.11      1855
              DE       0.17      0.28      0.22      1787
              ES       0.15      0.04      0.06      1814
              FR       0.12      0.05      0.07      1841
              GB       0.15      0.07      0.10      1782
              IT       0.11      0.28      0.16      1789
              NL       0.16      0.04      0.06      1794
              PT       0.17      0.27      0.21      1779
              US       0.13      0.22      0.17      1728
           other       0.20      0.04      0.07      1792

       micro avg       0.15      0.15      0.15     19800
       macro avg       0.15      0.15      0.13     19800
    weighted avg       0.15      0.15      0.13     19800

    CPU times: user 2.61 s, sys: 350 ms, total: 2.96 s
    Wall time: 929 ms


### K Nearest Neighbors


```python
%%time

## Train and Fit Model

knn = neighbors.KNeighborsClassifier(n_neighbors=2).fit(x_train, y_train)

```

    CPU times: user 3.62 s, sys: 27.9 ms, total: 3.65 s
    Wall time: 3.65 s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(knn.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(knn, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, knn.predict(x_test)))+'\n')

print(classification_report(y_test, knn.predict(x_test)))

```

    accuracy score:
    0.9134343434343435

    cross validation:
    [0.75327952 0.74615191 0.74217172 0.73755999 0.74102175]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   2    4    1    0 1820    5    4    0    0    4    1]
     [   0    0    0    0    0 1782    0    0    0    0    0]
     [   0    2    0    2    5    0 1780    0    0    0    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  38   63   34  126  220  120  169   42   12  651  253]
     [   8   31   18   33   75   42   39   10    7  344 1185]]

                  precision    recall  f1-score   support

              AU       0.97      1.00      0.99      1839
              CA       0.95      1.00      0.97      1855
              DE       0.97      1.00      0.99      1787
              ES       0.92      1.00      0.96      1814
              FR       0.86      0.99      0.92      1841
              GB       0.91      1.00      0.96      1782
              IT       0.89      0.99      0.94      1789
              NL       0.97      1.00      0.99      1794
              PT       0.99      1.00      0.99      1779
              US       0.65      0.38      0.48      1728
           other       0.82      0.66      0.73      1792

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800

    CPU times: user 50.9 s, sys: 225 ms, total: 51.1 s
    Wall time: 51.4 s


### Decision Tree


```python
%%time

## Train and Fit Model

dt = tree.DecisionTreeClassifier()

parameters = {
              'max_features': list(range(8,15)),
              'max_depth': list(range(24,34))
             }

acc_scorer = make_scorer(accuracy_score)

decision_tree = GridSearchCV(dt, parameters, scoring=acc_scorer).fit(x_train,  y_train)

print(decision_tree.best_params_)

```

    {'max_depth': 33, 'max_features': 14}
    CPU times: user 1min 11s, sys: 3.75 s, total: 1min 15s
    Wall time: 1min 15s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(decision_tree.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(decision_tree, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, decision_tree.predict(x_test)))+'\n')

print(classification_report(y_test, decision_tree.predict(x_test)))

```

    accuracy score:
    0.9483333333333334

    cross validation:
    [0.82971746 0.83068383 0.81212121 0.82242991 0.81942337]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    5    0    1 1816    3    4    0    0    6    6]
     [   0    0    0    0    0 1781    0    0    1    0    0]
     [   0    0    0    0    0    1 1786    0    0    2    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  24   44   26   95  129   71  100   35   11  890  303]
     [   4    3    4    6   21   11   16    4    3   84 1636]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1839
              CA       0.97      1.00      0.99      1855
              DE       0.98      1.00      0.99      1787
              ES       0.95      1.00      0.97      1814
              FR       0.92      0.99      0.95      1841
              GB       0.95      1.00      0.98      1782
              IT       0.94      1.00      0.97      1789
              NL       0.98      1.00      0.99      1794
              PT       0.99      1.00      1.00      1779
              US       0.91      0.52      0.66      1728
           other       0.84      0.91      0.88      1792

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.95      0.95      0.94     19800
    weighted avg       0.95      0.95      0.94     19800

    CPU times: user 1min 12s, sys: 1.68 s, total: 1min 14s
    Wall time: 1min 14s


### Random Forest


```python
%%time

## Train and Fit Model

rf = ensemble.RandomForestClassifier()

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(x_train,  y_train)

print(rfc.best_params_)
```

    {'max_depth': 95, 'max_features': 'sqrt'}
    CPU times: user 1min 5s, sys: 3.15 s, total: 1min 9s
    Wall time: 1min 9s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(rfc.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(rfc, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, rfc.predict(x_test)))+'\n')

print(classification_report(y_test, rfc.predict(x_test)))

```

    accuracy score:
    0.9621717171717171

    cross validation:
    [0.8259334  0.81402978 0.82222222 0.81586259 0.81335357]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    0    0    0 1830    0    1    0    0    7    3]
     [   0    0    0    0    0 1782    0    0    0    0    0]
     [   0    0    0    0    0    0 1787    0    0    2    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  10   22   16   53  108   41   65    5    0 1160  248]
     [   3    2    0    6   25    7    5    3    0  117 1624]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      1.00      1839
              CA       0.99      1.00      0.99      1855
              DE       0.99      1.00      1.00      1787
              ES       0.97      1.00      0.98      1814
              FR       0.93      0.99      0.96      1841
              GB       0.97      1.00      0.99      1782
              IT       0.96      1.00      0.98      1789
              NL       1.00      1.00      1.00      1794
              PT       1.00      1.00      1.00      1779
              US       0.90      0.67      0.77      1728
           other       0.87      0.91      0.89      1792

       micro avg       0.96      0.96      0.96     19800
       macro avg       0.96      0.96      0.96     19800
    weighted avg       0.96      0.96      0.96     19800

    CPU times: user 1min 8s, sys: 3.45 s, total: 1min 11s
    Wall time: 1min 12s


### Logistic Regression


```python
%%time

## Train and Fit Model

lr = LogisticRegression(penalty='l2').fit(x_train, y_train)

```

    CPU times: user 21 s, sys: 136 ms, total: 21.1 s
    Wall time: 21.3 s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(lr.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(lr, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, lr.predict(x_test)))+'\n')

print(classification_report(y_test, lr.predict(x_test)))

```

    accuracy score:
    0.1812121212121212

    cross validation:
    [0.1765893  0.17865254 0.1760101  0.1750442  0.18285281]

    confusion matrix:
    [[658 164  88 120  41 179  73 163 219  91  43]
     [274 385 112 119  60 234  85 167 253 128  38]
     [387 192 282  89  42 158  66 189 253  89  40]
     [262 208  96 272  80 148 120 166 258 161  43]
     [285 247 110 156 121 276 121 172 230  79  44]
     [318 154  81 151  60 421  97 165 225 100  10]
     [259 190 111 134 110 227 177 153 283 112  33]
     [269 176 167 192  93 144  52 380 178 115  28]
     [295 222 103  41  62 158 122 156 582  18  20]
     [300 208 100 159  48 172  75 137 255 216  58]
     [404 217 125 113  65 172  75 172 226 129  94]]

                  precision    recall  f1-score   support

              AU       0.18      0.36      0.24      1839
              CA       0.16      0.21      0.18      1855
              DE       0.21      0.16      0.18      1787
              ES       0.18      0.15      0.16      1814
              FR       0.15      0.07      0.09      1841
              GB       0.18      0.24      0.21      1782
              IT       0.17      0.10      0.12      1789
              NL       0.19      0.21      0.20      1794
              PT       0.20      0.33      0.25      1779
              US       0.17      0.12      0.15      1728
           other       0.21      0.05      0.08      1792

       micro avg       0.18      0.18      0.18     19800
       macro avg       0.18      0.18      0.17     19800
    weighted avg       0.18      0.18      0.17     19800

    CPU times: user 11.9 s, sys: 481 ms, total: 12.4 s
    Wall time: 10 s


### Gradient Boost


```python
%%time

## Train and Fit Model

cl = ensemble.GradientBoostingClassifier()

parameters = {
              'n_estimators': list(np.arange(250, 301, 50)),
              'max_depth': list(range(3,5))
             }

acc_scorer = make_scorer(accuracy_score)

clf = GridSearchCV(cl, parameters, scoring=acc_scorer).fit(x_train,  y_train)

print(clf.best_params_)

```

    {'max_depth': 4, 'n_estimators': 300}
    CPU times: user 1h 37min 32s, sys: 2min 17s, total: 1h 39min 49s
    Wall time: 1h 42min 18s



```python
%%time

## Running the Model with Best Parameters for Faster Evaluation

clf = ensemble.GradientBoostingClassifier(max_depth=clf.best_params_['max_depth'],n_estimators=clf.best_params_['n_estimators'] ).fit(x_train,  y_train)
```

    CPU times: user 13min 57s, sys: 17.8 s, total: 14min 15s
    Wall time: 14min 18s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(clf.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(clf, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, clf.predict(x_test)))+'\n')

print(classification_report(y_test, clf.predict(x_test)))

```

    accuracy score:
    0.8687373737373737

    cross validation:
    [0.82240161 0.81554378 0.81363636 0.81687295 0.81183612]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   2 1777    2    6   15    9    0    3    0   32    9]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   5    8   10 1658   19   26    6   12    0   60   10]
     [  11   38   17   29 1437   78   31   25    0  135   40]
     [   3    8    6   11   12 1619    7   18    6   81   11]
     [  18   28    9   21   23   32 1531   18    2   81   26]
     [   8    0    0    0    0    0    0 1783    0    3    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  33   76   43  116  124  129   79   50   14  841  223]
     [  26   49   29   71   74   82   41   28    0  242 1150]]

                  precision    recall  f1-score   support

              AU       0.95      1.00      0.97      1839
              CA       0.90      0.96      0.93      1855
              DE       0.94      1.00      0.97      1787
              ES       0.87      0.91      0.89      1814
              FR       0.84      0.78      0.81      1841
              GB       0.82      0.91      0.86      1782
              IT       0.90      0.86      0.88      1789
              NL       0.92      0.99      0.96      1794
              PT       0.99      1.00      0.99      1779
              US       0.57      0.49      0.53      1728
           other       0.78      0.64      0.71      1792

       micro avg       0.87      0.87      0.87     19800
       macro avg       0.86      0.87      0.86     19800
    weighted avg       0.86      0.87      0.86     19800

    CPU times: user 12min 19s, sys: 5.24 s, total: 12min 24s
    Wall time: 12min 26s


### MLP Neural Network


```python
%%time

## Train and Fit Model

mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,)).fit(x_train, y_train)
```

    CPU times: user 32min 23s, sys: 2min 47s, total: 35min 11s
    Wall time: 8min 55s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(mlp.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(mlp, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, mlp.predict(x_test)))+'\n')

print(classification_report(y_test, mlp.predict(x_test)))
```

    accuracy score:
    0.8148989898989899

    cross validation:
    [0.62916246 0.64168559 0.6290404  0.64561758 0.63884674]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1783    0    0   17   10   12    6    0   24    3]
     [   0    8 1768    0    0    0    6    0    0    5    0]
     [   0   35    2 1395   57   22   81   14    6  141   61]
     [   4   37   10   10 1279   51   89   16    0  234  111]
     [   0   13    0    0   33 1655   27    8    0   36   10]
     [   5   24    6   14   34   33 1549    3    0   79   42]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  27   91   51   78  218  146  194   60   13  517  333]
     [  10   71   19   36  157   57  133   38    5  489  777]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1839
              CA       0.86      0.96      0.91      1855
              DE       0.95      0.99      0.97      1787
              ES       0.91      0.77      0.83      1814
              FR       0.71      0.69      0.70      1841
              GB       0.84      0.93      0.88      1782
              IT       0.74      0.87      0.80      1789
              NL       0.93      1.00      0.96      1794
              PT       0.99      1.00      0.99      1779
              US       0.34      0.30      0.32      1728
           other       0.58      0.43      0.50      1792

       micro avg       0.81      0.81      0.81     19800
       macro avg       0.80      0.81      0.80     19800
    weighted avg       0.80      0.81      0.81     19800

    CPU times: user 15min 13s, sys: 1min 25s, total: 16min 39s
    Wall time: 4min 12s


Naive bayes and logistic regression had accuracies that were extremely low but substantially above random chance. This implies that the models were run correctly but these model types are not suited for the data. K nearest neighbors had an accuracy score over 90% but substantially lower cross validation scores. The decision tree and random forest both had high accuracy scores but the random forest was less prone to overfitting. The gradient boost and neural networks had middling accuracy scores, but took much longer than the other model types to attain their results.

The models that relied on the dataset’s unreduced features, in general, had the best accuracy in the study. An advantage of using all of the useful features is that as much meaningful variance was captured by the models as possible. A downside to this type of feature preparation that did stand out is the lack of efficiency. Since feature reduction didn’t take place with these models, their performance suffered and they had the longest runtimes. This method of feature selection also risks including features with variance that doesn’t aid in the predictive power of the models. However, this potential disadvantage didn’t hamper the model’s ability to perform well because many of the features that would noticeably have a negative effect on the models were already left out.



## Modeling the Data using PCA Components

### Naive Bayes


```python
%%time

## Train and Fit Model

p_bnb = BernoulliNB().fit(px_train, py_train)

```

    CPU times: user 348 ms, sys: 37.1 ms, total: 385 ms
    Wall time: 403 ms



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_bnb.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_bnb, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_bnb.predict(px_test)))+'\n')

print(classification_report(py_test, p_bnb.predict(px_test)))

```

    accuracy score:
    0.14156565656565656

    cross validation:
    [0.13269425 0.12875536 0.13632921 0.13393985 0.13115997]

    confusion matrix:
    [[526 222 137  52  35 142 114 249  65 164  64]
     [173 485  84  59  56 105 135 366 117 143  52]
     [368 306 230  19  35  65  63 327 140 171  37]
     [310 349 122  72  60 110 166 293 106 192  51]
     [253 352 132  81  83 143 189 307  90 184  36]
     [261 259 171  67  68 149 186 332  93 161  41]
     [232 323 168  52  86 126 229 321 103 178  15]
     [272 303 155  36  70  99 124 474  77 160  36]
     [259 385  71  42   0 165 162 316 215 162  21]
     [333 293 149  83  54  78 119 258  94 261  53]
     [355 293 181  70  48  75 126 262  95 229  79]]

                  precision    recall  f1-score   support

              AU       0.16      0.30      0.21      1770
              CA       0.14      0.27      0.18      1775
              DE       0.14      0.13      0.14      1761
              ES       0.11      0.04      0.06      1831
              FR       0.14      0.04      0.07      1850
              GB       0.12      0.08      0.10      1788
              IT       0.14      0.12      0.13      1833
              NL       0.14      0.26      0.18      1806
              PT       0.18      0.12      0.14      1798
              US       0.13      0.15      0.14      1775
           other       0.16      0.04      0.07      1813

       micro avg       0.14      0.14      0.14     19800
       macro avg       0.14      0.14      0.13     19800
    weighted avg       0.14      0.14      0.13     19800

    CPU times: user 2.33 s, sys: 253 ms, total: 2.58 s
    Wall time: 796 ms


### K Nearest Neighbors


```python
%%time

## Train and Fit Model

p_knn = neighbors.KNeighborsClassifier(n_neighbors=2).fit(px_train, py_train)

```

    CPU times: user 213 ms, sys: 9.34 ms, total: 222 ms
    Wall time: 224 ms



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_knn.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_knn, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_knn.predict(px_test)))+'\n')

print(classification_report(py_test, p_knn.predict(px_test)))

```

    accuracy score:
    0.9134848484848485

    cross validation:
    [0.75227043 0.74577127 0.74046958 0.74500885 0.74222896]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1831    0    0    0    0    0    0    0]
     [   0    1    2    2 1824    0    4    0    0   15    2]
     [   0    0    0    0    3 1785    0    0    0    0    0]
     [   0    4    0    0    0    0 1829    0    0    0    0]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  32   69   38  115  269  117  155   46    9  672  253]
     [   2   24    9   40   81   47   50   13    3  308 1236]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1770
              CA       0.95      1.00      0.97      1775
              DE       0.97      1.00      0.99      1761
              ES       0.92      1.00      0.96      1831
              FR       0.84      0.99      0.91      1850
              GB       0.92      1.00      0.96      1788
              IT       0.90      1.00      0.94      1833
              NL       0.97      1.00      0.98      1806
              PT       0.99      1.00      1.00      1798
              US       0.68      0.38      0.49      1775
           other       0.83      0.68      0.75      1813

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800

    CPU times: user 4.22 s, sys: 29.4 ms, total: 4.25 s
    Wall time: 4.25 s


### Decision Tree


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': list(range(8,15)),
              'max_depth': list(range(24,34))
             }

acc_scorer = make_scorer(accuracy_score)

p_decision_tree = GridSearchCV(dt, parameters, scoring=acc_scorer).fit(px_train,  py_train)

print(p_decision_tree.best_params_)

```

    {'max_depth': 33, 'max_features': 8}
    CPU times: user 2min 48s, sys: 1.43 s, total: 2min 50s
    Wall time: 2min 50s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_decision_tree.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_decision_tree, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_decision_tree.predict(px_test)))+'\n')

print(classification_report(py_test, p_decision_tree.predict(px_test)))

```

    accuracy score:
    0.914949494949495

    cross validation:
    [0.80171544 0.80888664 0.80358495 0.8018701  0.8033864 ]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1726    5    0   30    2   12    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0   12    0 1772   17    8    8    0    0    6    8]
     [   0   37    3   11 1709   13   32    4    0   28   13]
     [   0   14    0    0   18 1729   16    0    0    3    8]
     [   0   19    0    1   18    7 1782    0    0    3    3]
     [   0    0    0    0    8    0    0 1798    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  17   82   34   87  175   82  135   31   14  777  341]
     [   1   21    5   18   66   37   44    4    0  123 1494]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1770
              CA       0.90      0.97      0.94      1775
              DE       0.97      1.00      0.99      1761
              ES       0.94      0.97      0.95      1831
              FR       0.84      0.92      0.88      1850
              GB       0.92      0.97      0.94      1788
              IT       0.88      0.97      0.92      1833
              NL       0.98      1.00      0.99      1806
              PT       0.99      1.00      1.00      1798
              US       0.83      0.44      0.57      1775
           other       0.80      0.82      0.81      1813

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.91      0.91      0.91     19800
    weighted avg       0.91      0.91      0.91     19800

    CPU times: user 3min 17s, sys: 671 ms, total: 3min 18s
    Wall time: 3min 18s


### Random Forest


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

p_rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(px_train, py_train)

print(p_rfc.best_params_)

```

    {'max_depth': 90, 'max_features': 'log2'}
    CPU times: user 1min 37s, sys: 1.91 s, total: 1min 39s
    Wall time: 1min 42s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_rfc.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_rfc, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_rfc.predict(px_test)))+'\n')

print(classification_report(py_test, p_rfc.predict(px_test)))

```

    accuracy score:
    0.9469191919191919

    cross validation:
    [0.80776993 0.80964403 0.79651603 0.806419   0.80111195]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1831    0    0    0    0    0    0    0]
     [   1    0    2    0 1828    0    0    2    0   10    7]
     [   0    0    0    0    0 1788    0    0    0    0    0]
     [   0    0    0    0    0    0 1833    0    0    0    0]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  15   33   16   74  178   60  100   19    6  989  285]
     [   0    9    9   13   35   22   11    3    1  140 1570]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      1.00      1770
              CA       0.98      1.00      0.99      1775
              DE       0.98      1.00      0.99      1761
              ES       0.95      1.00      0.98      1831
              FR       0.90      0.99      0.94      1850
              GB       0.96      1.00      0.98      1788
              IT       0.94      1.00      0.97      1833
              NL       0.99      1.00      0.99      1806
              PT       1.00      1.00      1.00      1798
              US       0.87      0.56      0.68      1775
           other       0.84      0.87      0.85      1813

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.95      0.95      0.94     19800
    weighted avg       0.94      0.95      0.94     19800

    CPU times: user 1min 50s, sys: 2.13 s, total: 1min 52s
    Wall time: 1min 53s


### Logistic Regression


```python
%%time

## Train and Fit Model

p_lr = LogisticRegression(penalty='l2').fit(px_train, py_train)

```

    CPU times: user 3.28 s, sys: 50.3 ms, total: 3.33 s
    Wall time: 3.38 s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_lr.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_lr, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_lr.predict(px_test)))+'\n')

print(classification_report(py_test, p_lr.predict(px_test)))

```

    accuracy score:
    0.14792929292929294

    cross validation:
    [0.14379415 0.13885382 0.14390305 0.13217084 0.15036644]

    confusion matrix:
    [[701 137 154  66  39 238 136  98 125  76   0]
     [285 357 136  57  62 227 121 135 290  70  35]
     [466 167 258  66  35 187  99 124 237  75  47]
     [309 236 107 130  94 209 225 171 216  97  37]
     [322 224 124 105  77 314 242 152 179  70  41]
     [331 201 106 107  75 341 192 112 225  67  31]
     [294 233 133 115  76 241 246 137 260  65  33]
     [344 218 176 104 102 184 160 202 212  70  34]
     [264 264  50  16  74 225 159 137 392 132  85]
     [356 226 100 111  38 223 143 144 222 149  63]
     [436 219 149  93  39 172 181 134 196 118  76]]

                  precision    recall  f1-score   support

              AU       0.17      0.40      0.24      1770
              CA       0.14      0.20      0.17      1775
              DE       0.17      0.15      0.16      1761
              ES       0.13      0.07      0.09      1831
              FR       0.11      0.04      0.06      1850
              GB       0.13      0.19      0.16      1788
              IT       0.13      0.13      0.13      1833
              NL       0.13      0.11      0.12      1806
              PT       0.15      0.22      0.18      1798
              US       0.15      0.08      0.11      1775
           other       0.16      0.04      0.07      1813

       micro avg       0.15      0.15      0.15     19800
       macro avg       0.14      0.15      0.13     19800
    weighted avg       0.14      0.15      0.13     19800

    CPU times: user 4.9 s, sys: 422 ms, total: 5.33 s
    Wall time: 2.97 s


### Gradient Boost


```python
%%time

## Train and Fit Model

parameters = {
              'n_estimators': list(np.arange(250, 301, 50)),
              'max_depth': list(range(3,5))
             }

acc_scorer = make_scorer(accuracy_score)

p_clf = GridSearchCV(cl, parameters, scoring=acc_scorer).fit(px_train, py_train)

print(p_clf.best_params_)

```

    {'max_depth': 4, 'n_estimators': 300}
    CPU times: user 56min 20s, sys: 1min 48s, total: 58min 8s
    Wall time: 58min 21s



```python
%%time

## Running the Model with Best Parameters for Faster Evaluation

p_clf = ensemble.GradientBoostingClassifier(max_depth=p_clf.best_params_['max_depth'],n_estimators=p_clf.best_params_['n_estimators'] ).fit(px_train,  py_train)
```

    CPU times: user 7min 59s, sys: 18 s, total: 8min 17s
    Wall time: 8min 19s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_clf.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_clf, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_clf.predict(px_test)))+'\n')

print(classification_report(py_test, p_clf.predict(px_test)))

```

    accuracy score:
    0.8470707070707071

    cross validation:
    [0.77724521 0.77202727 0.77202727 0.7700278  0.77053323]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1714    2    0   11    0   16    9    0    7   16]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0   23   13 1659   15   14   17    5    0   31   54]
     [   6   26   16   39 1452   66   43   17    1   85   99]
     [   0    7    6   24   20 1656   21    4    0   30   20]
     [   0   12    7   38   38   41 1579    4    2   51   61]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  24   90   55  141  213  136  164   49   11  550  342]
     [  19   62   41   78  113   93  105   35    5  235 1027]]

                  precision    recall  f1-score   support

              AU       0.97      1.00      0.99      1770
              CA       0.89      0.97      0.92      1775
              DE       0.93      1.00      0.96      1761
              ES       0.84      0.91      0.87      1831
              FR       0.78      0.78      0.78      1850
              GB       0.83      0.93      0.87      1788
              IT       0.81      0.86      0.84      1833
              NL       0.94      1.00      0.97      1806
              PT       0.99      1.00      0.99      1798
              US       0.56      0.31      0.40      1775
           other       0.63      0.57      0.60      1813

       micro avg       0.85      0.85      0.85     19800
       macro avg       0.83      0.85      0.84     19800
    weighted avg       0.83      0.85      0.84     19800

    CPU times: user 7min 21s, sys: 5.01 s, total: 7min 26s
    Wall time: 7min 28s


### MLP Neural Network


```python
%%time

## Train and Fit Model

p_mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,)).fit(px_train, py_train)
```

    CPU times: user 15min 51s, sys: 1min 42s, total: 17min 34s
    Wall time: 4min 29s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_mlp.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_mlp, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_mlp.predict(px_test)))+'\n')

print(classification_report(py_test, p_mlp.predict(px_test)))

```

    accuracy score:
    0.8021717171717172

    cross validation:
    [0.65110999 0.6563999  0.65109821 0.6550417  0.64417488]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   5 1715    4    9    3   25    0    0    5    3    6]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   4   33   13 1657    6   19    5    3   16   45   30]
     [  13   41   21   89 1179   99   49   27    5  168  159]
     [   5   23    1    7   21 1676    6    5    0   14   30]
     [   3   21   19   48   50   74 1441    9   12   84   72]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  44   93   65  169  234  178  178   53   21  412  328]
     [  12   76   48  123  189  126  144   32   15  380  668]]

                  precision    recall  f1-score   support

              AU       0.95      1.00      0.98      1770
              CA       0.86      0.97      0.91      1775
              DE       0.91      1.00      0.95      1761
              ES       0.79      0.90      0.84      1831
              FR       0.70      0.64      0.67      1850
              GB       0.76      0.94      0.84      1788
              IT       0.79      0.79      0.79      1833
              NL       0.93      1.00      0.97      1806
              PT       0.96      1.00      0.98      1798
              US       0.37      0.23      0.29      1775
           other       0.52      0.37      0.43      1813

       micro avg       0.80      0.80      0.80     19800
       macro avg       0.78      0.80      0.79     19800
    weighted avg       0.78      0.80      0.78     19800

    CPU times: user 12min 29s, sys: 1min 23s, total: 13min 52s
    Wall time: 3min 33s


Similar trends existed among the models run with PCA components and the models run with unaltered features. The random forest model had the best performance out of all of the model.
The decision tree had slightly lower scores. Notably, the K nearest neighbor model had extremely similar accuracy scores regardless of if it was run with PCA components or unaltered features. The MLP neural network run with PCA components also showed fewer signs of overfitting than the version run with unaltered features.

The accuracy scores of the models that used PCA components were only slightly lower than the models that used unaltered features. Using a limited number of PCA components from the dataset likely removed some variance that was important to the predictive accuracy of the models. Using PCA components does have the advantage of reducing computational complexity and runtimes and in this case, this made up for the mild drop in accuracy of the better performing model types.



## Modeling the Data using Features Chosen with the SelectKbest Function

### Naive Bayes


```python
%%time

## Train and Fit Model

k_bnb = BernoulliNB().fit(kx_train, ky_train)

```

    CPU times: user 399 ms, sys: 96.4 ms, total: 495 ms
    Wall time: 495 ms



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_bnb.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_bnb, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_bnb.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_bnb.predict(kx_test)))

```

    accuracy score:
    0.14404040404040405

    cross validation:
    [0.14375788 0.14361434 0.12983077 0.14047499 0.14762386]

    confusion matrix:
    [[468 110 335  56  30  62 313  42 135 173  35]
     [213 223 236  63  38  85 444  36 224 204  41]
     [312  91 487  30  88  47 364  41 163 184  34]
     [232 103 242  94 107  51 474  29 182 248  52]
     [252 141 234  75  93  88 512  34 186 178  29]
     [256 138 194  38  80 102 538  22 227 204  33]
     [173 144 233  54  84  88 518  36 215 219  51]
     [335  79 248  48  90  24 460  40 184 240  33]
     [186 184 203   0  83 117 385  40 438 116  19]
     [281 168 256  69  53  44 337  28 175 330  59]
     [283 114 305  66  43  32 377  26 200 253  59]]

                  precision    recall  f1-score   support

              AU       0.16      0.27      0.20      1759
              CA       0.15      0.12      0.14      1807
              DE       0.16      0.26      0.20      1841
              ES       0.16      0.05      0.08      1814
              FR       0.12      0.05      0.07      1822
              GB       0.14      0.06      0.08      1832
              IT       0.11      0.29      0.16      1815
              NL       0.11      0.02      0.04      1781
              PT       0.19      0.25      0.21      1771
              US       0.14      0.18      0.16      1800
           other       0.13      0.03      0.05      1758

       micro avg       0.14      0.14      0.14     19800
       macro avg       0.14      0.14      0.13     19800
    weighted avg       0.14      0.14      0.13     19800

    CPU times: user 2.46 s, sys: 298 ms, total: 2.76 s
    Wall time: 880 ms


### K Nearest Neighbors


```python
%%time

## Train and Fit Model

k_knn = neighbors.KNeighborsClassifier(n_neighbors=2).fit(kx_train, ky_train)

```

    CPU times: user 2.79 s, sys: 17.2 ms, total: 2.81 s
    Wall time: 2.81 s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_knn.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_knn, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_knn.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_knn.predict(kx_test)))

```

    accuracy score:
    0.9138383838383838

    cross validation:
    [0.73972257 0.74785462 0.74008588 0.7385043  0.75278059]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1807    0    0    0    0    0    0    0    0    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   4    1    3    4 1808    0    0    0    0    2    0]
     [   0    0    0    0    0 1832    0    0    0    0    0]
     [   0    1    0    0    2    1 1811    0    0    0    0]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  31   88   38  109  221  120  169   54   19  686  265]
     [   8   24   16   44   78   19   53    6    4  322 1184]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1759
              CA       0.94      1.00      0.97      1807
              DE       0.97      1.00      0.98      1841
              ES       0.92      1.00      0.96      1814
              FR       0.86      0.99      0.92      1822
              GB       0.93      1.00      0.96      1832
              IT       0.89      1.00      0.94      1815
              NL       0.97      1.00      0.98      1781
              PT       0.99      1.00      0.99      1771
              US       0.68      0.38      0.49      1800
           other       0.82      0.67      0.74      1758

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800

    CPU times: user 38 s, sys: 96.4 ms, total: 38.1 s
    Wall time: 38.2 s


### Decision Tree


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': list(range(8,15)),
              'max_depth': list(range(24,34))
             }

acc_scorer = make_scorer(accuracy_score)

k_decision_tree = GridSearchCV(dt, parameters, scoring=acc_scorer).fit(kx_train, ky_train)

print(k_decision_tree.best_params_)

```

    {'max_depth': 33, 'max_features': 14}
    CPU times: user 1min 16s, sys: 2.36 s, total: 1min 18s
    Wall time: 1min 19s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_decision_tree.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_decision_tree, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_decision_tree.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_decision_tree.predict(kx_test)))

```

    accuracy score:
    0.9337878787878788

    cross validation:
    [0.81588903 0.81574962 0.8272291  0.81733199 0.80485339]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1800    0    0    7    0    0    0    0    0    0]
     [   0    0 1837    0    4    0    0    0    0    0    0]
     [   0    1    0 1799   10    0    0    4    0    0    0]
     [   2    4    0    6 1779   10   13    1    0    7    0]
     [   0    0    0    6    4 1818    0    0    0    4    0]
     [   0    4    0    2   10    0 1791    0    0    8    0]
     [   0    0    0    0   14    0    0 1767    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  20   54   25   91  167   92  136   27    8  848  332]
     [   5   16    5   19   29   14   21    9    0  120 1520]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1759
              CA       0.96      1.00      0.98      1807
              DE       0.98      1.00      0.99      1841
              ES       0.94      0.99      0.96      1814
              FR       0.88      0.98      0.93      1822
              GB       0.94      0.99      0.97      1832
              IT       0.91      0.99      0.95      1815
              NL       0.98      0.99      0.98      1781
              PT       1.00      1.00      1.00      1771
              US       0.86      0.47      0.61      1800
           other       0.82      0.86      0.84      1758

       micro avg       0.93      0.93      0.93     19800
       macro avg       0.93      0.93      0.93     19800
    weighted avg       0.93      0.93      0.93     19800

    CPU times: user 1min 12s, sys: 953 ms, total: 1min 13s
    Wall time: 1min 13s


### Random Forest


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

k_rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(kx_train, ky_train)

print(k_rfc.best_params_)

```

    {'max_depth': 95, 'max_features': 'sqrt'}
    CPU times: user 1min 4s, sys: 2.78 s, total: 1min 7s
    Wall time: 1min 8s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_rfc.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_rfc, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_rfc.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_rfc.predict(kx_test)))

```

    accuracy score:
    0.9595454545454546

    cross validation:
    [0.81614124 0.81701161 0.83000758 0.82339565 0.81875632]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1807    0    0    0    0    0    0    0    0    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    0    0    0 1818    0    0    0    0    4    0]
     [   0    0    0    0    0 1832    0    0    0    0    0]
     [   0    0    0    0    0    0 1815    0    0    0    0]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [   6   27   12   57  145   40   61    4    2 1207  239]
     [   1    5    0    9   18    7   11    1    0  152 1554]]

                  precision    recall  f1-score   support

              AU       1.00      1.00      1.00      1759
              CA       0.98      1.00      0.99      1807
              DE       0.99      1.00      1.00      1841
              ES       0.96      1.00      0.98      1814
              FR       0.92      1.00      0.96      1822
              GB       0.97      1.00      0.99      1832
              IT       0.96      1.00      0.98      1815
              NL       1.00      1.00      1.00      1781
              PT       1.00      1.00      1.00      1771
              US       0.89      0.67      0.76      1800
           other       0.87      0.88      0.88      1758

       micro avg       0.96      0.96      0.96     19800
       macro avg       0.96      0.96      0.96     19800
    weighted avg       0.96      0.96      0.96     19800

    CPU times: user 1min 5s, sys: 3.17 s, total: 1min 8s
    Wall time: 1min 8s


### Logistic Regression


```python
%%time

## Train and Fit Model

k_lr = LogisticRegression(penalty='l2').fit(kx_train, ky_train)

```

    CPU times: user 20.2 s, sys: 158 ms, total: 20.4 s
    Wall time: 20.6 s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_lr.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_lr, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_lr.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_lr.predict(kx_test)))

```

    accuracy score:
    0.17813131313131314

    cross validation:
    [0.17528373 0.17617365 0.17529679 0.17306721 0.1744186 ]

    confusion matrix:
    [[666 138  70 151  22 149 122 149 194  64  34]
     [261 382 129 118  24 229  92 116 258 155  43]
     [391 204 277  91  54 165  81 211 210 128  29]
     [247 204 136 255  53 172 142 128 259 155  63]
     [251 270 112 161 122 253 142 150 245  69  47]
     [294 224  77 154  62 436 111 119 234  97  24]
     [252 229  95 160  84 230 174 141 277 122  51]
     [274 227 132 195  57 149  92 321 187 108  39]
     [255 206  90  65  60 218 129  69 595  60  24]
     [339 239 105 161  44 186  90 133 227 204  72]
     [394 205 130 137  57 169  84 141 220 126  95]]

                  precision    recall  f1-score   support

              AU       0.18      0.38      0.25      1759
              CA       0.15      0.21      0.18      1807
              DE       0.20      0.15      0.17      1841
              ES       0.15      0.14      0.15      1814
              FR       0.19      0.07      0.10      1822
              GB       0.19      0.24      0.21      1832
              IT       0.14      0.10      0.11      1815
              NL       0.19      0.18      0.19      1781
              PT       0.20      0.34      0.25      1771
              US       0.16      0.11      0.13      1800
           other       0.18      0.05      0.08      1758

       micro avg       0.18      0.18      0.18     19800
       macro avg       0.18      0.18      0.17     19800
    weighted avg       0.18      0.18      0.17     19800

    CPU times: user 12.4 s, sys: 480 ms, total: 12.9 s
    Wall time: 10.6 s


### Gradient Boost


```python
%%time

## Train and Fit Model

parameters = {
              'n_estimators': list(np.arange(250, 301, 50)),
              'max_depth': list(range(2,5))
             }

acc_scorer = make_scorer(accuracy_score)

k_clf = GridSearchCV(cl, parameters, scoring=acc_scorer).fit(kx_train, ky_train)

print(k_clf.best_params_)

```

    {'max_depth': 4, 'n_estimators': 300}
    CPU times: user 2h 8min 47s, sys: 2min 42s, total: 2h 11min 30s
    Wall time: 2h 12min 1s



```python
%%time

## Running the Model with Best Parameters for Faster Evaluation

k_clf = ensemble.GradientBoostingClassifier(max_depth=k_clf.best_params_['max_depth'],n_estimators=k_clf.best_params_['n_estimators'] ).fit(kx_train, ky_train)
```

    CPU times: user 15min 2s, sys: 19.4 s, total: 15min 21s
    Wall time: 15min 24s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_clf.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_clf, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_clf.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_clf.predict(kx_test)))

```

    accuracy score:
    0.8651515151515151

    cross validation:
    [0.82522068 0.81953559 0.81409447 0.81480546 0.81496461]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   6 1749    4    0    0    6    0    8    0   19   15]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   8    3   15 1657    9   38   10   10    0   52   12]
     [   9   40   24   31 1399   55   49   15    0  140   60]
     [  10   10    9   38    4 1639   13   20    3   69   17]
     [  14   29   10   13   30   43 1520   22    2   77   55]
     [   7    0    0    0    0    0    0 1770    0    4    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  34   92   50  116  124   96   97   45    9  902  235]
     [  33   38   38   59   80   70   37   28    1  251 1123]]

                  precision    recall  f1-score   support

              AU       0.94      1.00      0.97      1759
              CA       0.89      0.97      0.93      1807
              DE       0.92      1.00      0.96      1841
              ES       0.87      0.91      0.89      1814
              FR       0.85      0.77      0.81      1822
              GB       0.84      0.89      0.87      1832
              IT       0.88      0.84      0.86      1815
              NL       0.92      0.99      0.96      1781
              PT       0.99      1.00      1.00      1771
              US       0.60      0.50      0.54      1800
           other       0.74      0.64      0.69      1758

       micro avg       0.87      0.87      0.87     19800
       macro avg       0.86      0.87      0.86     19800
    weighted avg       0.86      0.87      0.86     19800

    CPU times: user 12min 41s, sys: 4.97 s, total: 12min 46s
    Wall time: 12min 48s


### MLP Neural Network


```python
%%time

## Train and Fit Model

k_mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,)).fit(kx_train, ky_train)
```

    CPU times: user 22min 55s, sys: 2min 22s, total: 25min 18s
    Wall time: 6min 33s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_mlp.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_mlp, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_mlp.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_mlp.predict(kx_test)))
```

    accuracy score:
    0.8237373737373738

    cross validation:
    [0.63203026 0.6367996  0.63526143 0.63996968 0.65242669]

    confusion matrix:
    [[1736    0    0    0    0   10    0   13    0    0    0]
     [   0 1754    0    0    7   24    3    3    0    8    8]
     [   0    0 1767    0   13   23    8    0    0   11   19]
     [   0    5    2 1667   26   15    9   13    0   70    7]
     [   5   24    1   42 1392   92   36   36    0  145   49]
     [   0    5    0   20   26 1709   31   16    0   25    0]
     [   0   13    0   29   69   52 1484   49    0   88   31]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0   21 1750    0    0]
     [  35   77   35  140  268  189  187   93    8  523  245]
     [   5   60   20   65  171   90   94   55    3  448  747]]

                  precision    recall  f1-score   support

              AU       0.97      0.99      0.98      1759
              CA       0.91      0.97      0.94      1807
              DE       0.97      0.96      0.96      1841
              ES       0.85      0.92      0.88      1814
              FR       0.71      0.76      0.73      1822
              GB       0.78      0.93      0.85      1832
              IT       0.80      0.82      0.81      1815
              NL       0.86      1.00      0.92      1781
              PT       0.99      0.99      0.99      1771
              US       0.40      0.29      0.34      1800
           other       0.68      0.42      0.52      1758

       micro avg       0.82      0.82      0.82     19800
       macro avg       0.81      0.82      0.81     19800
    weighted avg       0.81      0.82      0.81     19800

    CPU times: user 15min 10s, sys: 1min 42s, total: 16min 53s
    Wall time: 4min 19s


The random forest model had the best performance out of all of the models run with features chosen using the selectkbest function. Cross validation showed few signs of overfitting with this model. The random forest model using selectkbest still had lower accuracy than the model that used all of the available features in the dataset, implying that useful features were removed prior to modeling. Since selectkbest removes a low number of features in this case, this would mean that the majority of the dataset's useful features had meaningful variance.

When it comes to comparing the accuracy scores of the full featured models and models that used sectkbest, most of the full featured models slightly outperformed their counterparts that used K best features. However, the significant drop in the runtimes of the selectkbest models more than compensated for this.
Using selectKbest allows for computational complexity to be reduced without abstracting the individual features (unlike PCA). Even though most of the features did not need to be dropped to preserve accuracy, reducing the size of the training data proved worthwhile.



## Modeling Data using Deep Learning

### Convolutional Neural Network


```python
%%time

## Reshaping Data

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)

## Building the Model

model = Sequential()
model.add(Conv1D(32, (3), input_shape=(X_train.shape[1],1), activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names), activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

model.summary()
```

    WARNING: Logging before flag parsing goes to stderr.
    W0925 05:50:19.462752 4595271104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

    W0925 05:50:19.688436 4595271104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

    W0925 05:50:19.799309 4595271104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

    W0925 05:50:19.886074 4595271104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

    W0925 05:50:19.987470 4595271104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

    W0925 05:50:20.025527 4595271104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.



    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv1d_1 (Conv1D)            (None, 73, 32)            128       
    _________________________________________________________________
    max_pooling1d_1 (MaxPooling1 (None, 36, 32)            0         
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 1152)              0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 128)               147584    
    _________________________________________________________________
    dense_2 (Dense)              (None, 256)               33024     
    _________________________________________________________________
    dense_3 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dense_4 (Dense)              (None, 11)                2827      
    =================================================================
    Total params: 249,355
    Trainable params: 249,355
    Non-trainable params: 0
    _________________________________________________________________
    CPU times: user 182 ms, sys: 218 ms, total: 401 ms
    Wall time: 639 ms



```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train, Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```

    W0925 05:50:23.569712 4595271104 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    W0925 05:50:23.656213 4595271104 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.



    Train on 71280 samples, validate on 7920 samples
    Epoch 1/100
    71280/71280 [==============================] - 6s 90us/step - loss: 2.2832 - acc: 0.1727 - val_loss: 2.1300 - val_acc: 0.2379
    Epoch 2/100
    71280/71280 [==============================] - 6s 80us/step - loss: 1.9713 - acc: 0.3009 - val_loss: 1.7971 - val_acc: 0.3641
    Epoch 3/100
    71280/71280 [==============================] - 6s 79us/step - loss: 1.6707 - acc: 0.4155 - val_loss: 1.5350 - val_acc: 0.4576
    Epoch 4/100
    71280/71280 [==============================] - 6s 80us/step - loss: 1.4304 - acc: 0.5016 - val_loss: 1.3598 - val_acc: 0.5266
    Epoch 5/100
    71280/71280 [==============================] - 6s 79us/step - loss: 1.2480 - acc: 0.5632 - val_loss: 1.1960 - val_acc: 0.5886
    Epoch 6/100
    71280/71280 [==============================] - 6s 79us/step - loss: 1.1081 - acc: 0.6134 - val_loss: 1.0648 - val_acc: 0.6324
    Epoch 7/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.9984 - acc: 0.6521 - val_loss: 0.9808 - val_acc: 0.6568
    Epoch 8/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.9106 - acc: 0.6835 - val_loss: 0.9233 - val_acc: 0.6770
    Epoch 9/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.8349 - acc: 0.7101 - val_loss: 0.8634 - val_acc: 0.7062
    Epoch 10/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.8011 - acc: 0.7271 - val_loss: 0.8695 - val_acc: 0.7125
    Epoch 11/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.7516 - acc: 0.7449 - val_loss: 0.7891 - val_acc: 0.7332
    Epoch 12/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.6913 - acc: 0.7617 - val_loss: 0.7829 - val_acc: 0.7323
    Epoch 13/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.6523 - acc: 0.7743 - val_loss: 0.7171 - val_acc: 0.7610
    Epoch 14/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.6162 - acc: 0.7865 - val_loss: 0.6518 - val_acc: 0.7837
    Epoch 15/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.5875 - acc: 0.7960 - val_loss: 0.6552 - val_acc: 0.7811
    Epoch 16/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.5637 - acc: 0.8041 - val_loss: 0.6800 - val_acc: 0.7758
    Epoch 17/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.5438 - acc: 0.8102 - val_loss: 0.6426 - val_acc: 0.7931
    Epoch 18/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.5201 - acc: 0.8194 - val_loss: 0.6357 - val_acc: 0.7896
    Epoch 19/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.4996 - acc: 0.8248 - val_loss: 0.6181 - val_acc: 0.7948
    Epoch 20/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.4905 - acc: 0.8285 - val_loss: 0.5855 - val_acc: 0.8100
    Epoch 21/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.4620 - acc: 0.8402 - val_loss: 0.5731 - val_acc: 0.8135
    Epoch 22/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.4656 - acc: 0.8402 - val_loss: 0.5607 - val_acc: 0.8207
    Epoch 23/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.4331 - acc: 0.8492 - val_loss: 0.5640 - val_acc: 0.8211
    Epoch 24/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.4116 - acc: 0.8560 - val_loss: 0.5585 - val_acc: 0.8162
    Epoch 25/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.4177 - acc: 0.8549 - val_loss: 0.5524 - val_acc: 0.8355
    Epoch 26/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.4031 - acc: 0.8595 - val_loss: 0.5717 - val_acc: 0.8251
    Epoch 27/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.4053 - acc: 0.8614 - val_loss: 0.5947 - val_acc: 0.8164
    Epoch 28/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3735 - acc: 0.8688 - val_loss: 0.4963 - val_acc: 0.8487
    Epoch 29/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3766 - acc: 0.8695 - val_loss: 0.5206 - val_acc: 0.8457
    Epoch 30/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3544 - acc: 0.8755 - val_loss: 0.5400 - val_acc: 0.8384
    Epoch 31/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3582 - acc: 0.8779 - val_loss: 0.5289 - val_acc: 0.8404
    Epoch 32/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.3606 - acc: 0.8776 - val_loss: 0.5226 - val_acc: 0.8477
    Epoch 33/100
    71280/71280 [==============================] - 6s 90us/step - loss: 0.3231 - acc: 0.8857 - val_loss: 0.5131 - val_acc: 0.8428
    Epoch 34/100
    71280/71280 [==============================] - 7s 93us/step - loss: 0.3315 - acc: 0.8848 - val_loss: 0.5076 - val_acc: 0.8463
    Epoch 35/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3224 - acc: 0.8881 - val_loss: 0.5143 - val_acc: 0.8477
    Epoch 36/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.3169 - acc: 0.8912 - val_loss: 0.4911 - val_acc: 0.8598
    Epoch 37/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3096 - acc: 0.8944 - val_loss: 0.5424 - val_acc: 0.8407
    Epoch 38/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3142 - acc: 0.8916 - val_loss: 0.4553 - val_acc: 0.8660
    Epoch 39/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.2927 - acc: 0.8995 - val_loss: 0.5077 - val_acc: 0.8489
    Epoch 40/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.2817 - acc: 0.9028 - val_loss: 0.4970 - val_acc: 0.8578
    Epoch 41/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.2911 - acc: 0.9006 - val_loss: 0.4617 - val_acc: 0.8674
    Epoch 42/100
    71280/71280 [==============================] - 6s 87us/step - loss: 0.2780 - acc: 0.9062 - val_loss: 0.5248 - val_acc: 0.8619
    Epoch 43/100
    71280/71280 [==============================] - 6s 87us/step - loss: 0.2927 - acc: 0.9006 - val_loss: 0.5001 - val_acc: 0.8551
    Epoch 44/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.2558 - acc: 0.9103 - val_loss: 0.4527 - val_acc: 0.8692
    Epoch 45/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.2743 - acc: 0.9070 - val_loss: 0.5222 - val_acc: 0.8576
    Epoch 46/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.2698 - acc: 0.9103 - val_loss: 0.4687 - val_acc: 0.8686
    Epoch 47/100
    71280/71280 [==============================] - 6s 90us/step - loss: 0.2627 - acc: 0.9101 - val_loss: 0.4808 - val_acc: 0.8640
    Epoch 48/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.2560 - acc: 0.9135 - val_loss: 0.4987 - val_acc: 0.8615
    Epoch 49/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.2480 - acc: 0.9152 - val_loss: 0.4807 - val_acc: 0.8756
    Epoch 50/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.2431 - acc: 0.9184 - val_loss: 0.4708 - val_acc: 0.8736
    Epoch 51/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.2426 - acc: 0.9166 - val_loss: 0.5015 - val_acc: 0.8775
    Epoch 52/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.2369 - acc: 0.9183 - val_loss: 0.4820 - val_acc: 0.8723
    Epoch 53/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.2365 - acc: 0.9195 - val_loss: 0.4465 - val_acc: 0.8832
    Epoch 54/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.2369 - acc: 0.9210 - val_loss: 0.5342 - val_acc: 0.8694
    Epoch 55/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.2135 - acc: 0.9268 - val_loss: 0.4478 - val_acc: 0.8864
    Epoch 56/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.2227 - acc: 0.9244 - val_loss: 0.4414 - val_acc: 0.8835
    Epoch 57/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.2335 - acc: 0.9224 - val_loss: 0.4682 - val_acc: 0.8802
    Epoch 58/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.2167 - acc: 0.9261 - val_loss: 0.4820 - val_acc: 0.8715
    Epoch 59/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.2154 - acc: 0.9267 - val_loss: 0.5379 - val_acc: 0.8681
    Epoch 60/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.2243 - acc: 0.9264 - val_loss: 0.4912 - val_acc: 0.8775
    Epoch 61/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.1989 - acc: 0.9326 - val_loss: 0.4884 - val_acc: 0.8803
    Epoch 62/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.2173 - acc: 0.9271 - val_loss: 0.4880 - val_acc: 0.8838
    Epoch 63/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.2138 - acc: 0.9303 - val_loss: 0.4989 - val_acc: 0.8792
    Epoch 64/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.2026 - acc: 0.9322 - val_loss: 0.4922 - val_acc: 0.8905
    Epoch 65/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1974 - acc: 0.9329 - val_loss: 0.4858 - val_acc: 0.8876
    Epoch 66/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.2043 - acc: 0.9320 - val_loss: 0.5239 - val_acc: 0.8801
    Epoch 67/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1985 - acc: 0.9342 - val_loss: 0.4836 - val_acc: 0.8899
    Epoch 68/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.1972 - acc: 0.9350 - val_loss: 0.4962 - val_acc: 0.8867
    Epoch 69/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1784 - acc: 0.9395 - val_loss: 0.4572 - val_acc: 0.8888
    Epoch 70/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1902 - acc: 0.9369 - val_loss: 0.5494 - val_acc: 0.8715
    Epoch 71/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1912 - acc: 0.9371 - val_loss: 0.4772 - val_acc: 0.8936
    Epoch 72/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1996 - acc: 0.9357 - val_loss: 0.4733 - val_acc: 0.8904
    Epoch 73/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1766 - acc: 0.9411 - val_loss: 0.5061 - val_acc: 0.8874
    Epoch 74/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1772 - acc: 0.9405 - val_loss: 0.4790 - val_acc: 0.8905
    Epoch 75/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1917 - acc: 0.9371 - val_loss: 0.4915 - val_acc: 0.8966
    Epoch 76/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1815 - acc: 0.9415 - val_loss: 0.5032 - val_acc: 0.8910
    Epoch 77/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1838 - acc: 0.9422 - val_loss: 0.4667 - val_acc: 0.8976
    Epoch 78/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1613 - acc: 0.9455 - val_loss: 0.4758 - val_acc: 0.8944
    Epoch 79/100
    71280/71280 [==============================] - 6s 87us/step - loss: 0.1707 - acc: 0.9415 - val_loss: 0.4695 - val_acc: 0.9000
    Epoch 80/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1887 - acc: 0.9386 - val_loss: 0.4760 - val_acc: 0.8986
    Epoch 81/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1478 - acc: 0.9510 - val_loss: 0.4749 - val_acc: 0.8966
    Epoch 82/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1782 - acc: 0.9415 - val_loss: 0.4414 - val_acc: 0.9027
    Epoch 83/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1677 - acc: 0.9442 - val_loss: 0.5216 - val_acc: 0.8941
    Epoch 84/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.1597 - acc: 0.9472 - val_loss: 0.4659 - val_acc: 0.9042
    Epoch 85/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1668 - acc: 0.9449 - val_loss: 0.5145 - val_acc: 0.8942
    Epoch 86/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1603 - acc: 0.9466 - val_loss: 0.4806 - val_acc: 0.9013
    Epoch 87/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1629 - acc: 0.9454 - val_loss: 0.4885 - val_acc: 0.8924
    Epoch 88/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1496 - acc: 0.9512 - val_loss: 0.5013 - val_acc: 0.9029
    Epoch 89/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.1735 - acc: 0.9447 - val_loss: 0.4954 - val_acc: 0.8913
    Epoch 90/100
    71280/71280 [==============================] - 6s 88us/step - loss: 0.1529 - acc: 0.9493 - val_loss: 0.4986 - val_acc: 0.8953
    Epoch 91/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1488 - acc: 0.9502 - val_loss: 0.4936 - val_acc: 0.8997
    Epoch 92/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.1597 - acc: 0.9476 - val_loss: 0.4921 - val_acc: 0.9051
    Epoch 93/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.1428 - acc: 0.9525 - val_loss: 0.5057 - val_acc: 0.9029
    Epoch 94/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1534 - acc: 0.9496 - val_loss: 0.5117 - val_acc: 0.8927
    Epoch 95/100
    71280/71280 [==============================] - 6s 85us/step - loss: 0.1425 - acc: 0.9546 - val_loss: 0.6081 - val_acc: 0.8782
    Epoch 96/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1470 - acc: 0.9511 - val_loss: 0.4983 - val_acc: 0.9054
    Epoch 97/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1457 - acc: 0.9511 - val_loss: 0.5680 - val_acc: 0.8934
    Epoch 98/100
    71280/71280 [==============================] - 6s 87us/step - loss: 0.1502 - acc: 0.9501 - val_loss: 0.5756 - val_acc: 0.8831
    Epoch 99/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1509 - acc: 0.9520 - val_loss: 0.5590 - val_acc: 0.8944
    Epoch 100/100
    71280/71280 [==============================] - 6s 87us/step - loss: 0.1396 - acc: 0.9546 - val_loss: 0.4782 - val_acc: 0.9032
    CPU times: user 26min 18s, sys: 13min 40s, total: 39min 59s
    Wall time: 9min 51s



```python
# Model Evaluation

pred =  model.predict_classes(X_test)
scores = model.evaluate(X_test, Y_test, verbose=0)


print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.9083838383838384

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1793    0    5   13    8   10    0    0   14   12]
     [   4    0 1773    0   10    0    0    0    0    0    0]
     [   0    0    4 1733   10    1    0    0    0   32   34]
     [   0    8    1   11 1662    7   16    2    0   75   59]
     [   0   10    0    0    4 1678   14    4    0   56   16]
     [   0    2    0    3    6   11 1716    0    0   31   20]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  25   42   20   72  155   67  117   30    5  815  380]
     [   4   18    7   12   44   14   39   11    5  234 1404]]

                  precision    recall  f1-score   support

               0       0.98      1.00      0.99      1839
               1       0.96      0.97      0.96      1855
               2       0.98      0.99      0.99      1787
               3       0.94      0.96      0.95      1814
               4       0.87      0.90      0.89      1841
               5       0.94      0.94      0.94      1782
               6       0.90      0.96      0.93      1789
               7       0.97      1.00      0.99      1794
               8       0.99      1.00      1.00      1779
               9       0.65      0.47      0.55      1728
              10       0.73      0.78      0.76      1792

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800



### Recurrent Neural Network


```python
%%time

## Building the Model

model = Sequential()
model.add(LSTM(72, input_shape=(1,X_train.shape[1])))
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names),activation="softmax"))

model.compile(optimizer='Adam',loss="categorical_crossentropy",metrics=["accuracy"])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    lstm_1 (LSTM)                (None, 72)                42624     
    _________________________________________________________________
    dense_5 (Dense)              (None, 128)               9344      
    _________________________________________________________________
    dense_6 (Dense)              (None, 256)               33024     
    _________________________________________________________________
    dense_7 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dense_8 (Dense)              (None, 11)                2827      
    =================================================================
    Total params: 153,611
    Trainable params: 153,611
    Non-trainable params: 0
    _________________________________________________________________
    CPU times: user 561 ms, sys: 71.7 ms, total: 632 ms
    Wall time: 267 ms



```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train.reshape(X_train.shape[0],1,X_train.shape[1]), Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```

    Train on 71280 samples, validate on 7920 samples
    Epoch 1/100
    71280/71280 [==============================] - 6s 79us/step - loss: 2.2296 - acc: 0.1978 - val_loss: 2.0233 - val_acc: 0.2857
    Epoch 2/100
    71280/71280 [==============================] - 5s 72us/step - loss: 1.8465 - acc: 0.3501 - val_loss: 1.6782 - val_acc: 0.4120
    Epoch 3/100
    71280/71280 [==============================] - 5s 76us/step - loss: 1.5157 - acc: 0.4695 - val_loss: 1.4001 - val_acc: 0.4991
    Epoch 4/100
    71280/71280 [==============================] - 5s 77us/step - loss: 1.2806 - acc: 0.5518 - val_loss: 1.2201 - val_acc: 0.5736
    Epoch 5/100
    71280/71280 [==============================] - 5s 68us/step - loss: 1.1126 - acc: 0.6126 - val_loss: 1.0638 - val_acc: 0.6186
    Epoch 6/100
    71280/71280 [==============================] - 5s 63us/step - loss: 0.9920 - acc: 0.6566 - val_loss: 0.9771 - val_acc: 0.6581
    Epoch 7/100
    71280/71280 [==============================] - 5s 64us/step - loss: 0.8927 - acc: 0.6902 - val_loss: 0.8762 - val_acc: 0.6952
    Epoch 8/100
    71280/71280 [==============================] - 5s 66us/step - loss: 0.8101 - acc: 0.7195 - val_loss: 0.8204 - val_acc: 0.7092
    Epoch 9/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.7485 - acc: 0.7408 - val_loss: 0.7452 - val_acc: 0.7433
    Epoch 10/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.6840 - acc: 0.7628 - val_loss: 0.7251 - val_acc: 0.7506
    Epoch 11/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.6353 - acc: 0.7792 - val_loss: 0.6922 - val_acc: 0.7600
    Epoch 12/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.6044 - acc: 0.7909 - val_loss: 0.6419 - val_acc: 0.7782
    Epoch 13/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.5588 - acc: 0.8036 - val_loss: 0.6333 - val_acc: 0.7751
    Epoch 14/100
    71280/71280 [==============================] - 5s 77us/step - loss: 0.5297 - acc: 0.8156 - val_loss: 0.6302 - val_acc: 0.7891
    Epoch 15/100
    71280/71280 [==============================] - 5s 75us/step - loss: 0.5040 - acc: 0.8231 - val_loss: 0.5906 - val_acc: 0.7908
    Epoch 16/100
    71280/71280 [==============================] - 5s 75us/step - loss: 0.4775 - acc: 0.8309 - val_loss: 0.5464 - val_acc: 0.8093
    Epoch 17/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.4577 - acc: 0.8408 - val_loss: 0.5462 - val_acc: 0.8125
    Epoch 18/100
    71280/71280 [==============================] - 6s 77us/step - loss: 0.4369 - acc: 0.8474 - val_loss: 0.5392 - val_acc: 0.8139
    Epoch 19/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.4106 - acc: 0.8553 - val_loss: 0.5348 - val_acc: 0.8218
    Epoch 20/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.3955 - acc: 0.8602 - val_loss: 0.5119 - val_acc: 0.8210
    Epoch 21/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.3892 - acc: 0.8623 - val_loss: 0.4920 - val_acc: 0.8405
    Epoch 22/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.3613 - acc: 0.8727 - val_loss: 0.5043 - val_acc: 0.8345
    Epoch 23/100
    71280/71280 [==============================] - 5s 65us/step - loss: 0.3585 - acc: 0.8755 - val_loss: 0.5247 - val_acc: 0.8312
    Epoch 24/100
    71280/71280 [==============================] - 5s 66us/step - loss: 0.3379 - acc: 0.8808 - val_loss: 0.4944 - val_acc: 0.8399
    Epoch 25/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.3277 - acc: 0.8848 - val_loss: 0.4803 - val_acc: 0.8490
    Epoch 26/100
    71280/71280 [==============================] - 5s 64us/step - loss: 0.3300 - acc: 0.8850 - val_loss: 0.4337 - val_acc: 0.8534
    Epoch 27/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.3097 - acc: 0.8906 - val_loss: 0.4376 - val_acc: 0.8605
    Epoch 28/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.3077 - acc: 0.8926 - val_loss: 0.4211 - val_acc: 0.8697
    Epoch 29/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.2781 - acc: 0.9023 - val_loss: 0.4180 - val_acc: 0.8707
    Epoch 30/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.2933 - acc: 0.8970 - val_loss: 0.4477 - val_acc: 0.8612
    Epoch 31/100
    71280/71280 [==============================] - 5s 77us/step - loss: 0.2664 - acc: 0.9059 - val_loss: 0.4513 - val_acc: 0.8621
    Epoch 32/100
    71280/71280 [==============================] - 5s 75us/step - loss: 0.2691 - acc: 0.9068 - val_loss: 0.4410 - val_acc: 0.8658
    Epoch 33/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.2581 - acc: 0.9096 - val_loss: 0.4361 - val_acc: 0.8730
    Epoch 34/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.2543 - acc: 0.9111 - val_loss: 0.4480 - val_acc: 0.8688
    Epoch 35/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.2476 - acc: 0.9148 - val_loss: 0.4283 - val_acc: 0.8804
    Epoch 36/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.2530 - acc: 0.9129 - val_loss: 0.4347 - val_acc: 0.8660
    Epoch 37/100
    71280/71280 [==============================] - 5s 66us/step - loss: 0.2280 - acc: 0.9209 - val_loss: 0.4409 - val_acc: 0.8698
    Epoch 38/100
    71280/71280 [==============================] - 5s 64us/step - loss: 0.2326 - acc: 0.9192 - val_loss: 0.4496 - val_acc: 0.8688
    Epoch 39/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.2256 - acc: 0.9223 - val_loss: 0.4512 - val_acc: 0.8737
    Epoch 40/100
    71280/71280 [==============================] - 5s 64us/step - loss: 0.2185 - acc: 0.9252 - val_loss: 0.4587 - val_acc: 0.8775
    Epoch 41/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.2214 - acc: 0.9242 - val_loss: 0.4278 - val_acc: 0.8792
    Epoch 42/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.2069 - acc: 0.9281 - val_loss: 0.3740 - val_acc: 0.8941
    Epoch 43/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.2094 - acc: 0.9271 - val_loss: 0.4231 - val_acc: 0.8828
    Epoch 44/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.2037 - acc: 0.9313 - val_loss: 0.4106 - val_acc: 0.8879
    Epoch 45/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.1987 - acc: 0.9326 - val_loss: 0.4314 - val_acc: 0.8847
    Epoch 46/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.2019 - acc: 0.9314 - val_loss: 0.3992 - val_acc: 0.8932
    Epoch 47/100
    71280/71280 [==============================] - 5s 75us/step - loss: 0.1899 - acc: 0.9347 - val_loss: 0.4294 - val_acc: 0.8878
    Epoch 48/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1942 - acc: 0.9335 - val_loss: 0.4135 - val_acc: 0.8893
    Epoch 49/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.1761 - acc: 0.9396 - val_loss: 0.4125 - val_acc: 0.8955
    Epoch 50/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.1845 - acc: 0.9377 - val_loss: 0.4153 - val_acc: 0.8872
    Epoch 51/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1800 - acc: 0.9391 - val_loss: 0.4259 - val_acc: 0.8910
    Epoch 52/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.1810 - acc: 0.9387 - val_loss: 0.4262 - val_acc: 0.8855
    Epoch 53/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.1647 - acc: 0.9440 - val_loss: 0.4804 - val_acc: 0.8797
    Epoch 54/100
    71280/71280 [==============================] - 5s 75us/step - loss: 0.1807 - acc: 0.9390 - val_loss: 0.4736 - val_acc: 0.8859
    Epoch 55/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.1689 - acc: 0.9427 - val_loss: 0.3911 - val_acc: 0.9053
    Epoch 56/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.1616 - acc: 0.9461 - val_loss: 0.4888 - val_acc: 0.8826
    Epoch 57/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1700 - acc: 0.9419 - val_loss: 0.4040 - val_acc: 0.9051
    Epoch 58/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.1566 - acc: 0.9463 - val_loss: 0.4197 - val_acc: 0.8960
    Epoch 59/100
    71280/71280 [==============================] - 7s 91us/step - loss: 0.1490 - acc: 0.9489 - val_loss: 0.3992 - val_acc: 0.8965
    Epoch 60/100
    71280/71280 [==============================] - 7s 97us/step - loss: 0.1588 - acc: 0.9454 - val_loss: 0.4413 - val_acc: 0.9001
    Epoch 61/100
    71280/71280 [==============================] - 8s 107us/step - loss: 0.1532 - acc: 0.9487 - val_loss: 0.4541 - val_acc: 0.8944
    Epoch 62/100
    71280/71280 [==============================] - 8s 112us/step - loss: 0.1564 - acc: 0.9466 - val_loss: 0.4214 - val_acc: 0.9072
    Epoch 63/100
    71280/71280 [==============================] - 7s 92us/step - loss: 0.1534 - acc: 0.9490 - val_loss: 0.4891 - val_acc: 0.8912
    Epoch 64/100
    71280/71280 [==============================] - 7s 91us/step - loss: 0.1511 - acc: 0.9481 - val_loss: 0.4284 - val_acc: 0.9020
    Epoch 65/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1411 - acc: 0.9525 - val_loss: 0.4590 - val_acc: 0.9024
    Epoch 66/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.1454 - acc: 0.9521 - val_loss: 0.4965 - val_acc: 0.8957
    Epoch 67/100
    71280/71280 [==============================] - 7s 96us/step - loss: 0.1545 - acc: 0.9503 - val_loss: 0.4572 - val_acc: 0.8946
    Epoch 68/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1281 - acc: 0.9573 - val_loss: 0.4624 - val_acc: 0.8999
    Epoch 69/100
    71280/71280 [==============================] - 6s 88us/step - loss: 0.1498 - acc: 0.9509 - val_loss: 0.4866 - val_acc: 0.8985
    Epoch 70/100
    71280/71280 [==============================] - 6s 88us/step - loss: 0.1366 - acc: 0.9552 - val_loss: 0.4454 - val_acc: 0.9054
    Epoch 71/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.1354 - acc: 0.9549 - val_loss: 0.4303 - val_acc: 0.8898
    Epoch 72/100
    71280/71280 [==============================] - 6s 90us/step - loss: 0.1332 - acc: 0.9556 - val_loss: 0.4393 - val_acc: 0.9028
    Epoch 73/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1345 - acc: 0.9551 - val_loss: 0.4523 - val_acc: 0.9106
    Epoch 74/100
    71280/71280 [==============================] - 5s 75us/step - loss: 0.1372 - acc: 0.9550 - val_loss: 0.5010 - val_acc: 0.8997
    Epoch 75/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.1307 - acc: 0.9570 - val_loss: 0.4704 - val_acc: 0.9071
    Epoch 76/100
    71280/71280 [==============================] - 6s 89us/step - loss: 0.1389 - acc: 0.9547 - val_loss: 0.4024 - val_acc: 0.9160
    Epoch 77/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1297 - acc: 0.9581 - val_loss: 0.4221 - val_acc: 0.9105
    Epoch 78/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.1145 - acc: 0.9618 - val_loss: 0.4284 - val_acc: 0.9116
    Epoch 79/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.1264 - acc: 0.9583 - val_loss: 0.4709 - val_acc: 0.9078
    Epoch 80/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.1319 - acc: 0.9579 - val_loss: 0.4267 - val_acc: 0.9163
    Epoch 81/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.1189 - acc: 0.9605 - val_loss: 0.4622 - val_acc: 0.9058
    Epoch 82/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.1251 - acc: 0.9595 - val_loss: 0.4449 - val_acc: 0.9186
    Epoch 83/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.1234 - acc: 0.9597 - val_loss: 0.4252 - val_acc: 0.9093
    Epoch 84/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.1141 - acc: 0.9636 - val_loss: 0.4298 - val_acc: 0.9182
    Epoch 85/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1257 - acc: 0.9590 - val_loss: 0.4593 - val_acc: 0.9098
    Epoch 86/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.1063 - acc: 0.9645 - val_loss: 0.4469 - val_acc: 0.9088
    Epoch 87/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.1230 - acc: 0.9601 - val_loss: 0.4373 - val_acc: 0.9102
    Epoch 88/100
    71280/71280 [==============================] - 7s 92us/step - loss: 0.1101 - acc: 0.9648 - val_loss: 0.5488 - val_acc: 0.9000
    Epoch 89/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.1252 - acc: 0.9615 - val_loss: 0.4601 - val_acc: 0.9149
    Epoch 90/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.1024 - acc: 0.9668 - val_loss: 0.4318 - val_acc: 0.9189
    Epoch 91/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.1250 - acc: 0.9617 - val_loss: 0.4517 - val_acc: 0.9146
    Epoch 92/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1026 - acc: 0.9663 - val_loss: 0.4416 - val_acc: 0.9167
    Epoch 93/100
    71280/71280 [==============================] - 6s 84us/step - loss: 0.1144 - acc: 0.9641 - val_loss: 0.4208 - val_acc: 0.9179
    Epoch 94/100
    71280/71280 [==============================] - 6s 81us/step - loss: 0.1176 - acc: 0.9619 - val_loss: 0.5073 - val_acc: 0.9096
    Epoch 95/100
    71280/71280 [==============================] - 6s 86us/step - loss: 0.1069 - acc: 0.9660 - val_loss: 0.5183 - val_acc: 0.9051
    Epoch 96/100
    71280/71280 [==============================] - 5s 77us/step - loss: 0.1067 - acc: 0.9665 - val_loss: 0.4328 - val_acc: 0.9150
    Epoch 97/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1175 - acc: 0.9626 - val_loss: 0.4869 - val_acc: 0.9086
    Epoch 98/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.0982 - acc: 0.9691 - val_loss: 0.4627 - val_acc: 0.9144
    Epoch 99/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.0955 - acc: 0.9693 - val_loss: 0.4830 - val_acc: 0.9074
    Epoch 100/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.1263 - acc: 0.9607 - val_loss: 0.4810 - val_acc: 0.9140
    CPU times: user 25min 31s, sys: 7min 46s, total: 33min 18s
    Wall time: 9min 10s



```python
# Model Evaluation

pred =  model.predict_classes(X_test.reshape(X_test.shape[0],1,X_test.shape[1]))
scores = model.evaluate(X_test.reshape(X_test.shape[0],1,X_test.shape[1]), Y_test, verbose=0)

print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.9223232323473151

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1839    0    6    0    0    0    0    0    2    8]
     [   0    0 1774    8    0    5    0    0    0    0    0]
     [   0    0    0 1792    5    0    2    0    0   13    2]
     [   1    8    2    5 1756    8    6    2    1   32   20]
     [   0    0    3    2    0 1769    3    0    0    2    3]
     [   1   11    0    0   19    2 1736    0    0   10   10]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  19   63   19   87  167   91  119   29   12  700  422]
     [   3   31    8   22   42   25   12    8    3  154 1484]]

                  precision    recall  f1-score   support

               0       0.99      1.00      0.99      1839
               1       0.94      0.99      0.97      1855
               2       0.98      0.99      0.99      1787
               3       0.93      0.99      0.96      1814
               4       0.88      0.95      0.92      1841
               5       0.93      0.99      0.96      1782
               6       0.92      0.97      0.95      1789
               7       0.98      1.00      0.99      1794
               8       0.99      1.00      1.00      1779
               9       0.77      0.41      0.53      1728
              10       0.76      0.83      0.79      1792

       micro avg       0.92      0.92      0.92     19800
       macro avg       0.92      0.92      0.91     19800
    weighted avg       0.92      0.92      0.91     19800



### Convolutional Neural Network with Recurrent Neural Networks


```python
%%time

## Reshaping Data

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)

## Building the Model

model = Sequential()
model.add(Conv1D(32, (3), input_shape=(X_train.shape[1],1), activation='relu'))
model.add(MaxPool1D(pool_size=2))
model.add(LSTM(100))
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names), activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv1d_2 (Conv1D)            (None, 73, 32)            128       
    _________________________________________________________________
    max_pooling1d_2 (MaxPooling1 (None, 36, 32)            0         
    _________________________________________________________________
    lstm_2 (LSTM)                (None, 100)               53200     
    _________________________________________________________________
    dense_9 (Dense)              (None, 128)               12928     
    _________________________________________________________________
    dense_10 (Dense)             (None, 256)               33024     
    _________________________________________________________________
    dense_11 (Dense)             (None, 256)               65792     
    _________________________________________________________________
    dense_12 (Dense)             (None, 11)                2827      
    =================================================================
    Total params: 167,899
    Trainable params: 167,899
    Non-trainable params: 0
    _________________________________________________________________
    CPU times: user 603 ms, sys: 66 ms, total: 669 ms
    Wall time: 284 ms



```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train, Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```

    Train on 71280 samples, validate on 7920 samples
    Epoch 1/100
    71280/71280 [==============================] - 48s 672us/step - loss: 2.3978 - acc: 0.0910 - val_loss: 2.3984 - val_acc: 0.0963
    Epoch 2/100
    71280/71280 [==============================] - 48s 669us/step - loss: 2.3931 - acc: 0.1012 - val_loss: 2.3894 - val_acc: 0.1090
    Epoch 3/100
    71280/71280 [==============================] - 45s 627us/step - loss: 2.3862 - acc: 0.1092 - val_loss: 2.3829 - val_acc: 0.1034
    Epoch 4/100
    71280/71280 [==============================] - 45s 637us/step - loss: 2.3553 - acc: 0.1283 - val_loss: 2.3026 - val_acc: 0.1515
    Epoch 5/100
    71280/71280 [==============================] - 48s 672us/step - loss: 2.2692 - acc: 0.1751 - val_loss: 2.1837 - val_acc: 0.2157
    Epoch 6/100
    71280/71280 [==============================] - 49s 681us/step - loss: 2.1626 - acc: 0.2236 - val_loss: 2.1145 - val_acc: 0.2436
    Epoch 7/100
    71280/71280 [==============================] - 49s 691us/step - loss: 2.0468 - acc: 0.2700 - val_loss: 2.0130 - val_acc: 0.2850
    Epoch 8/100
    71280/71280 [==============================] - 50s 706us/step - loss: 1.9252 - acc: 0.3169 - val_loss: 1.8525 - val_acc: 0.3466
    Epoch 9/100
    71280/71280 [==============================] - 65s 907us/step - loss: 1.8083 - acc: 0.3594 - val_loss: 1.7357 - val_acc: 0.3881
    Epoch 10/100
    71280/71280 [==============================] - 57s 804us/step - loss: 1.7084 - acc: 0.3979 - val_loss: 1.6498 - val_acc: 0.4187
    Epoch 11/100
    71280/71280 [==============================] - 46s 650us/step - loss: 1.6270 - acc: 0.4280 - val_loss: 1.5796 - val_acc: 0.4510
    Epoch 12/100
    71280/71280 [==============================] - 52s 724us/step - loss: 1.5425 - acc: 0.4572 - val_loss: 1.4788 - val_acc: 0.4890
    Epoch 13/100
    71280/71280 [==============================] - 52s 735us/step - loss: 1.4818 - acc: 0.4785 - val_loss: 1.4172 - val_acc: 0.5115
    Epoch 14/100
    71280/71280 [==============================] - 52s 733us/step - loss: 1.4206 - acc: 0.4996 - val_loss: 1.3888 - val_acc: 0.5090
    Epoch 15/100
    71280/71280 [==============================] - 47s 656us/step - loss: 1.3623 - acc: 0.5209 - val_loss: 1.3290 - val_acc: 0.5260
    Epoch 16/100
    71280/71280 [==============================] - 48s 667us/step - loss: 1.3175 - acc: 0.5372 - val_loss: 1.3500 - val_acc: 0.5299
    Epoch 17/100
    71280/71280 [==============================] - 48s 676us/step - loss: 1.2656 - acc: 0.5558 - val_loss: 1.2426 - val_acc: 0.5653
    Epoch 18/100
    71280/71280 [==============================] - 49s 683us/step - loss: 1.2208 - acc: 0.5719 - val_loss: 1.2037 - val_acc: 0.5827
    Epoch 19/100
    71280/71280 [==============================] - 48s 676us/step - loss: 1.2038 - acc: 0.5782 - val_loss: 1.2131 - val_acc: 0.5830
    Epoch 20/100
    71280/71280 [==============================] - 47s 664us/step - loss: 1.1646 - acc: 0.5928 - val_loss: 1.2093 - val_acc: 0.5797
    Epoch 21/100
    71280/71280 [==============================] - 46s 641us/step - loss: 1.1256 - acc: 0.6038 - val_loss: 1.1107 - val_acc: 0.6106
    Epoch 22/100
    71280/71280 [==============================] - 47s 658us/step - loss: 1.0902 - acc: 0.6176 - val_loss: 1.1233 - val_acc: 0.6078
    Epoch 23/100
    71280/71280 [==============================] - 50s 707us/step - loss: 1.0711 - acc: 0.6249 - val_loss: 1.0905 - val_acc: 0.6187
    Epoch 24/100
    71280/71280 [==============================] - 50s 695us/step - loss: 1.0489 - acc: 0.6335 - val_loss: 1.1181 - val_acc: 0.6153
    Epoch 25/100
    71280/71280 [==============================] - 46s 644us/step - loss: 1.0184 - acc: 0.6431 - val_loss: 1.1147 - val_acc: 0.6092
    Epoch 26/100
    71280/71280 [==============================] - 46s 652us/step - loss: 0.9918 - acc: 0.6528 - val_loss: 1.0435 - val_acc: 0.6374
    Epoch 27/100
    71280/71280 [==============================] - 48s 673us/step - loss: 0.9710 - acc: 0.6617 - val_loss: 0.9674 - val_acc: 0.6665
    Epoch 28/100
    71280/71280 [==============================] - 45s 631us/step - loss: 0.9463 - acc: 0.6694 - val_loss: 0.9974 - val_acc: 0.6659
    Epoch 29/100
    71280/71280 [==============================] - 44s 624us/step - loss: 0.9265 - acc: 0.6745 - val_loss: 0.9589 - val_acc: 0.6585
    Epoch 30/100
    71280/71280 [==============================] - 44s 623us/step - loss: 0.9163 - acc: 0.6796 - val_loss: 0.9135 - val_acc: 0.6856
    Epoch 31/100
    71280/71280 [==============================] - 44s 623us/step - loss: 0.8865 - acc: 0.6893 - val_loss: 0.8858 - val_acc: 0.6989
    Epoch 32/100
    71280/71280 [==============================] - 44s 619us/step - loss: 0.8774 - acc: 0.6931 - val_loss: 0.9217 - val_acc: 0.6874
    Epoch 33/100
    71280/71280 [==============================] - 45s 629us/step - loss: 0.8610 - acc: 0.6988 - val_loss: 0.8646 - val_acc: 0.7021
    Epoch 34/100
    71280/71280 [==============================] - 46s 639us/step - loss: 0.8452 - acc: 0.7036 - val_loss: 0.9451 - val_acc: 0.6751
    Epoch 35/100
    71280/71280 [==============================] - 45s 627us/step - loss: 0.8209 - acc: 0.7130 - val_loss: 0.8140 - val_acc: 0.7181
    Epoch 36/100
    71280/71280 [==============================] - 44s 617us/step - loss: 0.8242 - acc: 0.7134 - val_loss: 0.8666 - val_acc: 0.6947
    Epoch 37/100
    71280/71280 [==============================] - 45s 626us/step - loss: 0.7952 - acc: 0.7211 - val_loss: 0.8855 - val_acc: 0.7104
    Epoch 38/100
    71280/71280 [==============================] - 45s 630us/step - loss: 0.7763 - acc: 0.7288 - val_loss: 0.8433 - val_acc: 0.7114
    Epoch 39/100
    71280/71280 [==============================] - 45s 630us/step - loss: 0.7744 - acc: 0.7290 - val_loss: 0.8567 - val_acc: 0.6995
    Epoch 40/100
    71280/71280 [==============================] - 45s 629us/step - loss: 0.7572 - acc: 0.7349 - val_loss: 0.8915 - val_acc: 0.6972
    Epoch 41/100
    71280/71280 [==============================] - 45s 636us/step - loss: 0.7506 - acc: 0.7380 - val_loss: 0.7752 - val_acc: 0.7369
    Epoch 42/100
    71280/71280 [==============================] - 45s 633us/step - loss: 0.7424 - acc: 0.7404 - val_loss: 0.8114 - val_acc: 0.7220
    Epoch 43/100
    71280/71280 [==============================] - 45s 630us/step - loss: 0.7252 - acc: 0.7484 - val_loss: 0.7910 - val_acc: 0.7323
    Epoch 44/100
    71280/71280 [==============================] - 45s 629us/step - loss: 0.7207 - acc: 0.7489 - val_loss: 0.7313 - val_acc: 0.7528
    Epoch 45/100
    71280/71280 [==============================] - 45s 630us/step - loss: 0.7037 - acc: 0.7529 - val_loss: 0.7643 - val_acc: 0.7413
    Epoch 46/100
    71280/71280 [==============================] - 45s 629us/step - loss: 0.6997 - acc: 0.7569 - val_loss: 0.7246 - val_acc: 0.7600
    Epoch 47/100
    71280/71280 [==============================] - 45s 632us/step - loss: 0.6842 - acc: 0.7629 - val_loss: 0.7597 - val_acc: 0.7427
    Epoch 48/100
    71280/71280 [==============================] - 45s 632us/step - loss: 0.6720 - acc: 0.7658 - val_loss: 0.7608 - val_acc: 0.7438
    Epoch 49/100
    71280/71280 [==============================] - 45s 633us/step - loss: 0.6620 - acc: 0.7705 - val_loss: 0.8484 - val_acc: 0.7144
    Epoch 50/100
    71280/71280 [==============================] - 45s 634us/step - loss: 0.6438 - acc: 0.7752 - val_loss: 0.7732 - val_acc: 0.7414
    Epoch 51/100
    71280/71280 [==============================] - 46s 639us/step - loss: 0.6534 - acc: 0.7712 - val_loss: 0.7169 - val_acc: 0.7648
    Epoch 52/100
    71280/71280 [==============================] - 46s 652us/step - loss: 0.6295 - acc: 0.7801 - val_loss: 0.6749 - val_acc: 0.7722
    Epoch 53/100
    71280/71280 [==============================] - 49s 694us/step - loss: 0.6321 - acc: 0.7792 - val_loss: 0.6646 - val_acc: 0.7830
    Epoch 54/100
    71280/71280 [==============================] - 52s 729us/step - loss: 0.6235 - acc: 0.7826 - val_loss: 0.8222 - val_acc: 0.7240
    Epoch 55/100
    71280/71280 [==============================] - 60s 846us/step - loss: 0.6170 - acc: 0.7867 - val_loss: 0.6988 - val_acc: 0.7641
    Epoch 56/100
    71280/71280 [==============================] - 49s 692us/step - loss: 0.6138 - acc: 0.7884 - val_loss: 0.6545 - val_acc: 0.7838
    Epoch 57/100
    71280/71280 [==============================] - 49s 693us/step - loss: 0.5928 - acc: 0.7927 - val_loss: 0.7198 - val_acc: 0.7638
    Epoch 58/100
    71280/71280 [==============================] - 50s 701us/step - loss: 0.5993 - acc: 0.7903 - val_loss: 0.6474 - val_acc: 0.7884
    Epoch 59/100
    71280/71280 [==============================] - 49s 690us/step - loss: 0.5801 - acc: 0.7967 - val_loss: 0.7172 - val_acc: 0.7592
    Epoch 60/100
    71280/71280 [==============================] - 49s 686us/step - loss: 0.5850 - acc: 0.7964 - val_loss: 0.6674 - val_acc: 0.7770
    Epoch 61/100
    71280/71280 [==============================] - 49s 686us/step - loss: 0.5629 - acc: 0.8053 - val_loss: 0.6229 - val_acc: 0.7910
    Epoch 62/100
    71280/71280 [==============================] - 49s 685us/step - loss: 0.5587 - acc: 0.8063 - val_loss: 0.6632 - val_acc: 0.7803
    Epoch 63/100
    71280/71280 [==============================] - 49s 683us/step - loss: 0.5597 - acc: 0.8063 - val_loss: 0.7642 - val_acc: 0.7448
    Epoch 64/100
    71280/71280 [==============================] - 49s 681us/step - loss: 0.5499 - acc: 0.8100 - val_loss: 0.6835 - val_acc: 0.7809
    Epoch 65/100
    71280/71280 [==============================] - 48s 671us/step - loss: 0.5423 - acc: 0.8113 - val_loss: 0.6344 - val_acc: 0.7960
    Epoch 66/100
    71280/71280 [==============================] - 47s 662us/step - loss: 0.5357 - acc: 0.8137 - val_loss: 0.6842 - val_acc: 0.7692
    Epoch 67/100
    71280/71280 [==============================] - 47s 660us/step - loss: 0.5322 - acc: 0.8161 - val_loss: 0.6351 - val_acc: 0.7957
    Epoch 68/100
    71280/71280 [==============================] - 48s 667us/step - loss: 0.5267 - acc: 0.8174 - val_loss: 0.6492 - val_acc: 0.7936
    Epoch 69/100
    71280/71280 [==============================] - 47s 661us/step - loss: 0.5365 - acc: 0.8138 - val_loss: 0.6022 - val_acc: 0.8088
    Epoch 70/100
    71280/71280 [==============================] - 47s 658us/step - loss: 0.5066 - acc: 0.8252 - val_loss: 0.6291 - val_acc: 0.7977
    Epoch 71/100
    71280/71280 [==============================] - 47s 659us/step - loss: 0.5187 - acc: 0.8205 - val_loss: 0.7155 - val_acc: 0.7674
    Epoch 72/100
    71280/71280 [==============================] - 47s 653us/step - loss: 0.4830 - acc: 0.8319 - val_loss: 0.6159 - val_acc: 0.7991
    Epoch 73/100
    71280/71280 [==============================] - 47s 662us/step - loss: 0.5098 - acc: 0.8233 - val_loss: 0.6158 - val_acc: 0.7961
    Epoch 74/100
    71280/71280 [==============================] - 47s 660us/step - loss: 0.5109 - acc: 0.8240 - val_loss: 0.6706 - val_acc: 0.7832
    Epoch 75/100
    71280/71280 [==============================] - 47s 657us/step - loss: 0.4950 - acc: 0.8298 - val_loss: 0.6169 - val_acc: 0.8033
    Epoch 76/100
    71280/71280 [==============================] - 50s 697us/step - loss: 0.4995 - acc: 0.8258 - val_loss: 0.6166 - val_acc: 0.8062
    Epoch 77/100
    71280/71280 [==============================] - 51s 721us/step - loss: 0.4963 - acc: 0.8286 - val_loss: 0.5790 - val_acc: 0.8206
    Epoch 78/100
    71280/71280 [==============================] - 53s 744us/step - loss: 0.4715 - acc: 0.8354 - val_loss: 0.6672 - val_acc: 0.7816
    Epoch 79/100
    71280/71280 [==============================] - 57s 795us/step - loss: 0.4829 - acc: 0.8340 - val_loss: 0.7450 - val_acc: 0.7676
    Epoch 80/100
    71280/71280 [==============================] - 51s 713us/step - loss: 0.4677 - acc: 0.8388 - val_loss: 0.6872 - val_acc: 0.7838
    Epoch 81/100
    71280/71280 [==============================] - 51s 710us/step - loss: 0.4852 - acc: 0.8323 - val_loss: 0.6603 - val_acc: 0.7896
    Epoch 82/100
    71280/71280 [==============================] - 50s 705us/step - loss: 0.4526 - acc: 0.8438 - val_loss: 0.5745 - val_acc: 0.8206
    Epoch 83/100
    71280/71280 [==============================] - 51s 710us/step - loss: 0.4514 - acc: 0.8423 - val_loss: 0.6134 - val_acc: 0.8040
    Epoch 84/100
    71280/71280 [==============================] - 51s 710us/step - loss: 0.4542 - acc: 0.8433 - val_loss: 0.5906 - val_acc: 0.8184
    Epoch 85/100
    71280/71280 [==============================] - 52s 725us/step - loss: 0.4496 - acc: 0.8446 - val_loss: 0.5810 - val_acc: 0.8208
    Epoch 86/100
    71280/71280 [==============================] - 51s 713us/step - loss: 0.4437 - acc: 0.8461 - val_loss: 0.6306 - val_acc: 0.7975
    Epoch 87/100
    71280/71280 [==============================] - 51s 714us/step - loss: 0.4411 - acc: 0.8473 - val_loss: 0.6329 - val_acc: 0.7956
    Epoch 88/100
    71280/71280 [==============================] - 51s 710us/step - loss: 0.4492 - acc: 0.8463 - val_loss: 0.6104 - val_acc: 0.8144
    Epoch 89/100
    71280/71280 [==============================] - 50s 706us/step - loss: 0.4271 - acc: 0.8514 - val_loss: 0.6047 - val_acc: 0.8034
    Epoch 90/100
    71280/71280 [==============================] - 51s 721us/step - loss: 0.4448 - acc: 0.8486 - val_loss: 0.5960 - val_acc: 0.8174
    Epoch 91/100
    71280/71280 [==============================] - 51s 714us/step - loss: 0.4465 - acc: 0.8460 - val_loss: 0.6045 - val_acc: 0.8210
    Epoch 92/100
    71280/71280 [==============================] - 51s 719us/step - loss: 0.4139 - acc: 0.8582 - val_loss: 0.5793 - val_acc: 0.8220
    Epoch 93/100
    71280/71280 [==============================] - 51s 720us/step - loss: 0.4206 - acc: 0.8544 - val_loss: 0.5747 - val_acc: 0.8218
    Epoch 94/100
    71280/71280 [==============================] - 51s 715us/step - loss: 0.4209 - acc: 0.8543 - val_loss: 0.6180 - val_acc: 0.8091
    Epoch 95/100
    71280/71280 [==============================] - 51s 720us/step - loss: 0.4185 - acc: 0.8551 - val_loss: 0.5403 - val_acc: 0.8371
    Epoch 96/100
    71280/71280 [==============================] - 51s 721us/step - loss: 0.4054 - acc: 0.8590 - val_loss: 0.5651 - val_acc: 0.8258
    Epoch 97/100
    71280/71280 [==============================] - 51s 712us/step - loss: 0.4039 - acc: 0.8612 - val_loss: 0.5923 - val_acc: 0.8138
    Epoch 98/100
    71280/71280 [==============================] - 51s 715us/step - loss: 0.4068 - acc: 0.8599 - val_loss: 0.5899 - val_acc: 0.8245
    Epoch 99/100
    71280/71280 [==============================] - 51s 722us/step - loss: 0.4020 - acc: 0.8612 - val_loss: 0.6060 - val_acc: 0.8186
    Epoch 100/100
    71280/71280 [==============================] - 51s 713us/step - loss: 0.3952 - acc: 0.8641 - val_loss: 0.6408 - val_acc: 0.8061
    CPU times: user 4h 17min 2s, sys: 1h 21min 11s, total: 5h 38min 14s
    Wall time: 1h 20min 52s



```python
# Model Evaluation

pred =  model.predict_classes(X_test)
scores = model.evaluate(X_test, Y_test, verbose=0)


print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.8029797980038806

    confusion matrix:
    [[1749    0    0    0   12    0   10   14    0   38   16]
     [   0 1674   11   24   28   23   27    9    3   25   31]
     [   0    0 1694   23   24   10   18    0    0    5   13]
     [   7   22   13 1467   66   20   34   33   11   79   62]
     [  14   33   20   41 1415   43   39   35    6   97   98]
     [  13   24   18   19   67 1500   26   19    0   45   51]
     [  15   29   35   47   69   30 1380   24   11   87   62]
     [  10    5    0   20    8    0    5 1719    9    8   10]
     [   0    0    0    0    0    0    0    0 1762   17    0]
     [  25   90   45  147  208  141  131   53   26  507  355]
     [  38   47   31   61  140   70   72   38    3  260 1032]]

                  precision    recall  f1-score   support

               0       0.93      0.95      0.94      1839
               1       0.87      0.90      0.89      1855
               2       0.91      0.95      0.93      1787
               3       0.79      0.81      0.80      1814
               4       0.69      0.77      0.73      1841
               5       0.82      0.84      0.83      1782
               6       0.79      0.77      0.78      1789
               7       0.88      0.96      0.92      1794
               8       0.96      0.99      0.98      1779
               9       0.43      0.29      0.35      1728
              10       0.60      0.58      0.59      1792

       micro avg       0.80      0.80      0.80     19800
       macro avg       0.79      0.80      0.79     19800
    weighted avg       0.79      0.80      0.80     19800



The convolutional neural network and recurrent neural networks were both able to reach test accuracy scores of over 90% after 100 epochs. The convolutional neural network paired with the recurrent neural network took much longer to attain less accuracy. The recurrent neural network was the most efficient of the deep learning models, attaining 90% accuracy after 40 epochs. Despite the potential of deep learning models to attain high accuracies, the computational complexity and slow runtimes makes this type of modeling unsuitable for this business objective.

## Analysis and Conclusion

The random forest model using features chosen by selectkbest was the best model when it came to predicting the first booking destinations, making it the strongest base for a classifier that can scaled for production.
While other model types had potential to produce more accurate predictions, they had much higher runtimes, making them unfit to run larger amounts of data. By introducing more data to this modeling pipeline, it can be trained to yield even more accurate and consistent results.

This classifier created by pairing the best supervised modeling technique and feature reduction method was built to be both accurate and scalable. Potential improvements in this product includes adding more features and further tuning of the model type. The accuracy of this model will most likely increase as it is trained with more data as well.

Understanding how to better utilize supervised modeling techniques to predict booking destination, will give insight as to how people are using the Airbnb platform and particular habits different types of customers share.
This can allow for more direct marketing to specific types of users or changes in the product that better match how the service is used. Through the use of cheap and accessible data, decisions can be made that can result in increased efficiency and revenue for the company.




```python

```
