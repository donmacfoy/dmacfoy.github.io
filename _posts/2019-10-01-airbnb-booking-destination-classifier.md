---
title: "Comprehensive Modeling Analysis: Predicting Airbnb New User Bookings"
date: 2019-10-01
tags: [data science, classification]
header:
  image: "/images/projects/airbnb-booking-destination-classifier/airbnb.jpg"
excerpt: "This classifier uses an unsupervised-supervised modeling pipeline to  predict booking destination countries of first time AirBnB users."
---


Home rental services such as Airbnb allow people to find affordable temporary housing on short notice. The efficiency of these services can be increased by making decisions informed by knowledge about how future customers will use them. By being able to determine how new customers will use home rental services for the first time, proactive changes can be made to the service and the surrounding operations. This results in a better product for the customers and improved operations for stakeholders. Data science techniques allow for the prediction of future user activities at the cost of relatively few resources. The abundance of data generated by short-term home rental services such as Airbnb, allows for plenty of opportunities for data science to be used to improve the operations. In order to utilize this readily usable data, I am interested in predicting booking destinations of first time Airbnb users.

Airbnb’s platform allows customers to efficiently find homes that fit their needs through the use of features such as filtered searches and wishlists. After finding desirable lodging, customers input payment information and book the locations. Throughout this process data is generated through information provided by the users and details saved about the user’s web sessions.

Several types of classification models were used to predict the first booking destination countries of Airbnb users. Models focused on using demographic and web session data to assign booking destination countries to individual users. The different models were compared by their ability to accurately and efficiently predict booking country. A final model was chosen from those that were evaluated and deemed suitable to be scaled for production. The process used to build this product is as follows:


Initiation and Data Preprocessing
* Import Packages and Files
* Data Cleaning
* Feature Engineering

Data Analysis and Exploration
* Viewing the Distribution of the Different Classes
* Checking the Correlatedness of Different Variables
* Interpreting Descriptive Statistics

Preparing The Data For Modeling
* Class Balancing and Mathematical Transformations
* Feature Selection and Reduction
* Establishing Variables for Training and Testing

Unsupervised Clustering Analysis
* Selecting Appropriate Clustering Method
* Analyzing Clusters Made Using K Means

Supervised Learning Models
1. Using Unaltered Features
2. Using PCA Components
3. Using Selectkbest Function

Deep Learning Classification Models
1. Using Convolutional Neural Network
2. Using Recurrent Neural Network
3. Using Convolutional Neural and Recurrent Networks Together

Analysis and Conclusion
* Selecting and Analyzing Final Model
* Conclusion and Discussion


## Initiation and Data Preprocessing

The data used for this model was released by Airbnb in the following datasets: train_users.csv and sessions.csv. The train user dataset contains information about specific users and how they first accessed the service. This dataset has over 200000 records with each one containing information about a unique user. The train user dataset contains the outcome variable, country destination. The sessions dataset contains information about actions performed during the user’s time on the Airbnb platform. This dataset contains over 10 million records with each one reflecting a specific action performed on the platform. Multiple records on the sessions dataset can refer a single user’s actions. A dataframe was created for modeling containing features generated from both datasets.



## Data Exploration and Analysis

The Airbnb data contains information about activity on the platform that occurred between January 2010 and June 2014.  The train dataset originally contained columns related to the time specific activities first occured, information about the how the user accessed Airbnb, and demographics of the users. Additional features were engineered to include the amount of time users spent doing specific activities on the platform.

<br>


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_11_0.png)


Above are plots representing the gender frequencies and age frequencies of the data, respectively. There are more females than males included in the data but the disparity between the two groups is not strong. The distribution of the ages is centered around the 30’s and skewed to the right. This likely reflects an age demographic with both the energy and resources to travel. Since both of these variables contain a large amount of null values imputation will be needed to make use of this data prior to modeling.

<br>


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_13_0.png)



Above are plots of languages used on the platform and country destination, respectively. Both plots are scaled logarithmically for readability because the dominant classes far outnumbered the rest. With regard to the language counts, English outnumbered the other classes greatly; and with regard to the country destinations, ‘NDF’ and the US outnumbered the other classes. NDF represents the class of users that haven’t booked a destination yet. Since this would be the class that the model being built would be predicting, this was not be used as an outcome variable to train the model.

<br>


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_18_0.png)



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_18_1.png)



The above plots refer to the frequencies the accounts were first created and frequencies bookings were made over the course of multiple years. The frequency of accounts being created showed an increasing trajectory over the course of five years, likely reflecting an increase in the userbase of Airbnb. There is a sharp drop in bookings made around the time that the data was collected. This doesn’t reflect a drop in the usage of the platform, but rather people who use the platform but haven’t made a booking yet (these customers would have the country destination label ‘NDF’).


```python
%%time

## Graphing Seasonal Activity   

fig = plt.figure(figsize=(14, 8))
fig.subplots_adjust(hspace=.3)

# Barplot of Monthly Usage
plt.subplot(2, 2, 1)
ax = df.month_account_created.value_counts().sort_index().plot(kind='bar',  

                                    title="Frequency of Accounts Created Each Month")
ax.set_xlabel("Month")
ax.set_ylabel("Frequency")
plt.xticks(rotation=80)

# Barplot of Usage During Week
plt.subplot(2, 2, 2)
ax = df.date_account_created.apply(lambda x: x.weekday()).value_counts().sort_index().plot(kind='bar',
                                    title="Frequency of Accounts Created Each Day of the Week")
ax.set_xlabel("Week Day")
ax.set_ylabel("Frequency")

# Line Graph of Usage over the Year
plt.subplot(2, 2, 3)
ax = df.date_account_created.apply(lambda x: x.dayofyear).value_counts().sort_index().plot(kind='line',
                                    title="Frequency of Accounts Created Each Day of the Year")
ax.set_xlabel("Day of the Year")
ax.set_ylabel("Frequency")

# Line Graph of First Activity Over a Day (24 hour scale)
plt.subplot(2, 2, 4)
ax = df.hour_first_active.value_counts().sort_index().plot(kind='line',
                                    title="Hour of Day First Active")
ax.set_xlabel("Hour of the Day (Using 24 Hour Clock)")
ax.set_ylabel("Frequency")

plt.show()
```


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_20_0.png)



The above plots reflect the frequencies use of the platform created across different timespans.
There’s a notable drop of accounts created between June and July. Since summer is a season that is popular for travel, people are less likely to need accounts during this time (because they would’ve presumably made accounts earlier than when they would travel using the service). There is a slight drop of accounts made over the weekends as well. Initial activity drops during the day which is likely the result of people having less time during the average US workday to be on Airbnb.



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_22_0.png)


The scatterplot matrix gives information about the relationship between specific engineered features.


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_24_0.png)

The heatmap is meant to gauge the correlatedness of engineered features. There is much correlatedness among these features, which is expected since they reflect amounts of time spent on the platform.


```python
%%time

## Descriptive Statistics

df.iloc[:,30:].describe()
```





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>secs_elapsed</th>
      <th>view_search_results_count</th>
      <th>p3_count</th>
      <th>wishlist_content_update_count</th>
      <th>view_search_results_secs</th>
      <th>p3_secs</th>
      <th>wishlist_content_update_secs</th>
      <th>user_profile_secs</th>
      <th>change_trip_characteristics_secs</th>
      <th>similar_listings_secs</th>
      <th>user_social_connections_secs</th>
      <th>update_listing_secs</th>
      <th>listing_reviews_secs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1514234.959</td>
      <td>12.366</td>
      <td>8.265</td>
      <td>6.093</td>
      <td>276234.787</td>
      <td>208080.601</td>
      <td>41784.183</td>
      <td>27568.269</td>
      <td>21228.880</td>
      <td>7907.491</td>
      <td>16065.679</td>
      <td>67370.556</td>
      <td>8924.165</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1913191.475</td>
      <td>28.446</td>
      <td>20.404</td>
      <td>13.310</td>
      <td>555234.812</td>
      <td>539892.590</td>
      <td>153573.994</td>
      <td>120118.685</td>
      <td>99799.784</td>
      <td>50358.126</td>
      <td>91532.842</td>
      <td>273893.662</td>
      <td>56216.593</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>256920.500</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>872862.000</td>
      <td>2.000</td>
      <td>2.000</td>
      <td>1.000</td>
      <td>46453.000</td>
      <td>17676.000</td>
      <td>996.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2043487.500</td>
      <td>13.000</td>
      <td>9.000</td>
      <td>6.000</td>
      <td>296887.500</td>
      <td>151894.000</td>
      <td>16313.000</td>
      <td>3755.000</td>
      <td>3976.500</td>
      <td>1521.500</td>
      <td>0.000</td>
      <td>4857.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>38221363.000</td>
      <td>987.000</td>
      <td>1131.000</td>
      <td>524.000</td>
      <td>10194287.000</td>
      <td>15450650.000</td>
      <td>9160217.000</td>
      <td>4881477.000</td>
      <td>4871979.000</td>
      <td>1894503.000</td>
      <td>3128228.000</td>
      <td>11370071.000</td>
      <td>2464683.000</td>
    </tr>
  </tbody>
</table>
</div>



Data will be normalized prior to modeling to account for the vast difference in scale of the variables.

## Preparing The Data For Modeling

To prepare the data for modeling, values were imputed, the data was resampled to address the class imbalance in the outcome, and multiple forms of feature reduction were implemented.
This resulted in four sets of variables: One reflecting all of the unaltered features of the dataset, one reflecting PCA components, one reflecting features chosen by the selectKbest function, and one encoded for deep learning.



```python
%%time

## Train Test Split the Four Sets of Feature and Outcome Variables

# Unaltered Features
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)

# PCA
px_train, px_test, py_train, py_test = train_test_split(pca_components, y, test_size=0.2, random_state=22)

# Select K Best
kx_train, kx_test, ky_train, ky_test = train_test_split(k_predictors, y, test_size=0.2, random_state=21)

# Deep Learning
X_train, X_test, Y_train, Y_test = train_test_split(np.asarray(x), np.asarray(Y), test_size=0.2, random_state=20)

```


Training and testing sets of four variables were generated to be used in modeling.
The x and y variables represent the variables to be used for modeling that reflect the all of the useful features of the data.
The px and py variables represent the variables to be used for modeling that reflect PCA components of the initial features.
The kx and ky variables represent the variables to be used for modeling that reflect features chosen by selectKbest.
The X and Y variables represent the variables converted to arrays to be used for deep learning.

## Unsupervised Clustering Analysis

Clustering was done on the unaltered features. Country destination labels were dropped prior to clustering so the records would be clustered based on their contents as opposed to the pre-generated label.

### Selecting Appropriate Clustering Method

Three methods of clustering were attempted: mean shift, affinity propogation, and k means. The appropriate clustering method was selected based on the number of clusters generated by a particular method and sillhouette scores.



Mean shift returned a single cluster and was deemed unfil for analysis.


Affinity propogation proved to be computationaly intensive and returned 430 groups. This was also unfit for analysis.


```python
%%time

## K Means

# Testing Multiple Numbers of Clusters
print('Silhouette scores for K Means:\n')
range_n_clusters = [9,10,11,12]
for n_clusters in range_n_clusters:
    print(str(n_clusters) + ' clusters: ' + str(silhouette_score(x_train, KMeans(n_clusters=n_clusters, random_state=42).fit_predict(x_train))))
print('\n')
```

    Silhouette scores for K Means:

    9 clusters: 0.25633171822577977
    10 clusters: 0.2552001710517822
    11 clusters: 0.25648440346380796
    12 clusters: 0.2710002047276309



K means yielded reasonable silhouette scores for the selected numbers of groups. K means with 11 clusters (the number of classes in the outcome variable) was chosen for further analysis.

### Analyzing Clusters

K-means generated labels were compared to the original country labels to analyze the ability of K-means to group the records based on their content.



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_53_0.png)



While clustering didn't prove to be a reliable means of separating country destination classes, it is worth mentioning that the sizes of the cluster and sillhouette scores imply that there is a tangible metric that k means is using to separate clusters.

## Supervised Modeling using Unaltered Features


### K Nearest Neighbors


```python
%%time

## Train and Fit Model

knn = neighbors.KNeighborsClassifier(n_neighbors=2).fit(x_train, y_train)

```



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(knn.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(knn, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, knn.predict(x_test)))+'\n')

print(classification_report(y_test, knn.predict(x_test)))

```

    accuracy score:
    0.9134343434343435

    cross validation:
    [0.75327952 0.74615191 0.74217172 0.73755999 0.74102175]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   2    4    1    0 1820    5    4    0    0    4    1]
     [   0    0    0    0    0 1782    0    0    0    0    0]
     [   0    2    0    2    5    0 1780    0    0    0    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  38   63   34  126  220  120  169   42   12  651  253]
     [   8   31   18   33   75   42   39   10    7  344 1185]]

                  precision    recall  f1-score   support

              AU       0.97      1.00      0.99      1839
              CA       0.95      1.00      0.97      1855
              DE       0.97      1.00      0.99      1787
              ES       0.92      1.00      0.96      1814
              FR       0.86      0.99      0.92      1841
              GB       0.91      1.00      0.96      1782
              IT       0.89      0.99      0.94      1789
              NL       0.97      1.00      0.99      1794
              PT       0.99      1.00      0.99      1779
              US       0.65      0.38      0.48      1728
           other       0.82      0.66      0.73      1792

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800



### Random Forest


```python
%%time

## Train and Fit Model

rf = ensemble.RandomForestClassifier()

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(x_train,  y_train)

print(rfc.best_params_)
```

    {'max_depth': 95, 'max_features': 'sqrt'}


```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(rfc.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(rfc, x_test, y_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, rfc.predict(x_test)))+'\n')

print(classification_report(y_test, rfc.predict(x_test)))

```

    accuracy score:
    0.9621717171717171

    cross validation:
    [0.8259334  0.81402978 0.82222222 0.81586259 0.81335357]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    0    0    0 1830    0    1    0    0    7    3]
     [   0    0    0    0    0 1782    0    0    0    0    0]
     [   0    0    0    0    0    0 1787    0    0    2    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  10   22   16   53  108   41   65    5    0 1160  248]
     [   3    2    0    6   25    7    5    3    0  117 1624]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      1.00      1839
              CA       0.99      1.00      0.99      1855
              DE       0.99      1.00      1.00      1787
              ES       0.97      1.00      0.98      1814
              FR       0.93      0.99      0.96      1841
              GB       0.97      1.00      0.99      1782
              IT       0.96      1.00      0.98      1789
              NL       1.00      1.00      1.00      1794
              PT       1.00      1.00      1.00      1779
              US       0.90      0.67      0.77      1728
           other       0.87      0.91      0.89      1792

       micro avg       0.96      0.96      0.96     19800
       macro avg       0.96      0.96      0.96     19800
    weighted avg       0.96      0.96      0.96     19800



Naive bayes and logistic regression had accuracies that were extremely low but substantially above random chance. This implies that the models were run correctly but these model types are not suited for the data. K nearest neighbors had an accuracy score over 90% but substantially lower cross validation scores. The decision tree and random forest both had high accuracy scores but the random forest was less prone to overfitting. The gradient boost and neural networks had middling accuracy scores, but took much longer than the other model types to attain their results.

The models that relied on the dataset’s unreduced features, in general, had the best accuracy in the study. An advantage of using all of the useful features is that as much meaningful variance was captured by the models as possible. A downside to this type of feature preparation that did stand out is the lack of efficiency. Since feature reduction didn’t take place with these models, their performance suffered and they had the longest runtimes. This method of feature selection also risks including features with variance that doesn’t aid in the predictive power of the models. However, this potential disadvantage didn’t hamper the model’s ability to perform well because many of the features that would noticeably have a negative effect on the models were already left out.



## Modeling the Data using PCA Components


### K Nearest Neighbors


```python
%%time

## Train and Fit Model

p_knn = neighbors.KNeighborsClassifier(n_neighbors=2).fit(px_train, py_train)

```


```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_knn.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_knn, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_knn.predict(px_test)))+'\n')

print(classification_report(py_test, p_knn.predict(px_test)))

```

    accuracy score:
    0.9134848484848485

    cross validation:
    [0.75227043 0.74577127 0.74046958 0.74500885 0.74222896]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1831    0    0    0    0    0    0    0]
     [   0    1    2    2 1824    0    4    0    0   15    2]
     [   0    0    0    0    3 1785    0    0    0    0    0]
     [   0    4    0    0    0    0 1829    0    0    0    0]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  32   69   38  115  269  117  155   46    9  672  253]
     [   2   24    9   40   81   47   50   13    3  308 1236]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1770
              CA       0.95      1.00      0.97      1775
              DE       0.97      1.00      0.99      1761
              ES       0.92      1.00      0.96      1831
              FR       0.84      0.99      0.91      1850
              GB       0.92      1.00      0.96      1788
              IT       0.90      1.00      0.94      1833
              NL       0.97      1.00      0.98      1806
              PT       0.99      1.00      1.00      1798
              US       0.68      0.38      0.49      1775
           other       0.83      0.68      0.75      1813

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800


### Random Forest


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

p_rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(px_train, py_train)

print(p_rfc.best_params_)

```

    {'max_depth': 90, 'max_features': 'log2'}


```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(p_rfc.score(px_test, py_test))+'\n')

print("cross validation:\n" + str(cross_val_score(p_rfc, px_test, py_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(py_test, p_rfc.predict(px_test)))+'\n')

print(classification_report(py_test, p_rfc.predict(px_test)))

```

    accuracy score:
    0.9469191919191919

    cross validation:
    [0.80776993 0.80964403 0.79651603 0.806419   0.80111195]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1831    0    0    0    0    0    0    0]
     [   1    0    2    0 1828    0    0    2    0   10    7]
     [   0    0    0    0    0 1788    0    0    0    0    0]
     [   0    0    0    0    0    0 1833    0    0    0    0]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  15   33   16   74  178   60  100   19    6  989  285]
     [   0    9    9   13   35   22   11    3    1  140 1570]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      1.00      1770
              CA       0.98      1.00      0.99      1775
              DE       0.98      1.00      0.99      1761
              ES       0.95      1.00      0.98      1831
              FR       0.90      0.99      0.94      1850
              GB       0.96      1.00      0.98      1788
              IT       0.94      1.00      0.97      1833
              NL       0.99      1.00      0.99      1806
              PT       1.00      1.00      1.00      1798
              US       0.87      0.56      0.68      1775
           other       0.84      0.87      0.85      1813

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.95      0.95      0.94     19800
    weighted avg       0.94      0.95      0.94     19800



Similar trends existed among the models run with PCA components and the models run with unaltered features. The random forest model had the best performance out of all of the model.
The decision tree had slightly lower scores. Notably, the K nearest neighbor model had extremely similar accuracy scores regardless of if it was run with PCA components or unaltered features. The MLP neural network run with PCA components also showed fewer signs of overfitting than the version run with unaltered features.

The accuracy scores of the models that used PCA components were only slightly lower than the models that used unaltered features. Using a limited number of PCA components from the dataset likely removed some variance that was important to the predictive accuracy of the models. Using PCA components does have the advantage of reducing computational complexity and runtimes and in this case, this made up for the mild drop in accuracy of the better performing model types.



## Modeling the Data using Features Chosen with the SelectKbest Function


### K Nearest Neighbors


```python
%%time

## Train and Fit Model

k_knn = neighbors.KNeighborsClassifier(n_neighbors=2).fit(kx_train, ky_train)

```



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_knn.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_knn, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_knn.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_knn.predict(kx_test)))

```

    accuracy score:
    0.9138383838383838

    cross validation:
    [0.73972257 0.74785462 0.74008588 0.7385043  0.75278059]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1807    0    0    0    0    0    0    0    0    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   4    1    3    4 1808    0    0    0    0    2    0]
     [   0    0    0    0    0 1832    0    0    0    0    0]
     [   0    1    0    0    2    1 1811    0    0    0    0]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  31   88   38  109  221  120  169   54   19  686  265]
     [   8   24   16   44   78   19   53    6    4  322 1184]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1759
              CA       0.94      1.00      0.97      1807
              DE       0.97      1.00      0.98      1841
              ES       0.92      1.00      0.96      1814
              FR       0.86      0.99      0.92      1822
              GB       0.93      1.00      0.96      1832
              IT       0.89      1.00      0.94      1815
              NL       0.97      1.00      0.98      1781
              PT       0.99      1.00      0.99      1771
              US       0.68      0.38      0.49      1800
           other       0.82      0.67      0.74      1758

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800


### Random Forest


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

k_rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(kx_train, ky_train)

print(k_rfc.best_params_)

```

    {'max_depth': 95, 'max_features': 'sqrt'}


```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(k_rfc.score(kx_test, ky_test))+'\n')

print("cross validation:\n" + str(cross_val_score(k_rfc, kx_test, ky_test, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(ky_test, k_rfc.predict(kx_test)))+'\n')

print(classification_report(ky_test, k_rfc.predict(kx_test)))

```

    accuracy score:
    0.9595454545454546

    cross validation:
    [0.81614124 0.81701161 0.83000758 0.82339565 0.81875632]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1807    0    0    0    0    0    0    0    0    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    0    0    0 1818    0    0    0    0    4    0]
     [   0    0    0    0    0 1832    0    0    0    0    0]
     [   0    0    0    0    0    0 1815    0    0    0    0]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [   6   27   12   57  145   40   61    4    2 1207  239]
     [   1    5    0    9   18    7   11    1    0  152 1554]]

                  precision    recall  f1-score   support

              AU       1.00      1.00      1.00      1759
              CA       0.98      1.00      0.99      1807
              DE       0.99      1.00      1.00      1841
              ES       0.96      1.00      0.98      1814
              FR       0.92      1.00      0.96      1822
              GB       0.97      1.00      0.99      1832
              IT       0.96      1.00      0.98      1815
              NL       1.00      1.00      1.00      1781
              PT       1.00      1.00      1.00      1771
              US       0.89      0.67      0.76      1800
           other       0.87      0.88      0.88      1758

       micro avg       0.96      0.96      0.96     19800
       macro avg       0.96      0.96      0.96     19800
    weighted avg       0.96      0.96      0.96     19800


The random forest model had the best performance out of all of the models run with features chosen using the selectkbest function. Cross validation showed few signs of overfitting with this model. The random forest model using selectkbest still had lower accuracy than the model that used all of the available features in the dataset, implying that useful features were removed prior to modeling. Since selectkbest removes a low number of features in this case, this would mean that the majority of the dataset's useful features had meaningful variance.

When it comes to comparing the accuracy scores of the full featured models and models that used sectkbest, most of the full featured models slightly outperformed their counterparts that used K best features. However, the significant drop in the runtimes of the selectkbest models more than compensated for this.
Using selectKbest allows for computational complexity to be reduced without abstracting the individual features (unlike PCA). Even though most of the features did not need to be dropped to preserve accuracy, reducing the size of the training data proved worthwhile.



## Modeling Data using Deep Learning

### Convolutional Neural Network


```python
%%time

## Reshaping Data

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)

## Building the Model

model = Sequential()
model.add(Conv1D(32, (3), input_shape=(X_train.shape[1],1), activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names), activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

model.summary()
```


    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv1d_1 (Conv1D)            (None, 73, 32)            128       
    _________________________________________________________________
    max_pooling1d_1 (MaxPooling1 (None, 36, 32)            0         
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 1152)              0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 128)               147584    
    _________________________________________________________________
    dense_2 (Dense)              (None, 256)               33024     
    _________________________________________________________________
    dense_3 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dense_4 (Dense)              (None, 11)                2827      
    =================================================================
    Total params: 249,355
    Trainable params: 249,355
    Non-trainable params: 0
    _________________________________________________________________




```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train, Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```



```python
# Model Evaluation

pred =  model.predict_classes(X_test)
scores = model.evaluate(X_test, Y_test, verbose=0)


print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.9083838383838384

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1793    0    5   13    8   10    0    0   14   12]
     [   4    0 1773    0   10    0    0    0    0    0    0]
     [   0    0    4 1733   10    1    0    0    0   32   34]
     [   0    8    1   11 1662    7   16    2    0   75   59]
     [   0   10    0    0    4 1678   14    4    0   56   16]
     [   0    2    0    3    6   11 1716    0    0   31   20]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  25   42   20   72  155   67  117   30    5  815  380]
     [   4   18    7   12   44   14   39   11    5  234 1404]]

                  precision    recall  f1-score   support

               0       0.98      1.00      0.99      1839
               1       0.96      0.97      0.96      1855
               2       0.98      0.99      0.99      1787
               3       0.94      0.96      0.95      1814
               4       0.87      0.90      0.89      1841
               5       0.94      0.94      0.94      1782
               6       0.90      0.96      0.93      1789
               7       0.97      1.00      0.99      1794
               8       0.99      1.00      1.00      1779
               9       0.65      0.47      0.55      1728
              10       0.73      0.78      0.76      1792

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.90      0.91      0.90     19800
    weighted avg       0.90      0.91      0.90     19800



### Recurrent Neural Network


```python
%%time

## Building the Model

model = Sequential()
model.add(LSTM(72, input_shape=(1,X_train.shape[1])))
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names),activation="softmax"))

model.compile(optimizer='Adam',loss="categorical_crossentropy",metrics=["accuracy"])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    lstm_1 (LSTM)                (None, 72)                42624     
    _________________________________________________________________
    dense_5 (Dense)              (None, 128)               9344      
    _________________________________________________________________
    dense_6 (Dense)              (None, 256)               33024     
    _________________________________________________________________
    dense_7 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dense_8 (Dense)              (None, 11)                2827      
    =================================================================
    Total params: 153,611
    Trainable params: 153,611
    Non-trainable params: 0
    _________________________________________________________________



```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train.reshape(X_train.shape[0],1,X_train.shape[1]), Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```


```python
# Model Evaluation

pred =  model.predict_classes(X_test.reshape(X_test.shape[0],1,X_test.shape[1]))
scores = model.evaluate(X_test.reshape(X_test.shape[0],1,X_test.shape[1]), Y_test, verbose=0)

print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.9223232323473151

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1839    0    6    0    0    0    0    0    2    8]
     [   0    0 1774    8    0    5    0    0    0    0    0]
     [   0    0    0 1792    5    0    2    0    0   13    2]
     [   1    8    2    5 1756    8    6    2    1   32   20]
     [   0    0    3    2    0 1769    3    0    0    2    3]
     [   1   11    0    0   19    2 1736    0    0   10   10]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  19   63   19   87  167   91  119   29   12  700  422]
     [   3   31    8   22   42   25   12    8    3  154 1484]]

                  precision    recall  f1-score   support

               0       0.99      1.00      0.99      1839
               1       0.94      0.99      0.97      1855
               2       0.98      0.99      0.99      1787
               3       0.93      0.99      0.96      1814
               4       0.88      0.95      0.92      1841
               5       0.93      0.99      0.96      1782
               6       0.92      0.97      0.95      1789
               7       0.98      1.00      0.99      1794
               8       0.99      1.00      1.00      1779
               9       0.77      0.41      0.53      1728
              10       0.76      0.83      0.79      1792

       micro avg       0.92      0.92      0.92     19800
       macro avg       0.92      0.92      0.91     19800
    weighted avg       0.92      0.92      0.91     19800



### Convolutional Neural Network with Recurrent Neural Networks


```python
%%time

## Reshaping Data

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)

## Building the Model

model = Sequential()
model.add(Conv1D(32, (3), input_shape=(X_train.shape[1],1), activation='relu'))
model.add(MaxPool1D(pool_size=2))
model.add(LSTM(100))
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names), activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv1d_2 (Conv1D)            (None, 73, 32)            128       
    _________________________________________________________________
    max_pooling1d_2 (MaxPooling1 (None, 36, 32)            0         
    _________________________________________________________________
    lstm_2 (LSTM)                (None, 100)               53200     
    _________________________________________________________________
    dense_9 (Dense)              (None, 128)               12928     
    _________________________________________________________________
    dense_10 (Dense)             (None, 256)               33024     
    _________________________________________________________________
    dense_11 (Dense)             (None, 256)               65792     
    _________________________________________________________________
    dense_12 (Dense)             (None, 11)                2827      
    =================================================================
    Total params: 167,899
    Trainable params: 167,899
    Non-trainable params: 0
    _________________________________________________________________




```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train, Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```


```python
# Model Evaluation

pred =  model.predict_classes(X_test)
scores = model.evaluate(X_test, Y_test, verbose=0)


print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.8029797980038806

    confusion matrix:
    [[1749    0    0    0   12    0   10   14    0   38   16]
     [   0 1674   11   24   28   23   27    9    3   25   31]
     [   0    0 1694   23   24   10   18    0    0    5   13]
     [   7   22   13 1467   66   20   34   33   11   79   62]
     [  14   33   20   41 1415   43   39   35    6   97   98]
     [  13   24   18   19   67 1500   26   19    0   45   51]
     [  15   29   35   47   69   30 1380   24   11   87   62]
     [  10    5    0   20    8    0    5 1719    9    8   10]
     [   0    0    0    0    0    0    0    0 1762   17    0]
     [  25   90   45  147  208  141  131   53   26  507  355]
     [  38   47   31   61  140   70   72   38    3  260 1032]]

                  precision    recall  f1-score   support

               0       0.93      0.95      0.94      1839
               1       0.87      0.90      0.89      1855
               2       0.91      0.95      0.93      1787
               3       0.79      0.81      0.80      1814
               4       0.69      0.77      0.73      1841
               5       0.82      0.84      0.83      1782
               6       0.79      0.77      0.78      1789
               7       0.88      0.96      0.92      1794
               8       0.96      0.99      0.98      1779
               9       0.43      0.29      0.35      1728
              10       0.60      0.58      0.59      1792

       micro avg       0.80      0.80      0.80     19800
       macro avg       0.79      0.80      0.79     19800
    weighted avg       0.79      0.80      0.80     19800



The convolutional neural network and recurrent neural networks were both able to reach test accuracy scores of over 90% after 100 epochs. The convolutional neural network paired with the recurrent neural network took much longer to attain less accuracy. The recurrent neural network was the most efficient of the deep learning models, attaining 90% accuracy after 40 epochs. Despite the potential of deep learning models to attain high accuracies, the computational complexity and slow runtimes makes this type of modeling unsuitable for this business objective.

## Analysis and Conclusion

The random forest model using features chosen by selectkbest was the best model when it came to predicting the first booking destinations, making it the strongest base for a classifier that can scaled for production.
While other model types had potential to produce more accurate predictions, they had much higher runtimes, making them unfit to run larger amounts of data. By introducing more data to this modeling pipeline, it can be trained to yield even more accurate and consistent results.

This classifier created by pairing the best supervised modeling technique and feature reduction method was built to be both accurate and scalable. Potential improvements in this product includes adding more features and further tuning of the model type. The accuracy of this model will most likely increase as it is trained with more data as well.

Understanding how to better utilize supervised modeling techniques to predict booking destination, will give insight as to how people are using the Airbnb platform and particular habits different types of customers share.
This can allow for more direct marketing to specific types of users or changes in the product that better match how the service is used. Through the use of cheap and accessible data, decisions can be made that can result in increased efficiency and revenue for the company.





```
