---
title: "Comprehensive Modeling Analysis: Predicting Airbnb New User Bookings"
date: 2019-10-02
tags: [machine learning, data science, classification, neural network]
header:
  image: "/images/projects/airbnb-booking-destination-classifier/airbnb.jpg"
excerpt: "This classifier uses an unsupervised-supervised modeling pipeline to  predict booking destination countries of first time AirBnB users."
---

The full code can be found [here](https://colab.research.google.com/drive/1J3U0fmLw8Mv9Ppu_CJBNNE0tyTfsexcc).

Home rental services such as Airbnb allow people to find affordable temporary housing on short notice. The efficiency of these services can be increased by making decisions informed by knowledge about how future customers will use them. By being able to determine how new customers will use home rental services for the first time, proactive changes can be made to the service and the surrounding operations. This results in a better product for the customers and improved operations for stakeholders. Data science techniques allow for the prediction of future user activities at the cost of relatively few resources. The abundance of data generated by short-term home rental services such as Airbnb, allows for plenty of opportunities for data science to be used to improve the operations. In order to utilize this readily usable data, I am interested in predicting booking destinations of first time Airbnb users.

Airbnb’s platform allows customers to efficiently find homes that fit their needs through the use of features such as filtered searches and wishlists. After finding desirable lodging, customers input payment information and book the locations. Throughout this process data is generated through information provided by the users and details saved about the user’s web sessions.

Several types of classification models were used to predict the first booking destination countries of Airbnb users. Models focused on using demographic and web session data to assign booking destination countries to individual users. The different models were compared by their ability to accurately and efficiently predict booking country. A final model was chosen from those that were evaluated and deemed suitable to be scaled for production. The process used to build this product is as follows:

<br>

Initiation and Data Preprocessing
* Import Packages and Files
* Data Cleaning
* Feature Engineering

Data Analysis and Exploration
* Viewing the Distribution of the Different Classes
* Checking the Correlatedness of Different Variables
* Interpreting Descriptive Statistics

Preparing The Data For Modeling
* Class Balancing and Mathematical Transformations
* Feature Selection and Reduction
* Establishing Variables for Training and Testing

Unsupervised Clustering Analysis
* Selecting Appropriate Clustering Method
* Analyzing Clusters Made Using K Means

Supervised Learning Models
1. Using Unaltered Features
2. Using Features Selected using F Values
3. Using Features Selected using Chi Squared

Deep Learning Classification Models
1. Using Convolutional Neural Network
2. Using Recurrent Neural Network
3. Using Convolutional Neural Networks Paired With Recurrent Neural Networks

Analysis and Conclusion
* Final Model Selection and Analysis
* Conclusion and Discussion


## Initiation and Data Preprocessing

The data used for this model was released by Airbnb in the following datasets: train_users.csv and sessions.csv. The train user dataset contains information about specific users and how they first accessed the service. This dataset has over 200000 records with each one containing information about a unique user. The train user dataset contains the outcome variable, country destination. The sessions dataset contains information about actions performed during the user’s time on the Airbnb platform. This dataset contains over 10 million records with each one reflecting a specific action performed on the platform. Multiple records on the sessions dataset can refer a single user’s actions. A dataframe was created for modeling containing features generated from both datasets.



## Data Exploration and Analysis

The Airbnb data contains information about activity on the platform that occurred between January 2010 and June 2014.  The train dataset originally contained columns related to the time specific activities first occured, information about the how the user accessed Airbnb, and demographics of the users. Additional features were engineered to include the amount of time users spent doing specific activities on the platform.

![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_11_0.png)


Above are plots representing the gender frequencies and age frequencies of the data, respectively. There are more females than males included in the data but the disparity between the two groups is not strong. The distribution of the ages is centered around the 30’s and skewed to the right. This likely reflects an age demographic with both the energy and resources to travel. Since both of these variables contain a large amount of null values imputation will be needed to make use of this data prior to modeling.



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_13_0.png)

Above are plots of languages used on the platform and country destination, respectively. Both plots are scaled logarithmically for readability because the dominant classes far outnumbered the rest. With regard to the language counts, English outnumbered the other classes greatly; and with regard to the country destinations, ‘NDF’ and the US outnumbered the other classes. NDF represents the class of users that haven’t booked a destination yet. Since this would be the class that the model being built would be predicting, this was not be used as an outcome variable to train the model.


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_18_0.png)



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_18_1.png)


The above plots refer to the frequencies the accounts were first created and frequencies bookings were made over the course of multiple years. The frequency of accounts being created showed an increasing trajectory over the course of five years, likely reflecting an increase in the userbase of Airbnb. There is a sharp drop in bookings made around the time that the data was collected. This doesn’t reflect a drop in the usage of the platform, but rather people who use the platform but haven’t made a booking yet (these customers would have the country destination label ‘NDF’).


![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_20_0.png)




The above plots reflect the frequencies use of the platform created across different timespans.
There’s a notable drop of accounts created between June and July. Since summer is a season that is popular for travel, people are less likely to need accounts during this time (because they would’ve presumably made accounts earlier than when they would travel using the service). There is a slight drop of accounts made over the weekends as well. Initial activity drops during the day which is likely the result of people having less time during the average US workday to be on Airbnb.



![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_22_0.png)

The scatterplot matrix gives information about the relationship between specific engineered features.

![png](https://raw.githubusercontent.com/donmacfoy/donmacfoy.github.io/master/images/projects/airbnb-booking-destination-classifier/output_24_0.png)


The heatmap is meant to gauge the correlatedness of engineered features. There is much correlatedness among these features, which is expected since they reflect amounts of time spent on the platform.







<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>secs_elapsed</th>
      <th>view_search_results_count</th>
      <th>p3_count</th>
      <th>wishlist_content_update_count</th>
      <th>user_profile_count</th>
      <th>change_trip_characteristics_count</th>
      <th>similar_listings_count</th>
      <th>user_social_connections_count</th>
      <th>update_listing_count</th>
      <th>listing_reviews_count</th>
      <th>...</th>
      <th>dashboard_secs</th>
      <th>user_wishlists_secs</th>
      <th>header_userpic_secs</th>
      <th>message_thread_secs</th>
      <th>edit_profile_secs</th>
      <th>message_post_secs</th>
      <th>contact_host_secs</th>
      <th>unavailable_dates_secs</th>
      <th>confirm_email_link_secs</th>
      <th>create_user_secs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>...</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
      <td>73815.000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1514234.959</td>
      <td>12.366</td>
      <td>8.265</td>
      <td>6.093</td>
      <td>3.634</td>
      <td>4.278</td>
      <td>4.172</td>
      <td>1.940</td>
      <td>2.105</td>
      <td>1.128</td>
      <td>...</td>
      <td>14830.135</td>
      <td>29595.309</td>
      <td>4515.531</td>
      <td>51134.615</td>
      <td>16820.605</td>
      <td>74545.834</td>
      <td>21243.895</td>
      <td>7023.293</td>
      <td>94984.878</td>
      <td>100.872</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1913191.475</td>
      <td>28.446</td>
      <td>20.404</td>
      <td>13.310</td>
      <td>14.324</td>
      <td>9.715</td>
      <td>10.018</td>
      <td>9.129</td>
      <td>8.068</td>
      <td>5.697</td>
      <td>...</td>
      <td>112658.964</td>
      <td>177757.650</td>
      <td>45779.443</td>
      <td>306877.869</td>
      <td>93129.557</td>
      <td>268323.753</td>
      <td>104188.163</td>
      <td>53648.290</td>
      <td>259453.037</td>
      <td>7967.568</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>256920.500</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>143.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>872862.000</td>
      <td>2.000</td>
      <td>2.000</td>
      <td>1.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>695.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2043487.500</td>
      <td>13.000</td>
      <td>9.000</td>
      <td>6.000</td>
      <td>2.000</td>
      <td>4.000</td>
      <td>4.000</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.000</td>
      <td>...</td>
      <td>1268.000</td>
      <td>0.000</td>
      <td>2356.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>1583.000</td>
      <td>427.000</td>
      <td>0.000</td>
      <td>44589.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>38221363.000</td>
      <td>987.000</td>
      <td>1131.000</td>
      <td>524.000</td>
      <td>454.000</td>
      <td>308.000</td>
      <td>325.000</td>
      <td>280.000</td>
      <td>249.000</td>
      <td>198.000</td>
      <td>...</td>
      <td>4632786.000</td>
      <td>6713259.000</td>
      <td>2288760.000</td>
      <td>11242289.000</td>
      <td>3415239.000</td>
      <td>13260005.000</td>
      <td>3258808.000</td>
      <td>2310246.000</td>
      <td>2902601.000</td>
      <td>1723704.000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 39 columns</p>
</div>



Data will be normalized prior to modeling to account for the vast difference in scale of the variables.

## Preparing The Data For Modeling

To prepare the data for modeling, values were imputed, the data was resampled to address the class imbalance in the outcome, and multiple forms of feature reduction were implemented.
This resulted in four sets of variables: One reflecting all of the unaltered features of the dataset, one reflecting features with the highest F values, one reflecting features with the highest chi squared values, and encoded for deep learning.


```python
%%time

## Creating Modeling DataFrame

# Feature Selection
train_feature_names = ['language', 'age','gender','first_device_type',
                               'signup_app','affiliate_provider','affiliate_channel',
                               'month_first_active','signup_method','day_account_created',
                               'month_account_created','country_destination']

df = df[train_feature_names + session_feature_names]

# Keep Records with Session Features
df = df[df['secs_elapsed'].notnull()]

print(np.array(df.columns))
```

    ['language' 'age' 'gender' 'first_device_type' 'signup_app'
     'affiliate_provider' 'affiliate_channel' 'month_first_active'
     'signup_method' 'day_account_created' 'month_account_created'
     'country_destination' 'secs_elapsed' 'view_search_results_count'
     'p3_count' 'wishlist_content_update_count' 'user_profile_count'
     'change_trip_characteristics_count' 'similar_listings_count'
     'user_social_connections_count' 'update_listing_count'
     'listing_reviews_count' 'dashboard_count' 'user_wishlists_count'
     'header_userpic_count' 'message_thread_count' 'edit_profile_count'
     'message_post_count' 'contact_host_count' 'unavailable_dates_count'
     'confirm_email_link_count' 'create_user_count' 'view_search_results_secs'
     'p3_secs' 'wishlist_content_update_secs' 'user_profile_secs'
     'change_trip_characteristics_secs' 'similar_listings_secs'
     'user_social_connections_secs' 'update_listing_secs'
     'listing_reviews_secs' 'dashboard_secs' 'user_wishlists_secs'
     'header_userpic_secs' 'message_thread_secs' 'edit_profile_secs'
     'message_post_secs' 'contact_host_secs' 'unavailable_dates_secs'
     'confirm_email_link_secs' 'create_user_secs']
    CPU times: user 65.1 ms, sys: 44.1 ms, total: 109 ms
    Wall time: 109 ms



```python
%%time

## Impute Ages with Group Means

df['age'] = df.groupby([ 'signup_method','country_destination', 'first_device_type'])['age'].transform(lambda x: x.fillna(x.mean()))
```

    CPU times: user 350 ms, sys: 41.9 ms, total: 392 ms
    Wall time: 390 ms



```python
%%time

## Creating DataFrame for Imputation of Gender

df_impute_gender = df.dropna()

## Class Balancing and Resampling Imputation DataFrame for Random Forest Imputation

records_per_class = 18000

# Separate Majority and Minority Classes
df_majority = df_impute_gender[df_impute_gender.gender=='female']
df_minority = df_impute_gender[df_impute_gender.gender=='male']

# Upsample Minority Class
df_minority_upsampled = resample(df_minority,
                                 replace=True,     
                                 n_samples=records_per_class,    
                                 random_state=123)

# Downsample Majority Class
df_majority_downsampled = resample(df_majority,
                                 replace=False,    
                                 n_samples=records_per_class,     
                                 random_state=123)

# Combine Downsampled Majority Class With Upsampled Minority Class
df_impute_gender = pd.concat([df_majority_downsampled, df_minority_upsampled])

print(df_impute_gender.gender.value_counts())
```

    male      18000
    female    18000
    Name: gender, dtype: int64
    CPU times: user 139 ms, sys: 39 ms, total: 178 ms
    Wall time: 181 ms



```python
%%time

## Random Forest Imputation

# Create Gender Column for Imputation Using Random Forests
x = pd.get_dummies(df_impute_gender.drop(['gender', 'age','signup_method','affiliate_channel','first_device_type','signup_app','affiliate_provider', 'language'], axis=1))
y = df_impute_gender['gender']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)

rfc = ensemble.RandomForestClassifier().fit(x_train,  y_train)

imputed_gender = rfc.predict(pd.get_dummies(df.drop(['gender', 'age','signup_method','affiliate_channel','first_device_type','signup_app','affiliate_provider', 'language'],1)))

# Imputing Gender with Predicted Values
df['gender'] = np.where(df['gender'].isnull(), imputed_gender, df['gender'])

# Shedding Remaining Null Values
df = df.dropna()
```

    CPU times: user 1.13 s, sys: 94.9 ms, total: 1.22 s
    Wall time: 1.23 s



```python
%%time

## Class Balancing and Resampling of Training DataFrame

country_names = df.country_destination.unique()[df.country_destination.unique() != 'NDF']
records_per_class = 9000

df_resampled = pd.DataFrame()
for country in country_names:
    if len(df[df['country_destination'] == country]) >= records_per_class:
        df_resample_1 = df[df['country_destination'] == country]
        df_resample_1 = resample(df_resample_1, replace=False, n_samples=records_per_class, random_state=123)
        df_resampled = df_resampled.append(df_resample_1)
    else:
        df_resample_2 = df[df['country_destination'] == country]
        df_resample_2 = resample(df_resample_2, replace=True, n_samples=records_per_class, random_state=123)
        df_resampled = df_resampled.append(df_resample_2)


print(df_resampled.country_destination.value_counts())
```

    other    9000
    AU       9000
    US       9000
    ES       9000
    PT       9000
    IT       9000
    NL       9000
    DE       9000
    GB       9000
    FR       9000
    CA       9000
    Name: country_destination, dtype: int64
    CPU times: user 540 ms, sys: 258 ms, total: 798 ms
    Wall time: 804 ms



```python
%%time

## Establish DataFrame to be Used for Modeling

df = df_resampled

# Get Dummies so Categorical Variables Can be Used in Classification
df = pd.get_dummies(df.drop(['country_destination'], axis=1))

# Display New Number of Features
print('Number of Features: '+ str(len(df.columns)))
```

    Number of Features: 101
    CPU times: user 134 ms, sys: 59.2 ms, total: 193 ms
    Wall time: 194 ms



```python
%%time

## Rescaling DataFrame

df = ((df-df.min())/(df.max()-df.min()))

## Creating a Numeric Version of the Outcome Variable

df_resampled['country_destination_categorical'] = df_resampled['country_destination'].astype('category').cat.codes
```

    CPU times: user 479 ms, sys: 244 ms, total: 723 ms
    Wall time: 732 ms



```python
%%time

## Establish Feature and Outcome Variables to be Used for Modeling Based on Original Features

x = df
y = df_resampled['country_destination']

# This is a Version of the Outcome Variable Encoded for Deep Learning
Y = keras.utils.to_categorical(df_resampled['country_destination_categorical'], len(country_names))
```

    CPU times: user 2.1 ms, sys: 1.9 ms, total: 4 ms
    Wall time: 3.08 ms



```python
%%time

## PCA

# Normalize the Data for PCA
X = StandardScaler().fit_transform(x)

# Perform PCA
sklearn_pca = PCA(n_components=15)
Y_sklearn = sklearn_pca.fit_transform(X)

# Turn PCA Result into a Dataframe
pca_components = pd.DataFrame(data=Y_sklearn)

print(
    'The percentage of total variance in the dataset explained by each',
    'component from Sklearn PCA.\n',
    sklearn_pca.explained_variance_ratio_
)

print(
    '\nThe percentage of total variance in the dataset explained by all',
    'components from Sklearn PCA.\n',
    sum(sklearn_pca.explained_variance_ratio_)
)
```

    The percentage of total variance in the dataset explained by each component from Sklearn PCA.
     [0.07226476 0.06089718 0.03538927 0.03107406 0.02927466 0.02418825
     0.02306968 0.02104638 0.02006617 0.01872581 0.01807856 0.01627248
     0.01573593 0.01537226 0.01447386]

    The percentage of total variance in the dataset explained by all components from Sklearn PCA.
     0.4159293002175515
    CPU times: user 3.68 s, sys: 803 ms, total: 4.48 s
    Wall time: 1.09 s


### Feature Selection Using F values


```python
%%time

## Establish variables based on select K best to be used for modeling

selector = SelectKBest(f_classif, k=60)
k_predictors = selector.fit_transform(x,y)
```

    CPU times: user 238 ms, sys: 103 ms, total: 341 ms
    Wall time: 366 ms



```python
%%time

## Print Most Variables Deemed Most Important By SelectKbest

importance = pd.concat([pd.DataFrame(x.columns), pd.DataFrame(selector.fit(x, y).scores_)], axis=1)
importance.columns = ['feature', 'importance']
importance = importance.sort_values(by=['importance'], ascending=False)
print(np.array(list(importance.head(60)['feature'])))
```

    ['gender_male' 'gender_female' 'language_de' 'message_post_secs'
     'month_account_created' 'month_first_active' 'age' 'contact_host_secs'
     'first_device_type_Windows Desktop' 'language_pl' 'contact_host_count'
     'change_trip_characteristics_count' 'view_search_results_secs'
     'wishlist_content_update_count' 'confirm_email_link_count' 'language_fr'
     'signup_app_Web' 'message_post_count' 'first_device_type_Mac Desktop'
     'similar_listings_count' 'edit_profile_count' 'first_device_type_iPhone'
     'affiliate_provider_bing' 'update_listing_secs' 'signup_app_iOS'
     'confirm_email_link_secs' 'affiliate_provider_facebook'
     'create_user_count' 'language_en' 'signup_app_Android'
     'first_device_type_Android Phone' 'similar_listings_secs'
     'affiliate_channel_sem-brand' 'signup_method_google' 'edit_profile_secs'
     'secs_elapsed' 'p3_count' 'first_device_type_Desktop (Other)'
     'wishlist_content_update_secs' 'language_nl' 'signup_method_basic'
     'language_zh' 'unavailable_dates_count'
     'first_device_type_Android Tablet' 'signup_method_facebook' 'language_it'
     'affiliate_channel_content' 'dashboard_secs'
     'user_social_connections_secs' 'affiliate_channel_api'
     'user_social_connections_count' 'view_search_results_count' 'language_ko'
     'affiliate_provider_other' 'listing_reviews_count' 'user_profile_count'
     'p3_secs' 'affiliate_channel_sem-non-brand' 'header_userpic_secs'
     'affiliate_provider_google']
    CPU times: user 192 ms, sys: 40.4 ms, total: 233 ms
    Wall time: 232 ms



```python
%%time

## PCA

# Normalize the Data for PCA
X = StandardScaler().fit_transform(k_predictors)

# Perform PCA
sklearn_pca = PCA(n_components=15)
Y_sklearn = sklearn_pca.fit_transform(X)

# Turn PCA Result into a Dataframe
f_pca_components = pd.DataFrame(data=Y_sklearn)

print(
    'The percentage of total variance in the dataset explained by each',
    'component from Sklearn PCA.\n',
    sklearn_pca.explained_variance_ratio_
)


print(
    '\nThe percentage of total variance in the dataset explained by all',
    'components from Sklearn PCA.\n',
    sum(sklearn_pca.explained_variance_ratio_)
)
```

    The percentage of total variance in the dataset explained by each component from Sklearn PCA.
     [0.11076467 0.08473889 0.04687421 0.04012428 0.03777174 0.03326337
     0.03176101 0.03122808 0.02879891 0.02842363 0.02616265 0.02440817
     0.02085169 0.01876248 0.01813148]

    The percentage of total variance in the dataset explained by all components from Sklearn PCA.
     0.5820652748477901
    CPU times: user 3 s, sys: 575 ms, total: 3.58 s
    Wall time: 744 ms


### Feature Selection Using Chi-squared


```python
%%time

## Establish variables based on select K best to be used for modeling

selector = SelectKBest(chi2, k=60)
k_predictors = selector.fit_transform(x,y)
```

    CPU times: user 662 ms, sys: 74.7 ms, total: 736 ms
    Wall time: 488 ms



```python
%%time

## Print Most Variables Deemed Most Important By SelectKbest

importance = pd.concat([pd.DataFrame(x.columns), pd.DataFrame(selector.fit(x, y).scores_)], axis=1)
importance.columns = ['feature', 'importance']
importance = importance.sort_values(by=['importance'], ascending=False)
print(np.array(list(importance.head(60)['feature'])))
```

    ['language_de' 'gender_male' 'language_pl' 'gender_female' 'language_fr'
     'first_device_type_Windows Desktop' 'affiliate_provider_bing'
     'first_device_type_iPhone' 'affiliate_provider_facebook' 'signup_app_iOS'
     'first_device_type_Android Phone' 'signup_app_Android'
     'signup_method_google' 'first_device_type_Desktop (Other)' 'language_nl'
     'language_zh' 'first_device_type_Android Tablet' 'language_it'
     'affiliate_channel_content' 'affiliate_channel_sem-brand'
     'affiliate_channel_api' 'language_ko' 'affiliate_provider_other'
     'first_device_type_Mac Desktop' 'signup_method_facebook'
     'affiliate_channel_sem-non-brand' 'language_es' 'language_sv'
     'affiliate_channel_other' 'first_device_type_iPad' 'language_ja'
     'affiliate_provider_google' 'month_first_active' 'month_account_created'
     'first_device_type_Other/Unknown' 'affiliate_provider_yahoo'
     'affiliate_channel_remarketing' 'affiliate_provider_padmapper'
     'contact_host_secs' 'signup_app_Moweb' 'language_ru'
     'affiliate_channel_seo' 'language_pt' 'confirm_email_link_secs'
     'affiliate_channel_direct' 'affiliate_provider_direct'
     'header_userpic_secs' 'view_search_results_secs' 'signup_method_basic'
     'affiliate_provider_facebook-open-graph' 'affiliate_provider_vast'
     'user_social_connections_count' 'update_listing_secs'
     'contact_host_count' 'similar_listings_count' 'language_el'
     'user_social_connections_secs' 'create_user_count' 'language_fi'
     'similar_listings_secs']
    CPU times: user 661 ms, sys: 39.2 ms, total: 700 ms
    Wall time: 442 ms



```python
%%time

## PCA

# Normalize the Data for PCA
X = StandardScaler().fit_transform(k_predictors)

# Perform PCA
sklearn_pca = PCA(n_components=15)
Y_sklearn = sklearn_pca.fit_transform(X)

# Turn PCA Result into a Dataframe
x_pca_components = pd.DataFrame(data=Y_sklearn)

print(
    'The percentage of total variance in the dataset explained by each',
    'component from Sklearn PCA.\n',
    sklearn_pca.explained_variance_ratio_
)


print(
    '\nThe percentage of total variance in the dataset explained by all',
    'components from Sklearn PCA.\n',
    sum(sklearn_pca.explained_variance_ratio_)
)
```

    The percentage of total variance in the dataset explained by each component from Sklearn PCA.
     [0.0668957  0.05424831 0.04034062 0.0367811  0.03643161 0.03347796
     0.02970867 0.02735605 0.02712204 0.02565093 0.02282432 0.02211144
     0.01920074 0.01824602 0.01815636]

    The percentage of total variance in the dataset explained by all components from Sklearn PCA.
     0.47855186753063206
    CPU times: user 2.88 s, sys: 751 ms, total: 3.63 s
    Wall time: 696 ms


### Declaring Variables


```python
%%time

## Train Test Split the Four Sets of Feature and Outcome Variables

# Unaltered Features
x_train, x_test, y_train, y_test = train_test_split(pca_components, y, test_size=0.2, random_state=20)

# F-Value
fx_train, fx_test, fy_train, fy_test = train_test_split(f_pca_components, y, test_size=0.2, random_state=22)

# Chi-squared
xx_train, xx_test, xy_train, xy_test = train_test_split(x_pca_components, y, test_size=0.2, random_state=21)

# Deep Learning
X_train, X_test, Y_train, Y_test = train_test_split(np.asarray(f_pca_components), np.asarray(Y), test_size=0.2, random_state=20)

```

    CPU times: user 172 ms, sys: 64.7 ms, total: 237 ms
    Wall time: 161 ms


Training and testing sets of four variables were generated to be used in modeling.
The x and y variables represent the variables to be used for modeling that reflect the all of the useful features of the data.
The px and py variables represent the variables to be used for modeling that reflect PCA components of the initial features.
The kx and ky variables represent the variables to be used for modeling that reflect features chosen by selectKbest.
The X and Y variables represent the variables converted to arrays to be used for deep learning.

## Supervised Modeling using All Features

### K Nearest Neighbors


```python
%%time

## Train and Fit Model

knn = neighbors.KNeighborsClassifier(n_neighbors=1).fit(x_train, y_train)

```

    CPU times: user 227 ms, sys: 111 ms, total: 338 ms
    Wall time: 466 ms



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(knn.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(knn, x_train, y_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, knn.predict(x_test)))+'\n')

print(classification_report(y_test, knn.predict(x_test)))

```

    accuracy score:
    0.9466161616161616

    cross validation:
    [0.93645084 0.93567325 0.92809343 0.9324367  0.93382167]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    0    0    0 1830    0    0    0    0   10    1]
     [   0    0    0    0    0 1782    0    0    0    0    0]
     [   0    0    0    0    2    0 1787    0    0    0    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  17   56   18   69  155   74   84   19   14  867  355]
     [   3    4    1   14   20    9   10    2    2  118 1609]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1839
              CA       0.97      1.00      0.98      1855
              DE       0.99      1.00      0.99      1787
              ES       0.96      1.00      0.98      1814
              FR       0.91      0.99      0.95      1841
              GB       0.96      1.00      0.98      1782
              IT       0.95      1.00      0.97      1789
              NL       0.99      1.00      0.99      1794
              PT       0.99      1.00      1.00      1779
              US       0.87      0.50      0.64      1728
           other       0.82      0.90      0.86      1792

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.94      0.94      0.94     19800
    weighted avg       0.94      0.95      0.94     19800

    CPU times: user 8.44 s, sys: 128 ms, total: 8.57 s
    Wall time: 8.82 s


### Decision Tree


```python
%%time

## Train and Fit Model

dt = tree.DecisionTreeClassifier()

parameters = {
              'max_features': list(range(8,15)),
              'max_depth': list(range(24,34))
             }

acc_scorer = make_scorer(accuracy_score)

decision_tree = GridSearchCV(dt, parameters, scoring=acc_scorer).fit(x_train,  y_train)

print(decision_tree.best_params_)

```

    {'max_depth': 33, 'max_features': 9}
    CPU times: user 2min 47s, sys: 2.62 s, total: 2min 49s
    Wall time: 2min 57s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(decision_tree.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(decision_tree, x_train, y_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, decision_tree.predict(x_test)))+'\n')

print(classification_report(y_test, decision_tree.predict(x_test)))

```

    accuracy score:
    0.9146464646464646

    cross validation:
    [0.9293197  0.92437346 0.92171717 0.92517522 0.92390755]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1810    0   44    0    1    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    5    1 1794    0    2    0    0    0    5    7]
     [   0    0    7   54 1727   11   12    2    0   12   16]
     [   0    0    0   51   13 1691    6    7    0    8    6]
     [   0    0    6   50   11    7 1700    0    0   10    5]
     [   0    0    0   12    0    0    0 1782    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  23   71   32  127  175   89  108   28   15  710  350]
     [   7    8    5   67   35   33   18   10    0  118 1491]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1839
              CA       0.96      0.98      0.97      1855
              DE       0.97      1.00      0.99      1787
              ES       0.82      0.99      0.89      1814
              FR       0.88      0.94      0.91      1841
              GB       0.92      0.95      0.94      1782
              IT       0.92      0.95      0.94      1789
              NL       0.97      0.99      0.98      1794
              PT       0.99      1.00      1.00      1779
              US       0.82      0.41      0.55      1728
           other       0.80      0.83      0.81      1792

       micro avg       0.91      0.91      0.91     19800
       macro avg       0.91      0.91      0.91     19800
    weighted avg       0.91      0.91      0.91     19800

    CPU times: user 11min 23s, sys: 9.49 s, total: 11min 33s
    Wall time: 11min 55s


### Random Forest


```python
%%time

## Train and Fit Model

rf = ensemble.RandomForestClassifier()

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(x_train,  y_train)

print(rfc.best_params_)
```

    {'max_depth': 90, 'max_features': 'sqrt'}
    CPU times: user 1min 33s, sys: 1.81 s, total: 1min 35s
    Wall time: 1min 37s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(rfc.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(rfc, x_train, y_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, rfc.predict(x_test)))+'\n')

print(classification_report(y_test, rfc.predict(x_test)))

```

    accuracy score:
    0.9535858585858585

    cross validation:
    [0.93790231 0.93769333 0.93775253 0.93673044 0.93811569]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    0    0 1814    0    0    0    0    0    0    0]
     [   0    0    0    7 1827    0    0    0    0    7    0]
     [   0    0    0    0    0 1782    0    0    0    0    0]
     [   0    0    0    0    2    0 1787    0    0    0    0]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [   6   32   11   65  174   63   64    7    6 1021  279]
     [   3    2    4   14   25    7   16    0    1  124 1596]]

                  precision    recall  f1-score   support

              AU       1.00      1.00      1.00      1839
              CA       0.98      1.00      0.99      1855
              DE       0.99      1.00      1.00      1787
              ES       0.95      1.00      0.98      1814
              FR       0.90      0.99      0.94      1841
              GB       0.96      1.00      0.98      1782
              IT       0.96      1.00      0.98      1789
              NL       1.00      1.00      1.00      1794
              PT       1.00      1.00      1.00      1779
              US       0.89      0.59      0.71      1728
           other       0.85      0.89      0.87      1792

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.95      0.95      0.95     19800
    weighted avg       0.95      0.95      0.95     19800

    CPU times: user 6min 34s, sys: 8.79 s, total: 6min 43s
    Wall time: 6min 56s


### Gradient Boost


```python
%%time

## Train and Fit Model

clf = ensemble.GradientBoostingClassifier(n_estimators=900, min_samples_split=4).fit(x_train,  y_train)

```

    CPU times: user 17min 20s, sys: 58.1 s, total: 18min 18s
    Wall time: 18min 58s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(clf.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(clf, x_train, y_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, clf.predict(x_test)))+'\n')

print(classification_report(y_test, clf.predict(x_test)))

```

    accuracy score:
    0.8924242424242425

    cross validation:
    [0.88463966 0.8847295  0.87916667 0.88242723 0.87964132]

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1787    0    0    0    0    0    0    0    0]
     [   0    7    2 1755    6    7    6    0    0   11   20]
     [   3   15    0   21 1586   24   45    1    0   88   58]
     [   0    7    3    8   11 1723    0    0    0   19   11]
     [   0    5    0   16   33    8 1663    4    0   45   15]
     [   0    0    0    0    0    0    0 1794    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  12   63   20  108  210  101  145   16    3  721  329]
     [   3   32    6   56  138   55   73   12    3  246 1168]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      1.00      1839
              CA       0.93      1.00      0.97      1855
              DE       0.98      1.00      0.99      1787
              ES       0.89      0.97      0.93      1814
              FR       0.80      0.86      0.83      1841
              GB       0.90      0.97      0.93      1782
              IT       0.86      0.93      0.89      1789
              NL       0.98      1.00      0.99      1794
              PT       1.00      1.00      1.00      1779
              US       0.64      0.42      0.50      1728
           other       0.73      0.65      0.69      1792

       micro avg       0.89      0.89      0.89     19800
       macro avg       0.88      0.89      0.88     19800
    weighted avg       0.88      0.89      0.89     19800

    CPU times: user 1h 6min 38s, sys: 2min 55s, total: 1h 9min 34s
    Wall time: 1h 10min 59s


### MLP Neural Network


```python
%%time

## Train and Fit Model

mlp = MLPClassifier(hidden_layer_sizes=(256,256,128,)).fit(x_train, y_train)
```

    CPU times: user 30min 5s, sys: 3min 13s, total: 33min 19s
    Wall time: 8min 55s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(mlp.score(x_test, y_test))+'\n')

print("cross validation:\n" + str(cross_val_score(mlp, x_train, y_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(y_test, mlp.predict(x_test)))+'\n')

print(classification_report(y_test, mlp.predict(x_test)))
```

    accuracy score:
    0.8792929292929293

    cross validation:
    [0.91171274 0.86837952 0.8842803  0.90080192 0.87200051]

    confusion matrix:
    [[1807    0    0    0   19    0    0    0    0   13    0]
     [   0 1638    2    5   75    0   29    0    3   71   32]
     [   0    0 1770    0   12    0    0    0    0    0    5]
     [   0    4    0 1761   36    0    9    0    0    0    4]
     [   0    6    4   10 1761    0   27    2    0   25    6]
     [   0    0    0    2   41 1673   30    0    0   31    5]
     [   0    0    2    7   12    0 1727    4    0   36    1]
     [   0    0    0    0    3    8    0 1783    0    0    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  13   40   28  123  306   57  185   25   13  699  239]
     [   8   12   12   48  169   15  104    5    9  398 1012]]

                  precision    recall  f1-score   support

              AU       0.99      0.98      0.99      1839
              CA       0.96      0.88      0.92      1855
              DE       0.97      0.99      0.98      1787
              ES       0.90      0.97      0.93      1814
              FR       0.72      0.96      0.82      1841
              GB       0.95      0.94      0.95      1782
              IT       0.82      0.97      0.89      1789
              NL       0.98      0.99      0.99      1794
              PT       0.99      1.00      0.99      1779
              US       0.55      0.40      0.47      1728
           other       0.78      0.56      0.65      1792

       micro avg       0.88      0.88      0.88     19800
       macro avg       0.87      0.88      0.87     19800
    weighted avg       0.88      0.88      0.87     19800

    CPU times: user 1h 47min 29s, sys: 11min 21s, total: 1h 58min 51s
    Wall time: 32min 11s



The models that relied on the dataset’s unreduced features, in general, had the best accuracy in the study. An advantage of using all of the useful features is that as much meaningful variance was captured by the models as possible. A downside to this type of feature preparation that did stand out is the lack of efficiency. Since feature reduction didn’t take place with these models, their performance suffered and they had the longest runtimes. This method of feature selection also risks including features with variance that doesn’t aid in the predictive power of the models. However, this potential disadvantage didn’t hamper the model’s ability to perform well because many of the features that would noticeably have a negative effect on the models were already left out.



## Modeling the Data using Features with High F-Values

### K Nearest Neighbors


```python
%%time

## Train and Fit Model

f_knn = neighbors.KNeighborsClassifier(n_neighbors=1).fit(fx_train, fy_train)

```

    CPU times: user 195 ms, sys: 50.9 ms, total: 246 ms
    Wall time: 303 ms



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(f_knn.score(fx_test, fy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(f_knn, fx_train, fy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(fy_test, f_knn.predict(fx_test)))+'\n')

print(classification_report(fy_test, f_knn.predict(fx_test)))

```

    accuracy score:
    0.9421212121212121

    cross validation:
    [0.93265164 0.93208357 0.93522318 0.93295031 0.93363223]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1831    0    0    0    0    0    0    0]
     [   0    0    0    0 1831    0    0    2    0    7   10]
     [   0    0    0    0    0 1788    0    0    0    0    0]
     [   0    0    0    0    0    0 1833    0    0    0    0]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  19   54   19   84  150   79  111   21    9  873  356]
     [   4   11    6    9   39    5   21    0    1  129 1588]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1770
              CA       0.96      1.00      0.98      1775
              DE       0.99      1.00      0.99      1761
              ES       0.95      1.00      0.98      1831
              FR       0.91      0.99      0.95      1850
              GB       0.96      1.00      0.98      1788
              IT       0.93      1.00      0.97      1833
              NL       0.99      1.00      0.99      1806
              PT       0.99      1.00      1.00      1798
              US       0.87      0.49      0.63      1775
           other       0.81      0.88      0.84      1813

       micro avg       0.94      0.94      0.94     19800
       macro avg       0.94      0.94      0.94     19800
    weighted avg       0.94      0.94      0.94     19800

    CPU times: user 9.46 s, sys: 175 ms, total: 9.64 s
    Wall time: 10.8 s


### Decision Tree


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': list(range(8,15)),
              'max_depth': list(range(24,34))
             }

acc_scorer = make_scorer(accuracy_score)

f_decision_tree = GridSearchCV(dt, parameters, scoring=acc_scorer).fit(fx_train,  fy_train)

print(f_decision_tree.best_params_)

```

    {'max_depth': 33, 'max_features': 10}
    CPU times: user 2min 56s, sys: 1.47 s, total: 2min 57s
    Wall time: 2min 59s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(f_decision_tree.score(fx_test, fy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(f_decision_tree, fx_train, fy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(fy_test, f_decision_tree.predict(fx_test)))+'\n')

print(classification_report(fy_test, f_decision_tree.predict(fx_test)))

```

    accuracy score:
    0.9366161616161616

    cross validation:
    [0.8940226  0.92671842 0.9091483  0.90870636 0.92662288]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1823    2    0    6    0    0    0    0]
     [   1    3    0    2 1802    0    7    0    0   18   17]
     [   0    0    0    2    0 1766   11    0    0    6    3]
     [   0    1    0    0    7    0 1819    0    0    4    2]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  17   46   26   80  166   73  118   32    7  865  345]
     [   7   12   12   16   28   11   18    2    2  145 1560]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1770
              CA       0.97      1.00      0.98      1775
              DE       0.98      1.00      0.99      1761
              ES       0.95      1.00      0.97      1831
              FR       0.90      0.97      0.93      1850
              GB       0.95      0.99      0.97      1788
              IT       0.92      0.99      0.95      1833
              NL       0.98      1.00      0.99      1806
              PT       1.00      1.00      1.00      1798
              US       0.83      0.49      0.62      1775
           other       0.81      0.86      0.83      1813

       micro avg       0.94      0.94      0.94     19800
       macro avg       0.93      0.94      0.93     19800
    weighted avg       0.93      0.94      0.93     19800

    CPU times: user 11min 55s, sys: 6.48 s, total: 12min 2s
    Wall time: 12min 8s


### Random Forest


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

f_rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(fx_train, fy_train)

print(f_rfc.best_params_)

```

    {'max_depth': 85, 'max_features': 'sqrt'}
    CPU times: user 1min 35s, sys: 1.56 s, total: 1min 37s
    Wall time: 1min 38s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(f_rfc.score(fx_test, fy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(f_rfc, fx_train, fy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(fy_test, f_rfc.predict(fx_test)))+'\n')

print(classification_report(fy_test, f_rfc.predict(fx_test)))

```

    accuracy score:
    0.9503030303030303

    cross validation:
    [0.93890046 0.93681752 0.93831681 0.93730665 0.93761051]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    0    0 1831    0    0    0    0    0    0    0]
     [   0    5    0    0 1827    1    1    2    0   11    3]
     [   0    0    0    0    0 1788    0    0    0    0    0]
     [   0    0    0    0    0    0 1833    0    0    0    0]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  10   38    9   63  164   55   75    9    2 1048  302]
     [   0    7    2   11   40    8   11    0    0  155 1579]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      1.00      1770
              CA       0.97      1.00      0.99      1775
              DE       0.99      1.00      1.00      1761
              ES       0.96      1.00      0.98      1831
              FR       0.90      0.99      0.94      1850
              GB       0.97      1.00      0.98      1788
              IT       0.95      1.00      0.98      1833
              NL       0.99      1.00      1.00      1806
              PT       1.00      1.00      1.00      1798
              US       0.86      0.59      0.70      1775
           other       0.84      0.87      0.85      1813

       micro avg       0.95      0.95      0.95     19800
       macro avg       0.95      0.95      0.95     19800
    weighted avg       0.95      0.95      0.95     19800

    CPU times: user 6min 34s, sys: 6.38 s, total: 6min 40s
    Wall time: 6min 44s


### Gradient Boost


```python
%%time

## Train and Fit Model

f_clf = ensemble.GradientBoostingClassifier(n_estimators=900, min_samples_split=4).fit(fx_train, fy_train)
```

    CPU times: user 16min 24s, sys: 51.8 s, total: 17min 15s
    Wall time: 18min 9s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(f_clf.score(fx_test, fy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(f_clf, fx_train, fy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(fy_test, f_clf.predict(fx_test)))+'\n')

print(classification_report(fy_test, f_clf.predict(fx_test)))

```

    accuracy score:
    0.893080808080808

    cross validation:
    [0.88531213 0.87956826 0.88522003 0.88174759 0.88185148]

    confusion matrix:
    [[1770    0    0    0    0    0    0    0    0    0    0]
     [   0 1775    0    0    0    0    0    0    0    0    0]
     [   0    0 1761    0    0    0    0    0    0    0    0]
     [   0    2    0 1790    8    0    4    0    0   11   16]
     [   0   14    4   21 1578    9   42    5    0   99   78]
     [   0   10    0    9   17 1747    3    0    0    2    0]
     [   0    6    0   18   23   11 1731    3    0   25   16]
     [   0    0    0    0    0    0    0 1806    0    0    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [   5   64   17  109  186  124  161   24    1  744  340]
     [   3   32   12   40  111   58   96    8    0  270 1183]]

                  precision    recall  f1-score   support

              AU       1.00      1.00      1.00      1770
              CA       0.93      1.00      0.97      1775
              DE       0.98      1.00      0.99      1761
              ES       0.90      0.98      0.94      1831
              FR       0.82      0.85      0.84      1850
              GB       0.90      0.98      0.93      1788
              IT       0.85      0.94      0.89      1833
              NL       0.98      1.00      0.99      1806
              PT       1.00      1.00      1.00      1798
              US       0.65      0.42      0.51      1775
           other       0.72      0.65      0.69      1813

       micro avg       0.89      0.89      0.89     19800
       macro avg       0.88      0.89      0.89     19800
    weighted avg       0.88      0.89      0.89     19800

    CPU times: user 1h 4min 13s, sys: 2min 39s, total: 1h 6min 53s
    Wall time: 1h 7min 27s


### MLP Neural Network


```python
%%time

## Train and Fit Model

f_mlp = MLPClassifier(hidden_layer_sizes=(256,256,128,)).fit(fx_train, fy_train)
```

    CPU times: user 25min 21s, sys: 2min 52s, total: 28min 14s
    Wall time: 7min 54s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(f_mlp.score(fx_test, fy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(f_mlp, fx_train, fy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(fy_test, f_mlp.predict(fx_test)))+'\n')

print(classification_report(fy_test, f_mlp.predict(fx_test)))

```

    accuracy score:
    0.8906060606060606

    cross validation:
    [0.88935176 0.89414884 0.89967801 0.86842604 0.89492296]

    confusion matrix:
    [[1720    0    0    0    0    0    0    0    0   28   22]
     [   0 1712    0    6    3   14    8    0    5   18    9]
     [   0    0 1752    2    0    0    0    0    0    0    7]
     [   0   10    0 1765    3    6    4    0   11   16   16]
     [   2    7    8   22 1631   22   15   11    2   95   35]
     [   0    0    0    5    2 1762    9    3    0    6    1]
     [   1    1    3    8    3    8 1751    0    3   40   15]
     [   0    0    0    0    0    0    0 1798    0    8    0]
     [   0    0    0    0    0    0    0    0 1798    0    0]
     [  19   50   19   92  182  145  154   25   14  773  302]
     [   7   12    9   24   62   66   51    3    7  400 1172]]

                  precision    recall  f1-score   support

              AU       0.98      0.97      0.98      1770
              CA       0.96      0.96      0.96      1775
              DE       0.98      0.99      0.99      1761
              ES       0.92      0.96      0.94      1831
              FR       0.86      0.88      0.87      1850
              GB       0.87      0.99      0.92      1788
              IT       0.88      0.96      0.92      1833
              NL       0.98      1.00      0.99      1806
              PT       0.98      1.00      0.99      1798
              US       0.56      0.44      0.49      1775
           other       0.74      0.65      0.69      1813

       micro avg       0.89      0.89      0.89     19800
       macro avg       0.88      0.89      0.88     19800
    weighted avg       0.88      0.89      0.88     19800

    CPU times: user 2h 1min 57s, sys: 12min 29s, total: 2h 14min 27s
    Wall time: 36min 3s




The accuracy scores of the models that used features with high F values were similar to those the models that used unaltered features. This form of feature selection likely removed features that were not vital to the predictive accuracy of the models. This form of feature selection has the advantage advantage of reducing computational complexity and runtimes and in this case, this made up lack of difference in accuracy of the better performing model types.



## Modeling the Data using Features Chosen with Chi-Squared

### K Nearest Neighbors


```python
%%time

## Train and Fit Model

x_knn = neighbors.KNeighborsClassifier(n_neighbors=1).fit(xx_train, xy_train)

```

    CPU times: user 225 ms, sys: 61.6 ms, total: 287 ms
    Wall time: 365 ms



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(x_knn.score(xx_test, xy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(x_knn, xx_train, xy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(xy_test, x_knn.predict(xx_test)))+'\n')

print(classification_report(xy_test, x_knn.predict(xx_test)))

```

    accuracy score:
    0.934949494949495

    cross validation:
    [0.92476647 0.92362075 0.92273215 0.92334891 0.92497632]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1796    0    0    0    0    0    3    0    0    8]
     [   0    0 1838    0    0    0    3    0    0    0    0]
     [   0    3    0 1803    0    0    1    2    0    5    0]
     [   3    5    1    4 1786    6    2    4    0    6    5]
     [   0    6    8    4    0 1800    0    0    0    6    8]
     [   2    2    3    6    0    2 1793    2    0    2    3]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  14   62   38   79  170   91   86   29   12  859  360]
     [   1    5   14   12   34   14   14    4    3  131 1526]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1759
              CA       0.96      0.99      0.97      1807
              DE       0.97      1.00      0.98      1841
              ES       0.94      0.99      0.97      1814
              FR       0.90      0.98      0.94      1822
              GB       0.94      0.98      0.96      1832
              IT       0.94      0.99      0.97      1815
              NL       0.98      1.00      0.99      1781
              PT       0.99      1.00      1.00      1771
              US       0.85      0.48      0.61      1800
           other       0.80      0.87      0.83      1758

       micro avg       0.93      0.93      0.93     19800
       macro avg       0.93      0.93      0.93     19800
    weighted avg       0.93      0.93      0.93     19800

    CPU times: user 7.13 s, sys: 88.5 ms, total: 7.22 s
    Wall time: 7.34 s


### Decision Tree


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': list(range(8,15)),
              'max_depth': list(range(24,34))
             }

acc_scorer = make_scorer(accuracy_score)

x_decision_tree = GridSearchCV(dt, parameters, scoring=acc_scorer).fit(xx_train, xy_train)

print(x_decision_tree.best_params_)

```

    {'max_depth': 33, 'max_features': 14}
    CPU times: user 2min 49s, sys: 1.95 s, total: 2min 51s
    Wall time: 2min 56s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(x_decision_tree.score(xx_test, xy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(x_decision_tree, xx_train, xy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(xy_test, x_decision_tree.predict(xx_test)))+'\n')

print(classification_report(xy_test, x_decision_tree.predict(xx_test)))

```

    accuracy score:
    0.9208080808080809

    cross validation:
    [0.90873517 0.90626184 0.90455148 0.9112893  0.85923587]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1785    0    8    4    1    0    7    0    2    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   0    3    0 1788    1    1    1   12    0    8    0]
     [   3    9    1   10 1741   12    3   17    0   24    2]
     [   0    5    8   11    2 1773    1   10    0   19    3]
     [   2   12    5   17    1    6 1760    2    0    6    4]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  26   82   48  110  170   97  107   42    9  791  318]
     [   2   20   19   36   48   20   13   11    3  144 1442]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1759
              CA       0.93      0.99      0.96      1807
              DE       0.96      1.00      0.98      1841
              ES       0.90      0.99      0.94      1814
              FR       0.89      0.96      0.92      1822
              GB       0.93      0.97      0.95      1832
              IT       0.93      0.97      0.95      1815
              NL       0.95      1.00      0.97      1781
              PT       0.99      1.00      1.00      1771
              US       0.80      0.44      0.57      1800
           other       0.82      0.82      0.82      1758

       micro avg       0.92      0.92      0.92     19800
       macro avg       0.92      0.92      0.91     19800
    weighted avg       0.92      0.92      0.91     19800

    CPU times: user 11min 48s, sys: 10.4 s, total: 11min 58s
    Wall time: 12min 25s


### Random Forest


```python
%%time

## Train and Fit Model

parameters = {
              'max_features': ['log2', 'sqrt','auto'],
              'max_depth': list(np.arange(85, 111, 5)),
             }

acc_scorer = make_scorer(accuracy_score)

x_rfc = GridSearchCV(rf, parameters, scoring=acc_scorer).fit(xx_train, xy_train)

print(x_rfc.best_params_)

```

    {'max_depth': 110, 'max_features': 'auto'}
    CPU times: user 1min 48s, sys: 2.86 s, total: 1min 51s
    Wall time: 2min 13s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(x_rfc.score(xx_test, xy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(x_rfc, xx_train, xy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(xy_test, x_rfc.predict(xx_test)))+'\n')

print(classification_report(xy_test, x_rfc.predict(xx_test)))

```

    accuracy score:
    0.9348989898989899

    cross validation:
    [0.92552386 0.92393637 0.92386844 0.9222124  0.92194506]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1803    0    0    0    1    0    3    0    0    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   0    3    0 1804    1    1    1    2    0    2    0]
     [   3    3    1    4 1786    9    2    4    0    8    2]
     [   0    1    8    6    1 1813    0    0    0    3    0]
     [   2    2    3    6    0    2 1798    2    0    0    0]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  14   70   35   86  198  102   90   26    6  866  307]
     [   1   11   12   15   37   21   20    3    3  146 1489]]

                  precision    recall  f1-score   support

              AU       0.99      1.00      0.99      1759
              CA       0.95      1.00      0.97      1807
              DE       0.97      1.00      0.98      1841
              ES       0.94      0.99      0.97      1814
              FR       0.88      0.98      0.93      1822
              GB       0.93      0.99      0.96      1832
              IT       0.94      0.99      0.97      1815
              NL       0.98      1.00      0.99      1781
              PT       0.99      1.00      1.00      1771
              US       0.84      0.48      0.61      1800
           other       0.83      0.85      0.84      1758

       micro avg       0.93      0.93      0.93     19800
       macro avg       0.93      0.93      0.93     19800
    weighted avg       0.93      0.93      0.93     19800

    CPU times: user 7min, sys: 9.29 s, total: 7min 10s
    Wall time: 7min 37s


### Gradient Boost


```python
%%time

## Running the Model with Best Parameters for Faster Evaluation

x_clf = ensemble.GradientBoostingClassifier(n_estimators=900, min_samples_split=4).fit(xx_train, xy_train)
```

    CPU times: user 17min 17s, sys: 57.8 s, total: 18min 15s
    Wall time: 19min 57s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(x_clf.score(xx_test, xy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(x_clf, xx_train, xy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(xy_test, x_clf.predict(xx_test)))+'\n')

print(classification_report(xy_test, x_clf.predict(xx_test)))

```

    accuracy score:
    0.8755050505050505

    cross validation:
    [0.8686569  0.87028153 0.8664857  0.86999621 0.86769814]

    confusion matrix:
    [[1759    0    0    0    0    0    0    0    0    0    0]
     [   0 1795    0    0    4    0    5    3    0    0    0]
     [   0    0 1841    0    0    0    0    0    0    0    0]
     [   2    5    0 1749   11    6    6    3    0   23    9]
     [   3   22    3   17 1544   20   36   10    1  100   66]
     [   1   22    9   22    7 1741   10    0    0   15    5]
     [   2   17    7   17   20   27 1644    2    0   53   26]
     [   0    0    0    0    0    0    0 1781    0    0    0]
     [   0    0    0    0    0    0    0    0 1771    0    0]
     [  22   96   46  104  204  142  142   44    7  649  344]
     [   3   42   40   63  113   62   86   25    4  259 1061]]

                  precision    recall  f1-score   support

              AU       0.98      1.00      0.99      1759
              CA       0.90      0.99      0.94      1807
              DE       0.95      1.00      0.97      1841
              ES       0.89      0.96      0.92      1814
              FR       0.81      0.85      0.83      1822
              GB       0.87      0.95      0.91      1832
              IT       0.85      0.91      0.88      1815
              NL       0.95      1.00      0.98      1781
              PT       0.99      1.00      1.00      1771
              US       0.59      0.36      0.45      1800
           other       0.70      0.60      0.65      1758

       micro avg       0.88      0.88      0.88     19800
       macro avg       0.86      0.88      0.87     19800
    weighted avg       0.86      0.88      0.87     19800

    CPU times: user 1h 7min 46s, sys: 3min 1s, total: 1h 10min 48s
    Wall time: 1h 16min 47s


### MLP Neural Network


```python
%%time

## Train and Fit Model

x_mlp = MLPClassifier(hidden_layer_sizes=(256,256,128,)).fit(xx_train, xy_train)
```

    CPU times: user 55min 3s, sys: 6min 22s, total: 1h 1min 25s
    Wall time: 18min 4s



```python
%%time

## Model Evaluation

print("accuracy score:\n" + str(x_mlp.score(xx_test, xy_test))+'\n')

print("cross validation:\n" + str(cross_val_score(x_mlp, xx_train, xy_train, cv=5))+'\n')

print("confusion matrix:\n" + str(confusion_matrix(xy_test, x_mlp.predict(xx_test)))+'\n')

print(classification_report(xy_test, x_mlp.predict(xx_test)))
```

    accuracy score:
    0.8042929292929293

    cross validation:
    [0.79493815 0.79838404 0.77267849 0.8023109  0.78598042]

    confusion matrix:
    [[1747    0    0    0    0    0   12    0    0    0    0]
     [  16 1606   12   45   22    9   30   19   18   14   16]
     [   7   10 1743   41    7   14    0    7   12    0    0]
     [  16   28   12 1635   15   14   24   24   13   12   21]
     [  43   26   27   82 1336   35   56   35   19   82   81]
     [  15   40   30   44   47 1483   30   38    8   38   59]
     [  24   30   26   84   17   41 1481   26    5   46   35]
     [   8    4   25   41   22   10    4 1609   12   15   31]
     [  23    0    0    0    0    0   18    0 1730    0    0]
     [  33   91   57  176  200  117  149   64   25  558  330]
     [  21   40   46  117   96   45   98   29   35  234  997]]

                  precision    recall  f1-score   support

              AU       0.89      0.99      0.94      1759
              CA       0.86      0.89      0.87      1807
              DE       0.88      0.95      0.91      1841
              ES       0.72      0.90      0.80      1814
              FR       0.76      0.73      0.75      1822
              GB       0.84      0.81      0.82      1832
              IT       0.78      0.82      0.80      1815
              NL       0.87      0.90      0.89      1781
              PT       0.92      0.98      0.95      1771
              US       0.56      0.31      0.40      1800
           other       0.64      0.57      0.60      1758

       micro avg       0.80      0.80      0.80     19800
       macro avg       0.79      0.80      0.79     19800
    weighted avg       0.79      0.80      0.79     19800

    CPU times: user 2h 54min 47s, sys: 18min 9s, total: 3h 12min 57s
    Wall time: 53min 2s




When it comes to comparing the accuracy scores of the full featured models and models that used features based on their chi-squared scores, most of the full featured models outperformed their counterparts. However, the significant drop in the runtimes of the models that used features with high chi-squared values does make this form of feature selection better than simply retrieving PCA components from all generated features. However feature selection based on F values also had this effect without compromising predictive accuracy.




## Modeling Data using Deep Learning

### Convolutional Neural Network


```python
%%time

## Reshaping Data

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)

## Building the Model

model = Sequential()
model.add(Conv1D(32, (3), input_shape=(X_train.shape[1],1), activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names), activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

model.summary()
```

    WARNING: Logging before flag parsing goes to stderr.
    W1207 11:49:46.126329 4648428992 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

    W1207 11:49:46.761920 4648428992 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

    W1207 11:49:46.952773 4648428992 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

    W1207 11:49:47.171415 4648428992 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

    W1207 11:49:47.308588 4648428992 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

    W1207 11:49:47.366309 4648428992 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.



    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv1d_1 (Conv1D)            (None, 13, 32)            128       
    _________________________________________________________________
    max_pooling1d_1 (MaxPooling1 (None, 6, 32)             0         
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 192)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 128)               24704     
    _________________________________________________________________
    dense_2 (Dense)              (None, 256)               33024     
    _________________________________________________________________
    dense_3 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dense_4 (Dense)              (None, 11)                2827      
    =================================================================
    Total params: 126,475
    Trainable params: 126,475
    Non-trainable params: 0
    _________________________________________________________________
    CPU times: user 180 ms, sys: 535 ms, total: 716 ms
    Wall time: 1.38 s



```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train, Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```

    W1207 11:49:50.732538 4648428992 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    W1207 11:49:50.845293 4648428992 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.



    Train on 71280 samples, validate on 7920 samples
    Epoch 1/100
    71280/71280 [==============================] - 4s 62us/step - loss: 2.0754 - acc: 0.2674 - val_loss: 1.7693 - val_acc: 0.3907
    Epoch 2/100
    71280/71280 [==============================] - 4s 50us/step - loss: 1.5321 - acc: 0.4649 - val_loss: 1.3470 - val_acc: 0.5335
    Epoch 3/100
    71280/71280 [==============================] - 3s 48us/step - loss: 1.2200 - acc: 0.5740 - val_loss: 1.1159 - val_acc: 0.6042
    Epoch 4/100
    71280/71280 [==============================] - 3s 47us/step - loss: 1.0199 - acc: 0.6449 - val_loss: 0.9760 - val_acc: 0.6519
    Epoch 5/100
    71280/71280 [==============================] - 4s 55us/step - loss: 0.8814 - acc: 0.6928 - val_loss: 0.8718 - val_acc: 0.6920
    Epoch 6/100
    71280/71280 [==============================] - 4s 51us/step - loss: 0.7802 - acc: 0.7292 - val_loss: 0.7541 - val_acc: 0.7379
    Epoch 7/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.6907 - acc: 0.7605 - val_loss: 0.7697 - val_acc: 0.7477
    Epoch 8/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.6323 - acc: 0.7799 - val_loss: 0.7021 - val_acc: 0.7600
    Epoch 9/100
    71280/71280 [==============================] - 5s 64us/step - loss: 0.5764 - acc: 0.7990 - val_loss: 0.6231 - val_acc: 0.7824
    Epoch 10/100
    71280/71280 [==============================] - 4s 56us/step - loss: 0.5392 - acc: 0.8113 - val_loss: 0.5914 - val_acc: 0.7942
    Epoch 11/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.5014 - acc: 0.8239 - val_loss: 0.5739 - val_acc: 0.8112
    Epoch 12/100
    71280/71280 [==============================] - 4s 50us/step - loss: 0.4711 - acc: 0.8341 - val_loss: 0.5616 - val_acc: 0.8076
    Epoch 13/100
    71280/71280 [==============================] - 4s 61us/step - loss: 0.4515 - acc: 0.8418 - val_loss: 0.5795 - val_acc: 0.8056
    Epoch 14/100
    71280/71280 [==============================] - 4s 57us/step - loss: 0.4198 - acc: 0.8535 - val_loss: 0.5228 - val_acc: 0.8269
    Epoch 15/100
    71280/71280 [==============================] - 4s 53us/step - loss: 0.4150 - acc: 0.8540 - val_loss: 0.4822 - val_acc: 0.8381
    Epoch 16/100
    71280/71280 [==============================] - 4s 51us/step - loss: 0.3935 - acc: 0.8614 - val_loss: 0.5105 - val_acc: 0.8322
    Epoch 17/100
    71280/71280 [==============================] - 4s 51us/step - loss: 0.3717 - acc: 0.8696 - val_loss: 0.4545 - val_acc: 0.8460
    Epoch 18/100
    71280/71280 [==============================] - 3s 48us/step - loss: 0.3612 - acc: 0.8744 - val_loss: 0.5017 - val_acc: 0.8321
    Epoch 19/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.3441 - acc: 0.8797 - val_loss: 0.4954 - val_acc: 0.8466
    Epoch 20/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.3405 - acc: 0.8829 - val_loss: 0.4622 - val_acc: 0.8558
    Epoch 21/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.3259 - acc: 0.8859 - val_loss: 0.4311 - val_acc: 0.8571
    Epoch 22/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.3164 - acc: 0.8917 - val_loss: 0.4508 - val_acc: 0.8567
    Epoch 23/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.3218 - acc: 0.8902 - val_loss: 0.4492 - val_acc: 0.8561
    Epoch 24/100
    71280/71280 [==============================] - 4s 50us/step - loss: 0.2982 - acc: 0.8981 - val_loss: 0.4569 - val_acc: 0.8655
    Epoch 25/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.2868 - acc: 0.9009 - val_loss: 0.4925 - val_acc: 0.8497
    Epoch 26/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.2860 - acc: 0.9032 - val_loss: 0.4643 - val_acc: 0.8484
    Epoch 27/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2896 - acc: 0.9015 - val_loss: 0.4401 - val_acc: 0.8705
    Epoch 28/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2658 - acc: 0.9076 - val_loss: 0.4348 - val_acc: 0.8720
    Epoch 29/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.2646 - acc: 0.9100 - val_loss: 0.4165 - val_acc: 0.8789
    Epoch 30/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.2480 - acc: 0.9144 - val_loss: 0.4487 - val_acc: 0.8721
    Epoch 31/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.2598 - acc: 0.9118 - val_loss: 0.4299 - val_acc: 0.8725
    Epoch 32/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2544 - acc: 0.9142 - val_loss: 0.4550 - val_acc: 0.8755
    Epoch 33/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2442 - acc: 0.9184 - val_loss: 0.4035 - val_acc: 0.8811
    Epoch 34/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2289 - acc: 0.9221 - val_loss: 0.4326 - val_acc: 0.8842
    Epoch 35/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.2328 - acc: 0.9218 - val_loss: 0.4368 - val_acc: 0.8801
    Epoch 36/100
    71280/71280 [==============================] - 3s 43us/step - loss: 0.2347 - acc: 0.9210 - val_loss: 0.4281 - val_acc: 0.8843
    Epoch 37/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2189 - acc: 0.9271 - val_loss: 0.4299 - val_acc: 0.8830
    Epoch 38/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.2271 - acc: 0.9259 - val_loss: 0.3998 - val_acc: 0.8904
    Epoch 39/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.2152 - acc: 0.9291 - val_loss: 0.4568 - val_acc: 0.8785
    Epoch 40/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2140 - acc: 0.9276 - val_loss: 0.4555 - val_acc: 0.8821
    Epoch 41/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1944 - acc: 0.9332 - val_loss: 0.4417 - val_acc: 0.8872
    Epoch 42/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.2044 - acc: 0.9323 - val_loss: 0.4370 - val_acc: 0.8891
    Epoch 43/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.2145 - acc: 0.9304 - val_loss: 0.4178 - val_acc: 0.8909
    Epoch 44/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1925 - acc: 0.9355 - val_loss: 0.4370 - val_acc: 0.8797
    Epoch 45/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1883 - acc: 0.9364 - val_loss: 0.4027 - val_acc: 0.8968
    Epoch 46/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1990 - acc: 0.9339 - val_loss: 0.4204 - val_acc: 0.8923
    Epoch 47/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1874 - acc: 0.9372 - val_loss: 0.3962 - val_acc: 0.8967
    Epoch 48/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1701 - acc: 0.9429 - val_loss: 0.4179 - val_acc: 0.8918
    Epoch 49/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1902 - acc: 0.9370 - val_loss: 0.4489 - val_acc: 0.8846
    Epoch 50/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1912 - acc: 0.9377 - val_loss: 0.3934 - val_acc: 0.8942
    Epoch 51/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1692 - acc: 0.9437 - val_loss: 0.4259 - val_acc: 0.9005
    Epoch 52/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1856 - acc: 0.9394 - val_loss: 0.4306 - val_acc: 0.8953
    Epoch 53/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1734 - acc: 0.9427 - val_loss: 0.3890 - val_acc: 0.9016
    Epoch 54/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1687 - acc: 0.9452 - val_loss: 0.4315 - val_acc: 0.8917
    Epoch 55/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1721 - acc: 0.9438 - val_loss: 0.4303 - val_acc: 0.8946
    Epoch 56/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1591 - acc: 0.9473 - val_loss: 0.4014 - val_acc: 0.8979
    Epoch 57/100
    71280/71280 [==============================] - 3s 49us/step - loss: 0.1678 - acc: 0.9444 - val_loss: 0.4112 - val_acc: 0.9071
    Epoch 58/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1583 - acc: 0.9471 - val_loss: 0.4048 - val_acc: 0.9059
    Epoch 59/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1658 - acc: 0.9453 - val_loss: 0.4401 - val_acc: 0.8957
    Epoch 60/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1694 - acc: 0.9451 - val_loss: 0.4273 - val_acc: 0.9004
    Epoch 61/100
    71280/71280 [==============================] - 3s 43us/step - loss: 0.1518 - acc: 0.9515 - val_loss: 0.5272 - val_acc: 0.8955
    Epoch 62/100
    71280/71280 [==============================] - 3s 43us/step - loss: 0.1743 - acc: 0.9459 - val_loss: 0.4268 - val_acc: 0.8953
    Epoch 63/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1506 - acc: 0.9510 - val_loss: 0.4235 - val_acc: 0.9037
    Epoch 64/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1543 - acc: 0.9494 - val_loss: 0.4272 - val_acc: 0.8956
    Epoch 65/100
    71280/71280 [==============================] - 3s 43us/step - loss: 0.1633 - acc: 0.9517 - val_loss: 0.4178 - val_acc: 0.9029
    Epoch 66/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1521 - acc: 0.9517 - val_loss: 0.4492 - val_acc: 0.9001
    Epoch 67/100
    71280/71280 [==============================] - 4s 53us/step - loss: 0.1493 - acc: 0.9498 - val_loss: 0.4227 - val_acc: 0.9032
    Epoch 68/100
    71280/71280 [==============================] - 3s 49us/step - loss: 0.1425 - acc: 0.9541 - val_loss: 0.4303 - val_acc: 0.8975
    Epoch 69/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1524 - acc: 0.9509 - val_loss: 0.4685 - val_acc: 0.8980
    Epoch 70/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1387 - acc: 0.9549 - val_loss: 0.4268 - val_acc: 0.9083
    Epoch 71/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1377 - acc: 0.9556 - val_loss: 0.4293 - val_acc: 0.8979
    Epoch 72/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1421 - acc: 0.9534 - val_loss: 0.3929 - val_acc: 0.9130
    Epoch 73/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1455 - acc: 0.9543 - val_loss: 0.4694 - val_acc: 0.8985
    Epoch 74/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.1406 - acc: 0.9553 - val_loss: 0.4161 - val_acc: 0.9082
    Epoch 75/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1288 - acc: 0.9587 - val_loss: 0.4779 - val_acc: 0.9044
    Epoch 76/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1405 - acc: 0.9550 - val_loss: 0.4432 - val_acc: 0.9039
    Epoch 77/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1311 - acc: 0.9579 - val_loss: 0.4409 - val_acc: 0.9056
    Epoch 78/100
    71280/71280 [==============================] - 3s 44us/step - loss: 0.1466 - acc: 0.9537 - val_loss: 0.4401 - val_acc: 0.9054
    Epoch 79/100
    71280/71280 [==============================] - 3s 45us/step - loss: 0.1363 - acc: 0.9577 - val_loss: 0.4209 - val_acc: 0.9067
    Epoch 80/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.1351 - acc: 0.9581 - val_loss: 0.4255 - val_acc: 0.9076
    Epoch 81/100
    71280/71280 [==============================] - 3s 48us/step - loss: 0.1320 - acc: 0.9562 - val_loss: 0.5103 - val_acc: 0.9038
    Epoch 82/100
    71280/71280 [==============================] - 4s 55us/step - loss: 0.1280 - acc: 0.9582 - val_loss: 0.4348 - val_acc: 0.9067
    Epoch 83/100
    71280/71280 [==============================] - 4s 50us/step - loss: 0.1254 - acc: 0.9606 - val_loss: 0.4754 - val_acc: 0.9019
    Epoch 84/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.1355 - acc: 0.9564 - val_loss: 0.4558 - val_acc: 0.9092
    Epoch 85/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.1341 - acc: 0.9590 - val_loss: 0.4821 - val_acc: 0.9054
    Epoch 86/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1213 - acc: 0.9610 - val_loss: 0.4395 - val_acc: 0.9071
    Epoch 87/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1315 - acc: 0.9587 - val_loss: 0.4280 - val_acc: 0.9114
    Epoch 88/100
    71280/71280 [==============================] - 4s 49us/step - loss: 0.1277 - acc: 0.9599 - val_loss: 0.4843 - val_acc: 0.9047
    Epoch 89/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1365 - acc: 0.9593 - val_loss: 0.4558 - val_acc: 0.9112
    Epoch 90/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1304 - acc: 0.9600 - val_loss: 0.4450 - val_acc: 0.9126
    Epoch 91/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1214 - acc: 0.9611 - val_loss: 0.4627 - val_acc: 0.9114
    Epoch 92/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1247 - acc: 0.9603 - val_loss: 0.4463 - val_acc: 0.9095
    Epoch 93/100
    71280/71280 [==============================] - 3s 46us/step - loss: 0.1147 - acc: 0.9634 - val_loss: 0.5109 - val_acc: 0.8990
    Epoch 94/100
    71280/71280 [==============================] - 3s 48us/step - loss: 0.1274 - acc: 0.9603 - val_loss: 0.4682 - val_acc: 0.9093
    Epoch 95/100
    71280/71280 [==============================] - 4s 49us/step - loss: 0.1192 - acc: 0.9621 - val_loss: 0.4941 - val_acc: 0.9030
    Epoch 96/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1135 - acc: 0.9637 - val_loss: 0.4256 - val_acc: 0.9153
    Epoch 97/100
    71280/71280 [==============================] - 3s 49us/step - loss: 0.1286 - acc: 0.9606 - val_loss: 0.4581 - val_acc: 0.9097
    Epoch 98/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1065 - acc: 0.9657 - val_loss: 0.4248 - val_acc: 0.9141
    Epoch 99/100
    71280/71280 [==============================] - 4s 49us/step - loss: 0.1159 - acc: 0.9633 - val_loss: 0.4860 - val_acc: 0.9018
    Epoch 100/100
    71280/71280 [==============================] - 3s 47us/step - loss: 0.1159 - acc: 0.9620 - val_loss: 0.4744 - val_acc: 0.9117
    CPU times: user 12min 34s, sys: 3min 34s, total: 16min 8s
    Wall time: 5min 47s



```python
# Model Evaluation

pred =  model.predict_classes(X_test)
scores = model.evaluate(X_test, Y_test, verbose=0)


print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.9192424242424242

    confusion matrix:
    [[1828    0    0    0    0    0    0    0    0    0   11]
     [   0 1855    0    0    0    0    0    0    0    0    0]
     [   0    0 1782    0    0    0    0    0    0    5    0]
     [   0    5    1 1753   12    6   10    4    2   17    4]
     [   6    9    3    0 1761   11    2    2    0   34   13]
     [   1    4    3    0    5 1746    4    0    0   19    0]
     [   3    5    0    4   22    3 1703    4    0   34   11]
     [   3    0    0    0    0    0    0 1776   10    0    5]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  24   72   33   69  167   86  103   35    6  818  315]
     [   9   21   10   27   47   21   25    5    0  227 1400]]

                  precision    recall  f1-score   support

               0       0.98      0.99      0.98      1839
               1       0.94      1.00      0.97      1855
               2       0.97      1.00      0.98      1787
               3       0.95      0.97      0.96      1814
               4       0.87      0.96      0.91      1841
               5       0.93      0.98      0.96      1782
               6       0.92      0.95      0.94      1789
               7       0.97      0.99      0.98      1794
               8       0.99      1.00      0.99      1779
               9       0.71      0.47      0.57      1728
              10       0.80      0.78      0.79      1792

       micro avg       0.92      0.92      0.92     19800
       macro avg       0.91      0.92      0.91     19800
    weighted avg       0.91      0.92      0.91     19800



### Recurrent Neural Network


```python
%%time

## Building the Model

model = Sequential()
model.add(LSTM(72, input_shape=(1,X_train.shape[1])))
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names),activation="softmax"))

model.compile(optimizer='Adam',loss="categorical_crossentropy",metrics=["accuracy"])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    lstm_1 (LSTM)                (None, 72)                25344     
    _________________________________________________________________
    dense_5 (Dense)              (None, 128)               9344      
    _________________________________________________________________
    dense_6 (Dense)              (None, 256)               33024     
    _________________________________________________________________
    dense_7 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dense_8 (Dense)              (None, 11)                2827      
    =================================================================
    Total params: 136,331
    Trainable params: 136,331
    Non-trainable params: 0
    _________________________________________________________________
    CPU times: user 711 ms, sys: 186 ms, total: 897 ms
    Wall time: 974 ms



```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train.reshape(X_train.shape[0],1,X_train.shape[1]), Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```

    Train on 71280 samples, validate on 7920 samples
    Epoch 1/100
    71280/71280 [==============================] - 6s 91us/step - loss: 2.0987 - acc: 0.2536 - val_loss: 1.8142 - val_acc: 0.3612
    Epoch 2/100
    71280/71280 [==============================] - 5s 66us/step - loss: 1.6156 - acc: 0.4325 - val_loss: 1.4634 - val_acc: 0.4847
    Epoch 3/100
    71280/71280 [==============================] - 5s 67us/step - loss: 1.3200 - acc: 0.5348 - val_loss: 1.2401 - val_acc: 0.5562
    Epoch 4/100
    71280/71280 [==============================] - 5s 71us/step - loss: 1.1216 - acc: 0.6058 - val_loss: 1.0993 - val_acc: 0.6107
    Epoch 5/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.9730 - acc: 0.6582 - val_loss: 0.9734 - val_acc: 0.6667
    Epoch 6/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.8646 - acc: 0.6969 - val_loss: 0.8268 - val_acc: 0.7138
    Epoch 7/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.7760 - acc: 0.7279 - val_loss: 0.7839 - val_acc: 0.7229
    Epoch 8/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.7041 - acc: 0.7528 - val_loss: 0.7529 - val_acc: 0.7362
    Epoch 9/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.6435 - acc: 0.7731 - val_loss: 0.7001 - val_acc: 0.7530
    Epoch 10/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.5901 - acc: 0.7915 - val_loss: 0.6367 - val_acc: 0.7855
    Epoch 11/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.5502 - acc: 0.8050 - val_loss: 0.6513 - val_acc: 0.7766
    Epoch 12/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.5157 - acc: 0.8175 - val_loss: 0.5592 - val_acc: 0.8092
    Epoch 13/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.4812 - acc: 0.8286 - val_loss: 0.5615 - val_acc: 0.8124
    Epoch 14/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.4534 - acc: 0.8380 - val_loss: 0.5111 - val_acc: 0.8188
    Epoch 15/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.4375 - acc: 0.8448 - val_loss: 0.5360 - val_acc: 0.8172
    Epoch 16/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.4090 - acc: 0.8534 - val_loss: 0.4891 - val_acc: 0.8279
    Epoch 17/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.3937 - acc: 0.8592 - val_loss: 0.4829 - val_acc: 0.8332
    Epoch 18/100
    71280/71280 [==============================] - 5s 63us/step - loss: 0.3707 - acc: 0.8682 - val_loss: 0.4695 - val_acc: 0.8354
    Epoch 19/100
    71280/71280 [==============================] - 5s 64us/step - loss: 0.3605 - acc: 0.8715 - val_loss: 0.4916 - val_acc: 0.8328
    Epoch 20/100
    71280/71280 [==============================] - 5s 65us/step - loss: 0.3447 - acc: 0.8756 - val_loss: 0.4873 - val_acc: 0.8419
    Epoch 21/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.3355 - acc: 0.8807 - val_loss: 0.4383 - val_acc: 0.8521
    Epoch 22/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.3211 - acc: 0.8857 - val_loss: 0.4676 - val_acc: 0.8509
    Epoch 23/100
    71280/71280 [==============================] - 5s 66us/step - loss: 0.3064 - acc: 0.8916 - val_loss: 0.4304 - val_acc: 0.8566
    Epoch 24/100
    71280/71280 [==============================] - 5s 66us/step - loss: 0.3055 - acc: 0.8917 - val_loss: 0.4374 - val_acc: 0.8620
    Epoch 25/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.2844 - acc: 0.8997 - val_loss: 0.4332 - val_acc: 0.8580
    Epoch 26/100
    71280/71280 [==============================] - 5s 65us/step - loss: 0.2823 - acc: 0.8997 - val_loss: 0.4313 - val_acc: 0.8701
    Epoch 27/100
    71280/71280 [==============================] - 5s 65us/step - loss: 0.2680 - acc: 0.9049 - val_loss: 0.4415 - val_acc: 0.8611
    Epoch 28/100
    71280/71280 [==============================] - 5s 66us/step - loss: 0.2642 - acc: 0.9081 - val_loss: 0.4207 - val_acc: 0.8703
    Epoch 29/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.2615 - acc: 0.9081 - val_loss: 0.4297 - val_acc: 0.8699
    Epoch 30/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.2500 - acc: 0.9113 - val_loss: 0.4429 - val_acc: 0.8674
    Epoch 31/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.2487 - acc: 0.9132 - val_loss: 0.4290 - val_acc: 0.8694
    Epoch 32/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.2358 - acc: 0.9179 - val_loss: 0.4143 - val_acc: 0.8766
    Epoch 33/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.2358 - acc: 0.9182 - val_loss: 0.4237 - val_acc: 0.8737
    Epoch 34/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.2215 - acc: 0.9228 - val_loss: 0.4077 - val_acc: 0.8799
    Epoch 35/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.2225 - acc: 0.9215 - val_loss: 0.4284 - val_acc: 0.8794
    Epoch 36/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.2143 - acc: 0.9254 - val_loss: 0.4141 - val_acc: 0.8857
    Epoch 37/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.2107 - acc: 0.9273 - val_loss: 0.4073 - val_acc: 0.8774
    Epoch 38/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.2087 - acc: 0.9287 - val_loss: 0.4102 - val_acc: 0.8871
    Epoch 39/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.2092 - acc: 0.9278 - val_loss: 0.4037 - val_acc: 0.8894
    Epoch 40/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.1898 - acc: 0.9338 - val_loss: 0.3749 - val_acc: 0.8923
    Epoch 41/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1946 - acc: 0.9338 - val_loss: 0.3968 - val_acc: 0.8893
    Epoch 42/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1918 - acc: 0.9346 - val_loss: 0.4203 - val_acc: 0.8806
    Epoch 43/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.1857 - acc: 0.9368 - val_loss: 0.4150 - val_acc: 0.8948
    Epoch 44/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.1850 - acc: 0.9377 - val_loss: 0.4254 - val_acc: 0.8923
    Epoch 45/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.1767 - acc: 0.9397 - val_loss: 0.4046 - val_acc: 0.8894
    Epoch 46/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.1772 - acc: 0.9402 - val_loss: 0.4501 - val_acc: 0.8869
    Epoch 47/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1737 - acc: 0.9410 - val_loss: 0.3961 - val_acc: 0.8961
    Epoch 48/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.1703 - acc: 0.9416 - val_loss: 0.3965 - val_acc: 0.8977
    Epoch 49/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.1648 - acc: 0.9444 - val_loss: 0.4515 - val_acc: 0.8926
    Epoch 50/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.1641 - acc: 0.9442 - val_loss: 0.4647 - val_acc: 0.8822
    Epoch 51/100
    71280/71280 [==============================] - 5s 77us/step - loss: 0.1596 - acc: 0.9464 - val_loss: 0.3898 - val_acc: 0.9034
    Epoch 52/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.1578 - acc: 0.9468 - val_loss: 0.3766 - val_acc: 0.9042
    Epoch 53/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.1554 - acc: 0.9488 - val_loss: 0.3569 - val_acc: 0.9064
    Epoch 54/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1519 - acc: 0.9491 - val_loss: 0.4028 - val_acc: 0.9042
    Epoch 55/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.1610 - acc: 0.9466 - val_loss: 0.4355 - val_acc: 0.8990
    Epoch 56/100
    71280/71280 [==============================] - 8s 116us/step - loss: 0.1413 - acc: 0.9533 - val_loss: 0.3882 - val_acc: 0.9081
    Epoch 57/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.1587 - acc: 0.9470 - val_loss: 0.4094 - val_acc: 0.8992
    Epoch 58/100
    71280/71280 [==============================] - 6s 90us/step - loss: 0.1371 - acc: 0.9535 - val_loss: 0.4118 - val_acc: 0.9043
    Epoch 59/100
    71280/71280 [==============================] - 6s 87us/step - loss: 0.1459 - acc: 0.9517 - val_loss: 0.3995 - val_acc: 0.9073
    Epoch 60/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.1432 - acc: 0.9527 - val_loss: 0.4684 - val_acc: 0.8952
    Epoch 61/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.1468 - acc: 0.9525 - val_loss: 0.5385 - val_acc: 0.8845
    Epoch 62/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.1405 - acc: 0.9538 - val_loss: 0.4258 - val_acc: 0.9062
    Epoch 63/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.1360 - acc: 0.9552 - val_loss: 0.4119 - val_acc: 0.9104
    Epoch 64/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.1392 - acc: 0.9538 - val_loss: 0.4203 - val_acc: 0.8996
    Epoch 65/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.1269 - acc: 0.9583 - val_loss: 0.4344 - val_acc: 0.8999
    Epoch 66/100
    71280/71280 [==============================] - 6s 79us/step - loss: 0.1432 - acc: 0.9527 - val_loss: 0.4423 - val_acc: 0.9047
    Epoch 67/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.1267 - acc: 0.9584 - val_loss: 0.4183 - val_acc: 0.9009
    Epoch 68/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.1315 - acc: 0.9571 - val_loss: 0.4322 - val_acc: 0.9104
    Epoch 69/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1283 - acc: 0.9587 - val_loss: 0.4564 - val_acc: 0.9034
    Epoch 70/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.1234 - acc: 0.9595 - val_loss: 0.4482 - val_acc: 0.9086
    Epoch 71/100
    71280/71280 [==============================] - 7s 95us/step - loss: 0.1294 - acc: 0.9582 - val_loss: 0.5110 - val_acc: 0.8938
    Epoch 72/100
    71280/71280 [==============================] - 6s 78us/step - loss: 0.1261 - acc: 0.9592 - val_loss: 0.4292 - val_acc: 0.9134
    Epoch 73/100
    71280/71280 [==============================] - 6s 80us/step - loss: 0.1336 - acc: 0.9580 - val_loss: 0.4564 - val_acc: 0.9083
    Epoch 74/100
    71280/71280 [==============================] - 6s 91us/step - loss: 0.1122 - acc: 0.9636 - val_loss: 0.4387 - val_acc: 0.9071
    Epoch 75/100
    71280/71280 [==============================] - 7s 92us/step - loss: 0.1256 - acc: 0.9598 - val_loss: 0.4286 - val_acc: 0.9141
    Epoch 76/100
    71280/71280 [==============================] - 6s 87us/step - loss: 0.1225 - acc: 0.9607 - val_loss: 0.4731 - val_acc: 0.9016
    Epoch 77/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1199 - acc: 0.9607 - val_loss: 0.4587 - val_acc: 0.9109
    Epoch 78/100
    71280/71280 [==============================] - 6s 83us/step - loss: 0.1158 - acc: 0.9626 - val_loss: 0.4996 - val_acc: 0.9014
    Epoch 79/100
    71280/71280 [==============================] - 6s 90us/step - loss: 0.1172 - acc: 0.9617 - val_loss: 0.4264 - val_acc: 0.9120
    Epoch 80/100
    71280/71280 [==============================] - 7s 92us/step - loss: 0.1220 - acc: 0.9608 - val_loss: 0.4795 - val_acc: 0.9086
    Epoch 81/100
    71280/71280 [==============================] - 6s 82us/step - loss: 0.1095 - acc: 0.9651 - val_loss: 0.4557 - val_acc: 0.9043
    Epoch 82/100
    71280/71280 [==============================] - 6s 88us/step - loss: 0.1270 - acc: 0.9616 - val_loss: 0.4287 - val_acc: 0.9116
    Epoch 83/100
    71280/71280 [==============================] - 5s 75us/step - loss: 0.1124 - acc: 0.9646 - val_loss: 0.5353 - val_acc: 0.9025
    Epoch 84/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.1155 - acc: 0.9632 - val_loss: 0.4450 - val_acc: 0.9128
    Epoch 85/100
    71280/71280 [==============================] - 5s 74us/step - loss: 0.1114 - acc: 0.9643 - val_loss: 0.4235 - val_acc: 0.9165
    Epoch 86/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1152 - acc: 0.9639 - val_loss: 0.4616 - val_acc: 0.9064
    Epoch 87/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.0996 - acc: 0.9678 - val_loss: 0.4211 - val_acc: 0.9152
    Epoch 88/100
    71280/71280 [==============================] - 5s 73us/step - loss: 0.1228 - acc: 0.9615 - val_loss: 0.5183 - val_acc: 0.9029
    Epoch 89/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.1107 - acc: 0.9654 - val_loss: 0.4237 - val_acc: 0.9203
    Epoch 90/100
    71280/71280 [==============================] - 5s 71us/step - loss: 0.1096 - acc: 0.9669 - val_loss: 0.5312 - val_acc: 0.9020
    Epoch 91/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.1129 - acc: 0.9638 - val_loss: 0.5141 - val_acc: 0.9096
    Epoch 92/100
    71280/71280 [==============================] - 5s 72us/step - loss: 0.1153 - acc: 0.9634 - val_loss: 0.4797 - val_acc: 0.9083
    Epoch 93/100
    71280/71280 [==============================] - 5s 70us/step - loss: 0.0995 - acc: 0.9679 - val_loss: 0.4497 - val_acc: 0.9134
    Epoch 94/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.1155 - acc: 0.9640 - val_loss: 0.4388 - val_acc: 0.9186
    Epoch 95/100
    71280/71280 [==============================] - 5s 76us/step - loss: 0.0931 - acc: 0.9716 - val_loss: 0.5058 - val_acc: 0.9006
    Epoch 96/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.1109 - acc: 0.9639 - val_loss: 0.4572 - val_acc: 0.9182
    Epoch 97/100
    71280/71280 [==============================] - 5s 67us/step - loss: 0.0980 - acc: 0.9690 - val_loss: 0.4590 - val_acc: 0.9164
    Epoch 98/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.1109 - acc: 0.9652 - val_loss: 0.4644 - val_acc: 0.9154
    Epoch 99/100
    71280/71280 [==============================] - 5s 68us/step - loss: 0.0990 - acc: 0.9693 - val_loss: 0.4864 - val_acc: 0.9155
    Epoch 100/100
    71280/71280 [==============================] - 5s 69us/step - loss: 0.1147 - acc: 0.9644 - val_loss: 0.4812 - val_acc: 0.9138
    CPU times: user 23min 17s, sys: 6min 30s, total: 29min 48s
    Wall time: 8min 47s



```python
# Model Evaluation

pred =  model.predict_classes(X_test.reshape(X_test.shape[0],1,X_test.shape[1]))
scores = model.evaluate(X_test.reshape(X_test.shape[0],1,X_test.shape[1]), Y_test, verbose=0)

print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.9169191919432746

    confusion matrix:
    [[1823    0    0    0   16    0    0    0    0    0    0]
     [   0 1837    0    0   11    0    4    0    0    0    3]
     [   0    0 1775    0    5    0    0    0    0    7    0]
     [   0    7    1 1760   15    0   14    1    0   13    3]
     [   3    4    3    3 1775    5    2    3    0   21   22]
     [   0   11    3    2    9 1711    0   11    0   26    9]
     [   4    2    3    8   29    7 1666    2    0   48   20]
     [   0    0    0    0   15    0    0 1768    0   11    0]
     [   0    0    0    0    0    0    0    0 1779    0    0]
     [  20   68   23   64  211   52   75   35    5  815  360]
     [   7   20    3   20   76   21   31   14    0  154 1446]]

                  precision    recall  f1-score   support

               0       0.98      0.99      0.99      1839
               1       0.94      0.99      0.97      1855
               2       0.98      0.99      0.99      1787
               3       0.95      0.97      0.96      1814
               4       0.82      0.96      0.89      1841
               5       0.95      0.96      0.96      1782
               6       0.93      0.93      0.93      1789
               7       0.96      0.99      0.97      1794
               8       1.00      1.00      1.00      1779
               9       0.74      0.47      0.58      1728
              10       0.78      0.81      0.79      1792

       micro avg       0.92      0.92      0.92     19800
       macro avg       0.91      0.91      0.91     19800
    weighted avg       0.91      0.92      0.91     19800



### Convolutional Neural Network with Recurrent Neural Networks


```python
%%time

## Reshaping Data

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)

## Building the Model

model = Sequential()
model.add(Conv1D(32, (3), input_shape=(X_train.shape[1],1), activation='relu'))
model.add(MaxPool1D(pool_size=2))
model.add(LSTM(100))
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(len(country_names), activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv1d_2 (Conv1D)            (None, 13, 32)            128       
    _________________________________________________________________
    max_pooling1d_2 (MaxPooling1 (None, 6, 32)             0         
    _________________________________________________________________
    lstm_2 (LSTM)                (None, 100)               53200     
    _________________________________________________________________
    dense_9 (Dense)              (None, 128)               12928     
    _________________________________________________________________
    dense_10 (Dense)             (None, 256)               33024     
    _________________________________________________________________
    dense_11 (Dense)             (None, 256)               65792     
    _________________________________________________________________
    dense_12 (Dense)             (None, 11)                2827      
    =================================================================
    Total params: 167,899
    Trainable params: 167,899
    Non-trainable params: 0
    _________________________________________________________________
    CPU times: user 611 ms, sys: 109 ms, total: 720 ms
    Wall time: 371 ms



```python
%%time

## Train and Fit Model

batch_size = 64
epochs = 100
model.fit(X_train, Y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_split=0.1)
```

    Train on 71280 samples, validate on 7920 samples
    Epoch 1/100
    71280/71280 [==============================] - 11s 161us/step - loss: 2.3516 - acc: 0.1320 - val_loss: 2.2279 - val_acc: 0.2040
    Epoch 2/100
    71280/71280 [==============================] - 11s 153us/step - loss: 1.9989 - acc: 0.2925 - val_loss: 1.7166 - val_acc: 0.3890
    Epoch 3/100
    71280/71280 [==============================] - 12s 166us/step - loss: 1.5059 - acc: 0.4707 - val_loss: 1.3021 - val_acc: 0.5482
    Epoch 4/100
    71280/71280 [==============================] - 18s 252us/step - loss: 1.1619 - acc: 0.5932 - val_loss: 1.0644 - val_acc: 0.6266
    Epoch 5/100
    71280/71280 [==============================] - 13s 183us/step - loss: 0.9428 - acc: 0.6692 - val_loss: 0.9084 - val_acc: 0.6792
    Epoch 6/100
    71280/71280 [==============================] - 13s 188us/step - loss: 0.7960 - acc: 0.7201 - val_loss: 0.7889 - val_acc: 0.7174
    Epoch 7/100
    71280/71280 [==============================] - 13s 185us/step - loss: 0.6858 - acc: 0.7591 - val_loss: 0.6951 - val_acc: 0.7561
    Epoch 8/100
    71280/71280 [==============================] - 12s 165us/step - loss: 0.6100 - acc: 0.7843 - val_loss: 0.6373 - val_acc: 0.7737
    Epoch 9/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.5430 - acc: 0.8085 - val_loss: 0.5985 - val_acc: 0.7884
    Epoch 10/100
    71280/71280 [==============================] - 11s 150us/step - loss: 0.4965 - acc: 0.8235 - val_loss: 0.5757 - val_acc: 0.7947
    Epoch 11/100
    71280/71280 [==============================] - 11s 147us/step - loss: 0.4572 - acc: 0.8376 - val_loss: 0.5418 - val_acc: 0.8115
    Epoch 12/100
    71280/71280 [==============================] - 11s 149us/step - loss: 0.4240 - acc: 0.8488 - val_loss: 0.4881 - val_acc: 0.8303
    Epoch 13/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.3989 - acc: 0.8574 - val_loss: 0.5190 - val_acc: 0.8251
    Epoch 14/100
    71280/71280 [==============================] - 10s 145us/step - loss: 0.3829 - acc: 0.8644 - val_loss: 0.4879 - val_acc: 0.8323
    Epoch 15/100
    71280/71280 [==============================] - 10s 145us/step - loss: 0.3518 - acc: 0.8751 - val_loss: 0.4681 - val_acc: 0.8389
    Epoch 16/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.3405 - acc: 0.8779 - val_loss: 0.4752 - val_acc: 0.8356
    Epoch 17/100
    71280/71280 [==============================] - 11s 152us/step - loss: 0.3240 - acc: 0.8844 - val_loss: 0.4634 - val_acc: 0.8436
    Epoch 18/100
    71280/71280 [==============================] - 13s 181us/step - loss: 0.3148 - acc: 0.8876 - val_loss: 0.4445 - val_acc: 0.8451
    Epoch 19/100
    71280/71280 [==============================] - 11s 153us/step - loss: 0.3031 - acc: 0.8934 - val_loss: 0.4407 - val_acc: 0.8532
    Epoch 20/100
    71280/71280 [==============================] - 13s 180us/step - loss: 0.2816 - acc: 0.9009 - val_loss: 0.4219 - val_acc: 0.8631
    Epoch 21/100
    71280/71280 [==============================] - 12s 166us/step - loss: 0.2777 - acc: 0.9005 - val_loss: 0.4446 - val_acc: 0.8604
    Epoch 22/100
    71280/71280 [==============================] - 13s 179us/step - loss: 0.2703 - acc: 0.9054 - val_loss: 0.4289 - val_acc: 0.8649
    Epoch 23/100
    71280/71280 [==============================] - 13s 184us/step - loss: 0.2625 - acc: 0.9085 - val_loss: 0.4265 - val_acc: 0.8576
    Epoch 24/100
    71280/71280 [==============================] - 13s 182us/step - loss: 0.2591 - acc: 0.9102 - val_loss: 0.4120 - val_acc: 0.8678
    Epoch 25/100
    71280/71280 [==============================] - 12s 173us/step - loss: 0.2389 - acc: 0.9166 - val_loss: 0.4270 - val_acc: 0.8735
    Epoch 26/100
    71280/71280 [==============================] - 14s 191us/step - loss: 0.2436 - acc: 0.9154 - val_loss: 0.4125 - val_acc: 0.8699
    Epoch 27/100
    71280/71280 [==============================] - 13s 176us/step - loss: 0.2333 - acc: 0.9187 - val_loss: 0.4120 - val_acc: 0.8755
    Epoch 28/100
    71280/71280 [==============================] - 12s 169us/step - loss: 0.2178 - acc: 0.9244 - val_loss: 0.4418 - val_acc: 0.8707
    Epoch 29/100
    71280/71280 [==============================] - 12s 168us/step - loss: 0.2209 - acc: 0.9229 - val_loss: 0.4744 - val_acc: 0.8615
    Epoch 30/100
    71280/71280 [==============================] - 13s 186us/step - loss: 0.2133 - acc: 0.9257 - val_loss: 0.3794 - val_acc: 0.8878
    Epoch 31/100
    71280/71280 [==============================] - 13s 177us/step - loss: 0.2014 - acc: 0.9309 - val_loss: 0.4332 - val_acc: 0.8785
    Epoch 32/100
    71280/71280 [==============================] - 13s 188us/step - loss: 0.2041 - acc: 0.9297 - val_loss: 0.3846 - val_acc: 0.8927
    Epoch 33/100
    71280/71280 [==============================] - 13s 177us/step - loss: 0.1947 - acc: 0.9326 - val_loss: 0.4312 - val_acc: 0.8798
    Epoch 34/100
    71280/71280 [==============================] - 12s 172us/step - loss: 0.1926 - acc: 0.9337 - val_loss: 0.3980 - val_acc: 0.8926
    Epoch 35/100
    71280/71280 [==============================] - 12s 168us/step - loss: 0.1898 - acc: 0.9366 - val_loss: 0.4172 - val_acc: 0.8851
    Epoch 36/100
    71280/71280 [==============================] - 13s 184us/step - loss: 0.1871 - acc: 0.9360 - val_loss: 0.4500 - val_acc: 0.8782
    Epoch 37/100
    71280/71280 [==============================] - 16s 226us/step - loss: 0.1881 - acc: 0.9357 - val_loss: 0.3762 - val_acc: 0.9011
    Epoch 38/100
    71280/71280 [==============================] - 13s 176us/step - loss: 0.1748 - acc: 0.9405 - val_loss: 0.4058 - val_acc: 0.8912
    Epoch 39/100
    71280/71280 [==============================] - 14s 191us/step - loss: 0.1796 - acc: 0.9395 - val_loss: 0.3765 - val_acc: 0.8953
    Epoch 40/100
    71280/71280 [==============================] - 14s 192us/step - loss: 0.1717 - acc: 0.9422 - val_loss: 0.4208 - val_acc: 0.8915
    Epoch 41/100
    71280/71280 [==============================] - 13s 184us/step - loss: 0.1767 - acc: 0.9402 - val_loss: 0.4182 - val_acc: 0.8927
    Epoch 42/100
    71280/71280 [==============================] - 13s 180us/step - loss: 0.1627 - acc: 0.9446 - val_loss: 0.4011 - val_acc: 0.9008
    Epoch 43/100
    71280/71280 [==============================] - 14s 195us/step - loss: 0.1643 - acc: 0.9449 - val_loss: 0.4369 - val_acc: 0.8922
    Epoch 44/100
    71280/71280 [==============================] - 15s 206us/step - loss: 0.1630 - acc: 0.9442 - val_loss: 0.4148 - val_acc: 0.8958
    Epoch 45/100
    71280/71280 [==============================] - 12s 169us/step - loss: 0.1614 - acc: 0.9464 - val_loss: 0.4111 - val_acc: 0.8904
    Epoch 46/100
    71280/71280 [==============================] - 12s 170us/step - loss: 0.1541 - acc: 0.9486 - val_loss: 0.4053 - val_acc: 0.8990
    Epoch 47/100
    71280/71280 [==============================] - 13s 189us/step - loss: 0.1717 - acc: 0.9439 - val_loss: 0.4433 - val_acc: 0.8987
    Epoch 48/100
    71280/71280 [==============================] - 14s 194us/step - loss: 0.1499 - acc: 0.9491 - val_loss: 0.4311 - val_acc: 0.8956
    Epoch 49/100
    71280/71280 [==============================] - 12s 175us/step - loss: 0.1487 - acc: 0.9499 - val_loss: 0.4777 - val_acc: 0.8926
    Epoch 50/100
    71280/71280 [==============================] - 13s 184us/step - loss: 0.1501 - acc: 0.9496 - val_loss: 0.4501 - val_acc: 0.8917
    Epoch 51/100
    71280/71280 [==============================] - 13s 180us/step - loss: 0.1404 - acc: 0.9532 - val_loss: 0.4510 - val_acc: 0.8933
    Epoch 52/100
    71280/71280 [==============================] - 13s 182us/step - loss: 0.1529 - acc: 0.9489 - val_loss: 0.4320 - val_acc: 0.8946
    Epoch 53/100
    71280/71280 [==============================] - 11s 158us/step - loss: 0.1401 - acc: 0.9530 - val_loss: 0.3925 - val_acc: 0.9052
    Epoch 54/100
    71280/71280 [==============================] - 11s 155us/step - loss: 0.1438 - acc: 0.9513 - val_loss: 0.4790 - val_acc: 0.8899
    Epoch 55/100
    71280/71280 [==============================] - 11s 153us/step - loss: 0.1412 - acc: 0.9537 - val_loss: 0.4492 - val_acc: 0.8995
    Epoch 56/100
    71280/71280 [==============================] - 11s 161us/step - loss: 0.1385 - acc: 0.9531 - val_loss: 0.4436 - val_acc: 0.8982
    Epoch 57/100
    71280/71280 [==============================] - 11s 155us/step - loss: 0.1489 - acc: 0.9505 - val_loss: 0.4400 - val_acc: 0.8965
    Epoch 58/100
    71280/71280 [==============================] - 13s 181us/step - loss: 0.1236 - acc: 0.9578 - val_loss: 0.4142 - val_acc: 0.9047
    Epoch 59/100
    71280/71280 [==============================] - 11s 153us/step - loss: 0.1416 - acc: 0.9532 - val_loss: 0.4465 - val_acc: 0.9032
    Epoch 60/100
    71280/71280 [==============================] - 12s 167us/step - loss: 0.1313 - acc: 0.9563 - val_loss: 0.4495 - val_acc: 0.8982
    Epoch 61/100
    71280/71280 [==============================] - 12s 163us/step - loss: 0.1356 - acc: 0.9547 - val_loss: 0.4167 - val_acc: 0.9069
    Epoch 62/100
    71280/71280 [==============================] - 11s 153us/step - loss: 0.1234 - acc: 0.9592 - val_loss: 0.4241 - val_acc: 0.9014
    Epoch 63/100
    71280/71280 [==============================] - 12s 164us/step - loss: 0.1337 - acc: 0.9558 - val_loss: 0.4261 - val_acc: 0.9111
    Epoch 64/100
    71280/71280 [==============================] - 12s 164us/step - loss: 0.1242 - acc: 0.9586 - val_loss: 0.4944 - val_acc: 0.8900
    Epoch 65/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1357 - acc: 0.9564 - val_loss: 0.4047 - val_acc: 0.9073
    Epoch 66/100
    71280/71280 [==============================] - 13s 181us/step - loss: 0.1266 - acc: 0.9592 - val_loss: 0.4518 - val_acc: 0.9039
    Epoch 67/100
    71280/71280 [==============================] - 11s 152us/step - loss: 0.1243 - acc: 0.9596 - val_loss: 0.4377 - val_acc: 0.9067
    Epoch 68/100
    71280/71280 [==============================] - 11s 149us/step - loss: 0.1269 - acc: 0.9585 - val_loss: 0.4281 - val_acc: 0.9081
    Epoch 69/100
    71280/71280 [==============================] - 13s 186us/step - loss: 0.1213 - acc: 0.9596 - val_loss: 0.4342 - val_acc: 0.8985
    Epoch 70/100
    71280/71280 [==============================] - 11s 151us/step - loss: 0.1301 - acc: 0.9580 - val_loss: 0.3977 - val_acc: 0.9139
    Epoch 71/100
    71280/71280 [==============================] - 11s 153us/step - loss: 0.1291 - acc: 0.9577 - val_loss: 0.4347 - val_acc: 0.9054
    Epoch 72/100
    71280/71280 [==============================] - 11s 154us/step - loss: 0.1151 - acc: 0.9624 - val_loss: 0.4434 - val_acc: 0.9082
    Epoch 73/100
    71280/71280 [==============================] - 10s 145us/step - loss: 0.1160 - acc: 0.9618 - val_loss: 0.4245 - val_acc: 0.9058
    Epoch 74/100
    71280/71280 [==============================] - 10s 145us/step - loss: 0.1254 - acc: 0.9599 - val_loss: 0.4097 - val_acc: 0.9140
    Epoch 75/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1108 - acc: 0.9637 - val_loss: 0.4252 - val_acc: 0.9110
    Epoch 76/100
    71280/71280 [==============================] - 11s 148us/step - loss: 0.1264 - acc: 0.9594 - val_loss: 0.4236 - val_acc: 0.9058
    Epoch 77/100
    71280/71280 [==============================] - 11s 152us/step - loss: 0.1120 - acc: 0.9630 - val_loss: 0.4573 - val_acc: 0.9074
    Epoch 78/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1147 - acc: 0.9627 - val_loss: 0.4335 - val_acc: 0.9083
    Epoch 79/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.1193 - acc: 0.9609 - val_loss: 0.4842 - val_acc: 0.8990
    Epoch 80/100
    71280/71280 [==============================] - 10s 145us/step - loss: 0.1130 - acc: 0.9625 - val_loss: 0.4446 - val_acc: 0.9141
    Epoch 81/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1096 - acc: 0.9641 - val_loss: 0.4341 - val_acc: 0.9066
    Epoch 82/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1183 - acc: 0.9621 - val_loss: 0.4744 - val_acc: 0.9035
    Epoch 83/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1191 - acc: 0.9619 - val_loss: 0.4517 - val_acc: 0.9133
    Epoch 84/100
    71280/71280 [==============================] - 11s 151us/step - loss: 0.1170 - acc: 0.9631 - val_loss: 0.4431 - val_acc: 0.9101
    Epoch 85/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.1106 - acc: 0.9643 - val_loss: 0.4709 - val_acc: 0.9071
    Epoch 86/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.1147 - acc: 0.9642 - val_loss: 0.4702 - val_acc: 0.9083
    Epoch 87/100
    71280/71280 [==============================] - 11s 148us/step - loss: 0.0987 - acc: 0.9677 - val_loss: 0.4636 - val_acc: 0.9091
    Epoch 88/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1179 - acc: 0.9619 - val_loss: 0.4527 - val_acc: 0.9109
    Epoch 89/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.1203 - acc: 0.9613 - val_loss: 0.4523 - val_acc: 0.9093
    Epoch 90/100
    71280/71280 [==============================] - 10s 145us/step - loss: 0.1017 - acc: 0.9668 - val_loss: 0.4580 - val_acc: 0.9106
    Epoch 91/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.1060 - acc: 0.9660 - val_loss: 0.4244 - val_acc: 0.9133
    Epoch 92/100
    71280/71280 [==============================] - 11s 149us/step - loss: 0.1076 - acc: 0.9659 - val_loss: 0.4464 - val_acc: 0.9120
    Epoch 93/100
    71280/71280 [==============================] - 10s 146us/step - loss: 0.1019 - acc: 0.9677 - val_loss: 0.4746 - val_acc: 0.9057
    Epoch 94/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.1130 - acc: 0.9632 - val_loss: 0.4534 - val_acc: 0.9145
    Epoch 95/100
    71280/71280 [==============================] - 10s 145us/step - loss: 0.1018 - acc: 0.9669 - val_loss: 0.5021 - val_acc: 0.9025
    Epoch 96/100
    71280/71280 [==============================] - 10s 147us/step - loss: 0.1050 - acc: 0.9663 - val_loss: 0.4549 - val_acc: 0.9082
    Epoch 97/100
    71280/71280 [==============================] - 11s 150us/step - loss: 0.1050 - acc: 0.9662 - val_loss: 0.4858 - val_acc: 0.9072
    Epoch 98/100
    71280/71280 [==============================] - 11s 150us/step - loss: 0.1069 - acc: 0.9666 - val_loss: 0.4564 - val_acc: 0.9120
    Epoch 99/100
    71280/71280 [==============================] - 11s 148us/step - loss: 0.0975 - acc: 0.9675 - val_loss: 0.4245 - val_acc: 0.9149
    Epoch 100/100
    71280/71280 [==============================] - 11s 152us/step - loss: 0.1079 - acc: 0.9655 - val_loss: 0.4653 - val_acc: 0.9109
    CPU times: user 57min 10s, sys: 15min 58s, total: 1h 13min 9s
    Wall time: 19min 35s



```python
# Model Evaluation

pred =  model.predict_classes(X_test)
scores = model.evaluate(X_test, Y_test, verbose=0)


print('\nTest accuracy:', scores[1])

print("\nconfusion matrix:\n" + str(confusion_matrix(pd.DataFrame(Y_test).idxmax(1), pred))+'\n')

print(classification_report(pd.DataFrame(Y_test).idxmax(1), pred))
```


    Test accuracy: 0.9187373737614565

    confusion matrix:
    [[1839    0    0    0    0    0    0    0    0    0    0]
     [   0 1845    0    1    2    1    0    0    0    1    5]
     [   0    0 1775    0    5    0    0    0    0    7    0]
     [   0    2    0 1780   11    0    0    0    2   11    8]
     [   5    3    2   11 1731    8    4    0    0   34   43]
     [   0    8    3    1   21 1727    0    0    0   11   11]
     [   1   12    0   15    8    0 1710    0    0   29   14]
     [   0    0    0    0    0    3    0 1791    0    0    0]
     [   0    0    0    0    0    0    0    0 1754   25    0]
     [  21   50   20   99  172   74  107   35    5  733  412]
     [   6    7    5   44   44   15   26    3    2  134 1506]]

                  precision    recall  f1-score   support

               0       0.98      1.00      0.99      1839
               1       0.96      0.99      0.98      1855
               2       0.98      0.99      0.99      1787
               3       0.91      0.98      0.95      1814
               4       0.87      0.94      0.90      1841
               5       0.94      0.97      0.96      1782
               6       0.93      0.96      0.94      1789
               7       0.98      1.00      0.99      1794
               8       0.99      0.99      0.99      1779
               9       0.74      0.42      0.54      1728
              10       0.75      0.84      0.79      1792

       micro avg       0.92      0.92      0.92     19800
       macro avg       0.91      0.92      0.91     19800
    weighted avg       0.91      0.92      0.91     19800



The convolutional neural network and recurrent neural networks were both able to reach test accuracy scores of over 90% after 50 epochs.Despite the potential of deep learning models to attain high accuracies, the computational complexity and slow runtimes makes this type of modeling unsuitable for this business objective.

## Analysis and Conclusion

The random forest model using features chosen based on F values was the best model when it came to predicting the first booking destinations, making it the strongest base for a classifier that can scaled for production.
While other model types had potential to produce more accurate predictions, they had much higher runtimes, making them unfit to run larger amounts of data. By introducing more data to this modeling pipeline, it can be trained to yield even more accurate and consistent results.

This classifier created by pairing the best supervised modeling technique and feature reduction method was built to be both accurate and scalable. Potential improvements in this product includes adding more features and further tuning of the model type. The accuracy of this model will most likely increase as it is trained with more data as well.

Understanding how to better utilize supervised modeling techniques to predict booking destination, will give insight as to how people are using the Airbnb platform and particular habits different types of customers share.
This can allow for more direct marketing to specific types of users or changes in the product that better match how the service is used. Through the use of cheap and accessible data, decisions can be made that can result in increased efficiency and revenue for the company.




```python

```
